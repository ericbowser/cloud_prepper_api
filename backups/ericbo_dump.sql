--
-- PostgreSQL database dump
--

\restrict c6HX8DCQDod7ra3PzEQJpb9WtGdbSm3scLrzqrbpQmAWesYAdsMJ1XIMoEb0NLz

-- Dumped from database version 18.1
-- Dumped by pg_dump version 18.1

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET transaction_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

--
-- Name: config; Type: SCHEMA; Schema: -; Owner: ericbo
--

CREATE SCHEMA config;


ALTER SCHEMA config OWNER TO ericbo;

--
-- Name: lasertg; Type: SCHEMA; Schema: -; Owner: postgres
--

CREATE SCHEMA lasertg;


ALTER SCHEMA lasertg OWNER TO postgres;

--
-- Name: prepper; Type: SCHEMA; Schema: -; Owner: ericbo
--

CREATE SCHEMA prepper;


ALTER SCHEMA prepper OWNER TO ericbo;

--
-- Name: get_questions_by_difficulty(character varying, character varying, character varying, integer); Type: FUNCTION; Schema: prepper; Owner: postgres
--

CREATE FUNCTION prepper.get_questions_by_difficulty(p_cognitive_level character varying DEFAULT NULL::character varying, p_skill_level character varying DEFAULT NULL::character varying, p_domain character varying DEFAULT NULL::character varying, p_limit integer DEFAULT 10) RETURNS TABLE(question_id integer, question_number integer, category character varying, cognitive_level character varying, skill_level character varying, domain character varying, question_text text, options jsonb, correct_answer text, explanation text, multiple_answers boolean)
    LANGUAGE plpgsql
    AS $$
BEGIN
    RETURN QUERY
    SELECT 
        q.question_id,
        q.question_number,
        q.category,
        q.cognitive_level,
        q.skill_level,
        q.domain,
        q.question_text,
        q.options,
        q.correct_answer,
        q.explanation,
        q.multiple_answers
    FROM prepper.comptia_cloud_plus_questions q
    WHERE 
        (p_cognitive_level IS NULL OR q.cognitive_level = p_cognitive_level)
        AND (p_skill_level IS NULL OR q.skill_level = p_skill_level)
        AND (p_domain IS NULL OR q.domain = p_domain)
    ORDER BY RANDOM()
    LIMIT p_limit;
END;
$$;


ALTER FUNCTION prepper.get_questions_by_difficulty(p_cognitive_level character varying, p_skill_level character varying, p_domain character varying, p_limit integer) OWNER TO postgres;

--
-- Name: FUNCTION get_questions_by_difficulty(p_cognitive_level character varying, p_skill_level character varying, p_domain character varying, p_limit integer); Type: COMMENT; Schema: prepper; Owner: postgres
--

COMMENT ON FUNCTION prepper.get_questions_by_difficulty(p_cognitive_level character varying, p_skill_level character varying, p_domain character varying, p_limit integer) IS 'Retrieve random questions filtered by cognitive level, skill level, and/or domain';


SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: env; Type: TABLE; Schema: config; Owner: postgres
--

CREATE TABLE config.env (
    id integer NOT NULL,
    project text NOT NULL,
    url text,
    config jsonb
);


ALTER TABLE config.env OWNER TO postgres;

--
-- Name: env_id_seq; Type: SEQUENCE; Schema: config; Owner: postgres
--

ALTER TABLE config.env ALTER COLUMN id ADD GENERATED ALWAYS AS IDENTITY (
    SEQUENCE NAME config.env_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1
);


--
-- Name: contact; Type: TABLE; Schema: lasertg; Owner: postgres
--

CREATE TABLE lasertg.contact (
    id integer CONSTRAINT contact_contactid_not_null NOT NULL,
    firstname text,
    lastname text,
    petname text,
    phone text,
    address text,
    fullname text
);


ALTER TABLE lasertg.contact OWNER TO postgres;

--
-- Name: contact_contactid_seq; Type: SEQUENCE; Schema: lasertg; Owner: postgres
--

ALTER TABLE lasertg.contact ALTER COLUMN id ADD GENERATED ALWAYS AS IDENTITY (
    SEQUENCE NAME lasertg.contact_contactid_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1
);


--
-- Name: inventory; Type: TABLE; Schema: lasertg; Owner: postgres
--

CREATE TABLE lasertg.inventory (
    id integer DEFAULT 1 NOT NULL,
    materialid integer NOT NULL,
    num_in_stock integer
);


ALTER TABLE lasertg.inventory OWNER TO postgres;

--
-- Name: material; Type: TABLE; Schema: lasertg; Owner: postgres
--

CREATE TABLE lasertg.material (
    id integer DEFAULT 1 NOT NULL,
    type text NOT NULL,
    shape text NOT NULL,
    color text NOT NULL
);


ALTER TABLE lasertg.material OWNER TO postgres;

--
-- Name: orders; Type: TABLE; Schema: lasertg; Owner: postgres
--

CREATE TABLE lasertg.orders (
    id integer CONSTRAINT orders_orderid_not_null NOT NULL,
    contactid integer NOT NULL,
    stripe_payment_intent_id text,
    amount integer NOT NULL,
    currency text DEFAULT 'usd'::text,
    status text DEFAULT 'pending'::text,
    tag_text_line_1 text,
    tag_text_line_2 text,
    tag_text_line_3 text,
    has_qr_code boolean DEFAULT true,
    created_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP,
    updated_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP
);


ALTER TABLE lasertg.orders OWNER TO postgres;

--
-- Name: orders_orderid_seq; Type: SEQUENCE; Schema: lasertg; Owner: postgres
--

ALTER TABLE lasertg.orders ALTER COLUMN id ADD GENERATED ALWAYS AS IDENTITY (
    SEQUENCE NAME lasertg.orders_orderid_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1
);


--
-- Name: qrcode; Type: TABLE; Schema: lasertg; Owner: postgres
--

CREATE TABLE lasertg.qrcode (
    id integer DEFAULT 1 NOT NULL,
    contactid integer NOT NULL,
    qrcode bytea NOT NULL,
    orderid integer
);


ALTER TABLE lasertg.qrcode OWNER TO postgres;

--
-- Name: tag; Type: TABLE; Schema: lasertg; Owner: postgres
--

CREATE TABLE lasertg.tag (
    id integer DEFAULT 1 NOT NULL,
    tagside character(10)[],
    text_line_1 character varying(100)[],
    text_line_2 text,
    text_line_3 text
);


ALTER TABLE lasertg.tag OWNER TO postgres;

--
-- Name: admin; Type: TABLE; Schema: prepper; Owner: postgres
--

CREATE TABLE prepper.admin (
    id integer DEFAULT 1 NOT NULL,
    name text,
    adminkey bigint NOT NULL
);


ALTER TABLE prepper.admin OWNER TO postgres;

--
-- Name: admin_adminkey_seq; Type: SEQUENCE; Schema: prepper; Owner: postgres
--

CREATE SEQUENCE prepper.admin_adminkey_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE prepper.admin_adminkey_seq OWNER TO postgres;

--
-- Name: admin_adminkey_seq; Type: SEQUENCE OWNED BY; Schema: prepper; Owner: postgres
--

ALTER SEQUENCE prepper.admin_adminkey_seq OWNED BY prepper.admin.adminkey;


--
-- Name: aws_certified_architect_associate_questions; Type: TABLE; Schema: prepper; Owner: ericbo
--

CREATE TABLE prepper.aws_certified_architect_associate_questions (
    id integer NOT NULL,
    question_id integer CONSTRAINT aws_certified_architect_associate_question_question_id_not_null NOT NULL,
    question_number integer CONSTRAINT aws_certified_architect_associate_ques_question_number_not_null NOT NULL,
    category text,
    difficulty text,
    domain text,
    question_text text CONSTRAINT aws_certified_architect_associate_questi_question_text_not_null NOT NULL,
    options jsonb NOT NULL,
    correct_answer text CONSTRAINT aws_certified_architect_associate_quest_correct_answer_not_null NOT NULL,
    explanation text,
    explanation_details jsonb,
    multiple_answers boolean
);


ALTER TABLE prepper.aws_certified_architect_associate_questions OWNER TO ericbo;

--
-- Name: aws_certified_architect_associate_questions_id_seq; Type: SEQUENCE; Schema: prepper; Owner: ericbo
--

ALTER TABLE prepper.aws_certified_architect_associate_questions ALTER COLUMN id ADD GENERATED ALWAYS AS IDENTITY (
    SEQUENCE NAME prepper.aws_certified_architect_associate_questions_id_seq
    START WITH 95
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1
);


--
-- Name: aws_question_id_seq; Type: SEQUENCE; Schema: prepper; Owner: ericbo
--

CREATE SEQUENCE prepper.aws_question_id_seq
    START WITH 194
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE prepper.aws_question_id_seq OWNER TO ericbo;

--
-- Name: aws_question_number_seq; Type: SEQUENCE; Schema: prepper; Owner: ericbo
--

CREATE SEQUENCE prepper.aws_question_number_seq
    START WITH 94
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE prepper.aws_question_number_seq OWNER TO ericbo;

--
-- Name: comptia_cloud_plus_questions; Type: TABLE; Schema: prepper; Owner: postgres
--

CREATE TABLE prepper.comptia_cloud_plus_questions (
    question_id integer NOT NULL,
    question_number integer NOT NULL,
    category text,
    difficulty text,
    domain text,
    question_text text NOT NULL,
    options jsonb NOT NULL,
    correct_answer text NOT NULL,
    explanation text,
    explanation_details jsonb,
    multiple_answers bit(1),
    correct_answers text[],
    id bigint NOT NULL,
    cognitive_level character varying(50),
    skill_level character varying(50),
    CONSTRAINT chk_cognitive_level CHECK (((cognitive_level)::text = ANY ((ARRAY['Knowledge'::character varying, 'Comprehension'::character varying, 'Application'::character varying, 'Analysis'::character varying, 'Synthesis'::character varying, 'Evaluation'::character varying])::text[]))),
    CONSTRAINT chk_skill_level CHECK (((skill_level)::text = ANY ((ARRAY['Beginner'::character varying, 'Intermediate'::character varying, 'Advanced'::character varying, 'Expert'::character varying])::text[])))
);


ALTER TABLE prepper.comptia_cloud_plus_questions OWNER TO postgres;

--
-- Name: COLUMN comptia_cloud_plus_questions.difficulty; Type: COMMENT; Schema: prepper; Owner: postgres
--

COMMENT ON COLUMN prepper.comptia_cloud_plus_questions.difficulty IS 'Legacy combined difficulty field. New applications should use cognitive_level and skill_level separately.';


--
-- Name: COLUMN comptia_cloud_plus_questions.cognitive_level; Type: COMMENT; Schema: prepper; Owner: postgres
--

COMMENT ON COLUMN prepper.comptia_cloud_plus_questions.cognitive_level IS 'Bloom''s Taxonomy level: What type of thinking is required? (Knowledge, Comprehension, Application, Analysis, Synthesis, Evaluation)';


--
-- Name: COLUMN comptia_cloud_plus_questions.skill_level; Type: COMMENT; Schema: prepper; Owner: postgres
--

COMMENT ON COLUMN prepper.comptia_cloud_plus_questions.skill_level IS 'Experience level required: Who should be able to answer this? (Beginner, Intermediate, Advanced, Expert)';


--
-- Name: comptia_cloud_plus_questions_id_seq; Type: SEQUENCE; Schema: prepper; Owner: postgres
--

ALTER TABLE prepper.comptia_cloud_plus_questions ALTER COLUMN id ADD GENERATED BY DEFAULT AS IDENTITY (
    SEQUENCE NAME prepper.comptia_cloud_plus_questions_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1
);


--
-- Name: question_id_seq; Type: SEQUENCE; Schema: prepper; Owner: postgres
--

CREATE SEQUENCE prepper.question_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE prepper.question_id_seq OWNER TO postgres;

--
-- Name: question_id_seq; Type: SEQUENCE OWNED BY; Schema: prepper; Owner: postgres
--

ALTER SEQUENCE prepper.question_id_seq OWNED BY prepper.comptia_cloud_plus_questions.question_id;


--
-- Name: question_number_seq; Type: SEQUENCE; Schema: prepper; Owner: postgres
--

CREATE SEQUENCE prepper.question_number_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE prepper.question_number_seq OWNER TO postgres;

--
-- Name: question_number_seq; Type: SEQUENCE OWNED BY; Schema: prepper; Owner: postgres
--

ALTER SEQUENCE prepper.question_number_seq OWNED BY prepper.comptia_cloud_plus_questions.question_number;


--
-- Name: users; Type: TABLE; Schema: prepper; Owner: postgres
--

CREATE TABLE prepper.users (
    id integer NOT NULL,
    username character varying(50) NOT NULL,
    email character varying(255) NOT NULL,
    password_hash text NOT NULL,
    role character varying(20) DEFAULT 'user'::character varying NOT NULL,
    created_at timestamp without time zone DEFAULT CURRENT_TIMESTAMP NOT NULL,
    last_login timestamp without time zone,
    is_active boolean DEFAULT true NOT NULL,
    CONSTRAINT users_role_check CHECK (((role)::text = ANY ((ARRAY['user'::character varying, 'admin'::character varying])::text[])))
);


ALTER TABLE prepper.users OWNER TO postgres;

--
-- Name: users_id_seq; Type: SEQUENCE; Schema: prepper; Owner: postgres
--

CREATE SEQUENCE prepper.users_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER SEQUENCE prepper.users_id_seq OWNER TO postgres;

--
-- Name: users_id_seq; Type: SEQUENCE OWNED BY; Schema: prepper; Owner: postgres
--

ALTER SEQUENCE prepper.users_id_seq OWNED BY prepper.users.id;


--
-- Name: v_beginner_questions; Type: VIEW; Schema: prepper; Owner: postgres
--

CREATE VIEW prepper.v_beginner_questions AS
 SELECT question_id,
    question_number,
    category,
    cognitive_level,
    skill_level,
    domain,
    question_text,
    correct_answer,
    multiple_answers
   FROM prepper.comptia_cloud_plus_questions
  WHERE ((skill_level)::text = 'Beginner'::text)
  ORDER BY cognitive_level, category;


ALTER VIEW prepper.v_beginner_questions OWNER TO postgres;

--
-- Name: v_difficulty_stats; Type: VIEW; Schema: prepper; Owner: postgres
--

CREATE VIEW prepper.v_difficulty_stats AS
 SELECT cognitive_level,
    skill_level,
    count(*) AS total_questions,
    round((((count(*))::numeric * 100.0) / sum(count(*)) OVER ()), 2) AS percentage
   FROM prepper.comptia_cloud_plus_questions
  GROUP BY cognitive_level, skill_level
  ORDER BY (count(*)) DESC;


ALTER VIEW prepper.v_difficulty_stats OWNER TO postgres;

--
-- Name: v_questions_by_cognitive; Type: VIEW; Schema: prepper; Owner: postgres
--

CREATE VIEW prepper.v_questions_by_cognitive AS
 SELECT cognitive_level,
    skill_level,
    category,
    domain,
    count(*) AS question_count
   FROM prepper.comptia_cloud_plus_questions
  GROUP BY cognitive_level, skill_level, category, domain
  ORDER BY
        CASE cognitive_level
            WHEN 'Knowledge'::text THEN 1
            WHEN 'Comprehension'::text THEN 2
            WHEN 'Application'::text THEN 3
            WHEN 'Analysis'::text THEN 4
            WHEN 'Synthesis'::text THEN 5
            WHEN 'Evaluation'::text THEN 6
            ELSE NULL::integer
        END,
        CASE skill_level
            WHEN 'Beginner'::text THEN 1
            WHEN 'Intermediate'::text THEN 2
            WHEN 'Advanced'::text THEN 3
            WHEN 'Expert'::text THEN 4
            ELSE NULL::integer
        END;


ALTER VIEW prepper.v_questions_by_cognitive OWNER TO postgres;

--
-- Name: admin adminkey; Type: DEFAULT; Schema: prepper; Owner: postgres
--

ALTER TABLE ONLY prepper.admin ALTER COLUMN adminkey SET DEFAULT nextval('prepper.admin_adminkey_seq'::regclass);


--
-- Name: users id; Type: DEFAULT; Schema: prepper; Owner: postgres
--

ALTER TABLE ONLY prepper.users ALTER COLUMN id SET DEFAULT nextval('prepper.users_id_seq'::regclass);


--
-- Data for Name: env; Type: TABLE DATA; Schema: config; Owner: postgres
--

COPY config.env (id, project, url, config) FROM stdin;
1	Assist	git@github.com:ericbowser/ConfigurationManager.git	{"ID": "105094990767274797585", "KEY": "00b91c7fe7a65a99a42cf55931bf853a1093d4bf", "HOST": "127.0.0.1", "PORT": "32636", "DB_URL": "postgres://postgres:1006@localhost:5432/localhost", "DB_PORT": "5432", "DB_USER": "postgres", "NODE_ENV": "development", "DB_SERVER": "127.0.0.1", "GEMINI_PRO": "gemini-1.5-pro", "OPENAI_ORG": "org-8tTqfyzCfx2lAhM8NkNGw2eQ", "DB_DATABASE": "postgres", "DB_PASSWORD": "1006", "CLAUDE_MODEL": "claude-sonnet-4-5-20250929", "GEMINI_FLASH": "gemini-1.5-flash", "EMBEDDING_URL": "https://api.openai.com/v1/embeddings", "GMAIL_API_KEY": "AIzaSyBEkiaGEeJk_DbOYZd1QUY32Judg-C0bIE", "ASSISTANT_NAME": "prompt_assist", "CLAUDE_API_KEY": "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", "GEMINI_API_KEY": "AIzaSyAPYPrVLCo7Gm4v3gZ5vlQg4mknkv1ke1M", "GEMINI_PROJECT": "google_cloud", "GMAIL_SA_EMAIL": "erb-gmail@gmail-send-426819.iam.gserviceaccount.com", "LANGFLOW_TOKEN": "AstraCS:ZXOQykGYYggfIBRkfsjcBAra:1edf36bdb888506bb8fee62030e3091db9a0666ac1bbac29222e2f2cfea1ebed", "OPENAI_API_KEY": "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", "OPENAI_DALLE_2": "dall-e-2", "OPENAI_DALLE_3": "dall-e-3", "SEND_GRID_MAIL": "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", "CLAUDE_BASE_URL": "https://api.anthropic.com", "EMBEDDING_MODEL": "text-embedding-ada-002", "LANGFLOW_SECRET": "L0-7sOlKIKth+hzFt8qEoCngMOeRDwjii9itE8XiirDfFZcEyjRa0jo85n2rnn.-NX2Jths20IJz-mnp-y+yYI.0Mkqhy-qZCP.lSKwHk1kd19+AfJZYl0Tzb_XxQ8T.", "VERTEX_AI_MODEL": "gemini-2.0-flash-001", "BINANCE_LIVE_URL": "https://api.binance.com", "DEEPSEEK_API_KEY": "sk-1e8f332ac7774728b1abc89c951c7996", "OPENAI_IMAGE_URL": "https://api.openai.com/v1/images/generations", "OPENAI_THREAD_ID": "thread_u4PAlA68MIdbv47jh85MQGl5", "SENDGRID_API_KEY": "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", "DEEPSEEK_BASE_URL": "https://api.deepseek.com", "DEEPSEEK_TEXT_URL": "https://api.deepseek.com/v1/chat/completions", "GEMINI_PRO_VISION": "gemini-1.0-pro-vision", "LANGFLOW_CLIENTID": "ZXOQykGYYggfIBRkfsjcBAra", "OPENAI_PROJECT_ID": "proj_worMp6juSG4RJXnmWKqUkSv9", "BINANCE_US_API_KEY": "oymZxLRj0l8meBdRGq7BS7BYJm7nM3qKf36wzqf0co6w8lt4SdSeLDzpWzVRIvUb", "DEEPSEEK_IMAGE_URL": "https://api.deepseek.com/v1/images/generations", "EMAIL_ADDRESS_FROM": "laser@new-collar.space", "GMAIL_APP_PASSWORD": "swxd nhwk qiqi bknk", "OPENAI_API_FILE_ID": "vs_s5bXewPzITsOHPj6uhH61onJ", "CLAUDE_MESSAGES_URL": "https://api.anthropic.com/v1/messages", "DEEPSEEK_CHAT_MODEL": "deepseek-chat", "OPENAI_API_BASE_URL": "https://api.openai.com/v2/", "OPENAI_ASSISTANT_ID": "asst_OMXl7F9gTXdbPRFACurBdVlj", "OPENAI_PROJECT_NAME": "outline", "STRIPE_TEST_API_KEY": "pk_test_51PvG4lJs9qng1MN2387d7B095bosQX8V2nhaY8ZHyNwXhNNcvcAldJL7tAJoWZ3nip6wKjn8p98fpa1EtwRQSPbm00eCIULHjW", "ALPACA_LIVE_REST_URL": "https://api.alpaca.markets", "ALPACA_PAPER_API_KEY": "PK3YR9RBKPM03NT8V52T", "BINANCE_TEST_API_KEY": "uLAS2pF0mI2LhAaGEvAhS9rvjOpi1Gzv2sJX1kNRyiJBrTMdOuHvKKLU6MYOFNw7", "ALPACA_PAPER_REST_URL": "https://paper-api.alpaca.markets/v2", "ALPACA_WEB_SOCKET_URL": "wss://stream.data.alpaca.markets/v1beta3/crypto/us", "ALPHA_SQUARED_API_KEY": "LQrBnhUlSVW9fSWarhSgXyDVN0fYfc3U10EW0GXy", "DEEPSEEK_REASON_MODEL": "deepseek-reasoner", "OPENAI_ASSISTANT_MODEL": "gpt-5-mini-2025-08-07", "OPENAI_REASONING_MODEL": "gpt-5-nano-2025-08-07", "ALPACA_PAPER_API_SECRET": "CuGTF2w9fejIwUek0gnl2U4LVOHBlIiOO9hcOu1u", "ALPHA_SQUARED_STRATEGIES": "https://alphasquared.io/wp-json/as/v1/strategy-values", "BINANCE_US_API_SECRET_KEY": "Sz19vRyXghZwxNzzYoabBrSfGuUt13Ka3PPWW8hAnHbPOMRM5bLyZa3USu8IvXHx", "CLAUDE_MODELS_INFORMATION": "https://api.anthropic.com/v1/models", "ALPHA_SQUARED_HYPOTHETICALS": "https://alphasquared.io/wp-json/as/v1/hypotheticals", "BINANCE_TEST_API_SECRET_KEY": "iIUVQPWElnY5Fcu1lfQXAqFdzP2g83P1LzPcuGG6bS1MWk1378byxg2n3lxc6izx", "ALPHA_SQUARED_ASSET_INFO_URL": "https://alphasquared.io/wp-json/as/v1/asset-info", "ALPACA_WEB_SOCKET_SANDBOX_URL": "wss://stream.data.sandbox.alpaca.markets/v1beta3/crypto/us"}
2	CloudPrepper	git@github.com:ericbowser/CloudPrepper.git	{"HOST": "localhost", "PORT": "55234", "CLOUD_PREPPER_BASE_URL": "http://localhost:55235", "CLOUD_PREPPER_ADD_QUESTION": "/addQuestion", "CLOUD_PREPPER_GET_QUESTIONS": "/getExamQuestions", "CLOUD_PREPPER_UPDATE_QUESTION": "/updateQuestion"}
4	backendlaser	git@github.com:ericbowser/backendlaser.git	{"HOST": "localhost", "PORT": "32638", "DB_PORT": "5433", "DB_USER": "ericbo", "NODE_ENV": "development", "DB_SERVER": "localhost", "DB_PASSWORD": "1007", "STRIPE_SANDBOX_API_KEY": "sk_test_51RyGwgR7FRhkVqEWyLp6l82m6xuvtSNTe9h8cRcxElLgbk3ZyjnYYFKGA5In5EIyD5cZDRNuBKMY0Ue5tyDfuJwY002ibjVq8T", "STRIPE_TEST_SECRET_API_KEY": "sk_test_51RyGwgR7FRhkVqEWyLp6l82m6xuvtSNTe9h8cRcxElLgbk3ZyjnYYFKGA5In5EIyD5cZDRNuBKMY0Ue5tyDfuJwY002ibjVq8T", "STRIPE_TEST_PUBLISHABLE_API_KEY": "pk_test_51RyGwgR7FRhkVqEWb96LPpIOxn5KSexvRcyjmvGhDdAVv0cXB22ZtK3hbXzWPcp77F5HTdWG126rCKbevI15fVFZ00yLYPrvAs"}
3	cloud_prepper_api	git@github.com:ericbowser/cloud_prepper_api.git	{"HOST": "localhost", "PORT": "55235", "DB_HOST": "localhost", "DB_PORT": "5432", "DB_USER": "ericbo", "DB_SERVER": "localhost", "DB_PASSWORD": "1007", "GMAIL_APP_PASSWORD": "swxd nhwk qiqi bknk"}
\.


--
-- Data for Name: contact; Type: TABLE DATA; Schema: lasertg; Owner: postgres
--

COPY lasertg.contact (id, firstname, lastname, petname, phone, address, fullname) FROM stdin;
\.


--
-- Data for Name: inventory; Type: TABLE DATA; Schema: lasertg; Owner: postgres
--

COPY lasertg.inventory (id, materialid, num_in_stock) FROM stdin;
\.


--
-- Data for Name: material; Type: TABLE DATA; Schema: lasertg; Owner: postgres
--

COPY lasertg.material (id, type, shape, color) FROM stdin;
\.


--
-- Data for Name: orders; Type: TABLE DATA; Schema: lasertg; Owner: postgres
--

COPY lasertg.orders (id, contactid, stripe_payment_intent_id, amount, currency, status, tag_text_line_1, tag_text_line_2, tag_text_line_3, has_qr_code, created_at, updated_at) FROM stdin;
\.


--
-- Data for Name: qrcode; Type: TABLE DATA; Schema: lasertg; Owner: postgres
--

COPY lasertg.qrcode (id, contactid, qrcode, orderid) FROM stdin;
\.


--
-- Data for Name: tag; Type: TABLE DATA; Schema: lasertg; Owner: postgres
--

COPY lasertg.tag (id, tagside, text_line_1, text_line_2, text_line_3) FROM stdin;
\.


--
-- Data for Name: admin; Type: TABLE DATA; Schema: prepper; Owner: postgres
--

COPY prepper.admin (id, name, adminkey) FROM stdin;
\.


--
-- Data for Name: aws_certified_architect_associate_questions; Type: TABLE DATA; Schema: prepper; Owner: ericbo
--

COPY prepper.aws_certified_architect_associate_questions (id, question_id, question_number, category, difficulty, domain, question_text, options, correct_answer, explanation, explanation_details, multiple_answers) FROM stdin;
72	172	72	Performance - Storage Optimization	Advanced	Design High-Performing Architectures	A high-frequency trading application requires consistent sub-millisecond latency for database operations. The application processes millions of transactions per second with frequent random read/write patterns. Which storage solution provides optimal performance?	[{"text": "EBS gp3 volumes with baseline performance", "isCorrect": false}, {"text": "EBS io2 volumes with provisioned IOPS", "isCorrect": true}, {"text": "EFS with General Purpose performance mode", "isCorrect": false}, {"text": "S3 with Transfer Acceleration", "isCorrect": false}]	EBS io2 volumes with provisioned IOPS	EBS io2 provides consistent high IOPS performance with sub-millisecond latency, ideal for high-frequency trading applications requiring predictable performance.	{"summary": "High-performance storage requirements:", "breakdown": ["io2: Up to 64,000 IOPS per volume", "Consistent performance: no performance variability", "Sub-millisecond latency: ideal for trading systems", "Provisioned IOPS: guaranteed performance levels"], "otherOptions": "gp3 has variable performance and IOPS limits\\nEFS has higher latency than block storage\\nS3 is object storage, not suitable for databases"}	\N
8	108	8	AWS Security - IAM Policies	Expert	Design Secure Applications	A company has multiple AWS accounts and needs to grant developers access to specific S3 buckets based on their team assignment. Developers should only access buckets prefixed with their team name (e.g., team-alpha-*). Which approach provides the MOST secure and scalable solution?	[{"text": "Create individual IAM users in each account with specific S3 bucket policies", "isCorrect": false}, {"text": "Use AWS Organizations with SCPs and cross-account IAM roles with dynamic policy conditions", "isCorrect": true}, {"text": "Create shared IAM users with different access keys for each team", "isCorrect": false}, {"text": "Use S3 bucket policies with IAM user conditions for each team", "isCorrect": false}]	Use AWS Organizations with SCPs and cross-account IAM roles with dynamic policy conditions	Organizations with SCPs provide account-level governance, while cross-account roles with dynamic conditions enable secure, scalable team-based access.	{"summary": "Multi-account secure access strategy:", "breakdown": ["SCPs provide organization-wide security boundaries", "Cross-account roles eliminate need for multiple IAM users", "Dynamic conditions based on user attributes (team tags)", "Centralized access management across accounts"], "otherOptions": "Individual users don't scale across multiple accounts\\nShared users violate security best practices\\nBucket policies alone can't handle multi-account scenarios efficiently"}	\N
9	109	9	AWS Security - Data Encryption	Application	Design Secure Applications	A healthcare company must encrypt all data at rest and in transit. They need to maintain full control over encryption keys and meet HIPAA compliance requirements. The solution must integrate with RDS, S3, and EBS. Which key management approach is MOST appropriate?	[{"text": "Use AWS managed keys (SSE-S3, default RDS encryption)", "isCorrect": false}, {"text": "Use AWS KMS customer managed keys with CloudTrail logging", "isCorrect": true}, {"text": "Use client-side encryption with application-managed keys", "isCorrect": false}, {"text": "Use AWS KMS AWS managed keys with detailed monitoring", "isCorrect": false}]	Use AWS KMS customer managed keys with CloudTrail logging	Customer managed KMS keys provide full control over key policies, rotation, and access, while CloudTrail provides complete audit trails required for HIPAA compliance.	{"summary": "HIPAA-compliant key management requirements:", "breakdown": ["Customer managed keys: Full control over key policies and access", "CloudTrail: Complete audit trail of key usage", "Cross-service integration: Works with RDS, S3, EBS", "HIPAA compliance: Meets regulatory requirements for key control"], "otherOptions": "AWS managed keys don't provide sufficient control for HIPAA\\nClient-side encryption complex and doesn't integrate well\\nAWS managed keys still don't give customer full control"}	\N
11	111	11	AWS Security - Advanced Threats	Expert	Design Secure Applications	A company's web application experiences DDoS attacks and SQL injection attempts. They need comprehensive protection that automatically adapts to new attack patterns. The solution must integrate with CloudFront and ALB. Which combination provides the MOST comprehensive protection?	[{"text": "AWS Shield Standard + CloudFront + Security Groups", "isCorrect": false}, {"text": "AWS Shield Advanced + AWS WAF + AWS Firewall Manager", "isCorrect": true}, {"text": "AWS Shield Standard + AWS WAF + VPC Flow Logs", "isCorrect": false}, {"text": "Network ACLs + AWS Inspector + CloudWatch", "isCorrect": false}]	AWS Shield Advanced + AWS WAF + AWS Firewall Manager	Shield Advanced provides enhanced DDoS protection, WAF blocks application-layer attacks, and Firewall Manager centrally manages security policies across resources.	{"summary": "Comprehensive web application protection stack:", "breakdown": ["Shield Advanced: Enhanced DDoS protection with attack response team", "WAF: Blocks SQL injection, XSS, and other application attacks", "Firewall Manager: Centralized security policy management", "Auto-adaptation: Machine learning capabilities for new threats"], "otherOptions": "Shield Standard insufficient for advanced DDoS\\nVPC Flow Logs don't provide active protection\\nInspector scans instances, doesn't provide runtime protection"}	\N
18	118	18	AWS Security - Secrets Management	Application	Design Secure Applications	A microservices application needs to securely store and rotate database passwords, API keys, and certificates. The solution must integrate with RDS automatic rotation and provide audit trails. Which service combination is MOST appropriate?	[{"text": "Store secrets in S3 with bucket encryption and versioning", "isCorrect": false}, {"text": "Use AWS Secrets Manager with CloudTrail logging", "isCorrect": true}, {"text": "Store secrets in Systems Manager Parameter Store with SecureString", "isCorrect": false}, {"text": "Use AWS KMS to encrypt secrets stored in DynamoDB", "isCorrect": false}]	Use AWS Secrets Manager with CloudTrail logging	Secrets Manager provides automatic rotation for RDS, integration with AWS services, and complete audit trails through CloudTrail.	{"summary": "Secrets Manager comprehensive benefits:", "breakdown": ["Automatic rotation for RDS, Redshift, DocumentDB", "Native integration with AWS services", "Complete audit trail through CloudTrail", "Secure retrieval with fine-grained IAM permissions"], "otherOptions": "S3 not designed for secrets management\\nParameter Store lacks automatic rotation for databases\\nCustom solution requires building rotation logic"}	\N
24	124	24	AWS Security - ML Data Protection	Application	Design Secure Applications	A healthcare company uses Amazon Comprehend Medical to analyze patient records and Amazon Textract for insurance form processing. They must ensure HIPAA compliance and data isolation. Which security configuration is MOST appropriate?	[{"text": "Use default service encryption and enable CloudTrail logging", "isCorrect": false}, {"text": "Enable VPC endpoints, use customer-managed KMS keys, and create separate IAM roles per service", "isCorrect": true}, {"text": "Process all data in a single private subnet with network ACLs", "isCorrect": false}, {"text": "Use AWS managed keys and restrict access with S3 bucket policies", "isCorrect": false}]	Enable VPC endpoints, use customer-managed KMS keys, and create separate IAM roles per service	VPC endpoints ensure private connectivity, customer-managed KMS keys provide encryption control, and separate IAM roles enable least privilege access for HIPAA compliance.	{"summary": "HIPAA-compliant ML service configuration:", "breakdown": ["VPC endpoints: Keep data traffic within AWS network", "Customer-managed KMS: Full control over encryption keys", "Separate IAM roles: Least privilege per service", "Audit trail: CloudTrail logs all API calls for compliance"], "otherOptions": "Default encryption insufficient for HIPAA requirements\\nNetwork ACLs don't provide service-level isolation\\nAWS managed keys don't provide sufficient control"}	\N
25	125	25	AWS Security - Container Security	Expert	Design Secure Applications	A fintech application runs on EKS with sensitive payment processing workloads. The security team requires pod-level network isolation, secrets rotation every 30 days, and runtime threat detection. Which solution meets ALL requirements?	[{"text": "Calico network policies, AWS Secrets Manager, and Amazon Inspector", "isCorrect": false}, {"text": "AWS App Mesh, Systems Manager Parameter Store, and CloudWatch Container Insights", "isCorrect": false}, {"text": "EKS security groups for pods, AWS Secrets Manager with rotation, and Amazon GuardDuty EKS protection", "isCorrect": true}, {"text": "Network ACLs, HashiCorp Vault, and AWS Config rules", "isCorrect": false}]	EKS security groups for pods, AWS Secrets Manager with rotation, and Amazon GuardDuty EKS protection	EKS security groups provide pod-level isolation, Secrets Manager handles automatic rotation, and GuardDuty EKS protection offers runtime threat detection.	{"summary": "Comprehensive EKS security architecture:", "breakdown": ["Security groups for pods: Fine-grained network isolation at pod level", "Secrets Manager: Automatic 30-day rotation with EKS integration", "GuardDuty EKS: Runtime threat detection and anomaly detection", "Native AWS integration: Simplified management and compliance"], "otherOptions": "Inspector scans vulnerabilities, not runtime threats\\nApp Mesh is service mesh, not security focused\\nNetwork ACLs too coarse for pod-level control"}	\N
33	133	33	AWS ML Services Integration	Application	Design Secure Applications	A social media platform needs to detect inappropriate content in user posts (text and images) in real-time. They process 1 million posts daily and must comply with COPPA regulations for users under 13. Which combination of AWS services provides comprehensive content moderation with compliance controls?	[{"text": "Amazon Comprehend for text and custom SageMaker model for images", "isCorrect": false}, {"text": "Amazon Rekognition Content Moderation, Amazon Comprehend toxicity detection, with Lambda age-gating logic", "isCorrect": true}, {"text": "Amazon Textract for text extraction and Amazon Personalize for content filtering", "isCorrect": false}, {"text": "AWS Glue for data preparation and Amazon Forecast for predicting inappropriate content", "isCorrect": false}]	Amazon Rekognition Content Moderation, Amazon Comprehend toxicity detection, with Lambda age-gating logic	Rekognition provides pre-trained image moderation, Comprehend detects toxic text, and Lambda implements age-appropriate filtering logic for COPPA compliance.	{"summary": "Content moderation architecture components:", "breakdown": ["Rekognition: Detects inappropriate images (violence, adult content)", "Comprehend: Identifies toxic text, PII, and sentiment", "Lambda age-gating: Applies stricter rules for users under 13", "Real-time processing: Sub-second moderation decisions"], "otherOptions": "Custom models require training data and maintenance\\nTextract is for document processing, not moderation\\nForecast predicts time-series data, not content issues"}	\N
73	173	73	Performance - Global Content Delivery	Intermediate	Design High-Performing Architectures	A media streaming service serves video content globally but experiences slow loading for users in Asia-Pacific. Content is stored in S3 in us-east-1. Which solution provides the BEST performance improvement?	[{"text": "Enable S3 Transfer Acceleration globally", "isCorrect": false}, {"text": "Deploy CloudFront distribution with regional edge caches", "isCorrect": true}, {"text": "Replicate S3 buckets to all AWS regions", "isCorrect": false}, {"text": "Use ElastiCache clusters in each target region", "isCorrect": false}]	Deploy CloudFront distribution with regional edge caches	CloudFront edge locations provide low-latency content delivery globally, with regional edge caches reducing origin load for popular content.	{"summary": "Global content delivery optimization:", "breakdown": ["Edge locations: 400+ global points of presence", "Regional edge caches: reduce origin load", "Automatic content caching: faster subsequent requests", "Optimized for media streaming workloads"], "otherOptions": "Transfer Acceleration helps uploads, not downloads\\nMulti-region replication is complex and expensive\\nElastiCache doesn't help with media streaming"}	\N
37	137	37	AWS Security - Zero Trust Architecture	Expert	Design Secure Applications	A financial services company needs to implement zero-trust access to their AWS environment for 500 developers across 50 teams. Requirements include: no long-lived credentials, audit trail of all actions, team-based access boundaries, and integration with existing Okta SSO. Which solution provides the most comprehensive zero-trust implementation?	[{"text": "IAM users with MFA and IP restrictions for each developer", "isCorrect": false}, {"text": "AWS SSO with Okta integration, Permission Sets with session tags, and CloudTrail with EventBridge rules", "isCorrect": true}, {"text": "Cognito user pools with custom authorizers", "isCorrect": false}, {"text": "IAM roles with external ID and session policies", "isCorrect": false}]	AWS SSO with Okta integration, Permission Sets with session tags, and CloudTrail with EventBridge rules	AWS SSO provides temporary credentials, Okta integration enables SAML-based authentication, Permission Sets with session tags enforce team boundaries, while CloudTrail ensures complete audit trails.	{"summary": "Zero-trust implementation components:", "breakdown": ["AWS SSO: No long-lived credentials, automatic rotation", "Permission Sets: Team-based access with session tags", "Okta SAML: Leverages existing identity provider", "CloudTrail + EventBridge: Real-time audit and alerting"], "otherOptions": "IAM users require long-lived access keys\\nCognito designed for application users, not AWS access\\nMissing centralized management for 500 developers"}	\N
41	141	41	AWS Storage Concepts	Knowledge	Design High-Performing Architectures	A company needs to choose between different AWS storage types for various workloads. Which statement BEST describes the appropriate use cases for each storage type?	[{"text": "Use object storage for databases, block storage for file shares, file storage for web content", "isCorrect": false}, {"text": "Use block storage for databases, file storage for shared access, object storage for web content", "isCorrect": true}, {"text": "Use file storage for databases, object storage for shared access, block storage for web content", "isCorrect": false}, {"text": "All storage types can be used interchangeably for any workload", "isCorrect": false}]	Use block storage for databases, file storage for shared access, object storage for web content	Block storage (EBS) provides high IOPS for databases, file storage (EFS) enables shared access, object storage (S3) scales for web content.	{"summary": "AWS storage type characteristics:", "breakdown": ["Block storage (EBS): High IOPS, low latency, ideal for databases and boot volumes", "File storage (EFS): POSIX-compliant, shared access, good for content repositories", "Object storage (S3): Auto-scaling, REST API, perfect for web content and backups", "Each type optimized for specific access patterns and performance requirements"], "otherOptions": "Object storage not suitable for database IOPS requirements\\nFile storage lacks the IOPS performance needed for databases\\nEach storage type has specific use cases and limitations"}	\N
42	142	42	AWS Storage Concepts	Application	Design Resilient Architectures	An enterprise is migrating from on-premises and needs to maintain compatibility with existing storage protocols. They use NFS for Unix systems, SMB for Windows, and iSCSI for SAN storage. Which AWS services provide the BEST protocol compatibility?	[{"text": "S3 for all protocols using third-party gateways", "isCorrect": false}, {"text": "EFS for NFS, FSx for Windows for SMB, Storage Gateway for iSCSI", "isCorrect": true}, {"text": "EBS for all protocols using EC2 instance configuration", "isCorrect": false}, {"text": "DataSync for protocol translation across all storage types", "isCorrect": false}]	EFS for NFS, FSx for Windows for SMB, Storage Gateway for iSCSI	AWS provides native protocol support: EFS (NFS), FSx for Windows (SMB), Storage Gateway (iSCSI) for seamless migration.	{"summary": "Native AWS protocol support:", "breakdown": ["EFS: Native NFS v4.1, POSIX-compliant, Linux/Unix compatibility", "FSx for Windows: Native SMB, Active Directory integration", "Storage Gateway: iSCSI interface, hybrid cloud connectivity", "No application changes required for protocol compatibility"], "otherOptions": "S3 doesn't natively support NFS/SMB/iSCSI protocols\\nEBS provides block storage but not protocol-level compatibility\\nDataSync is for data transfer, not protocol translation"}	\N
46	146	46	AWS EBS Deep Dive	Application	Design High-Performing Architectures	A database application requires consistent 20,000 IOPS with low latency for transaction processing. The database size is 8TB and growing. Which EBS volume type and configuration provides the BEST performance while supporting future growth?	[{"text": "gp3 volumes with 20,000 provisioned IOPS", "isCorrect": false}, {"text": "io2 Block Express volumes with 20,000 provisioned IOPS", "isCorrect": true}, {"text": "gp2 volumes in RAID 0 configuration", "isCorrect": false}, {"text": "st1 throughput optimized volumes for cost savings", "isCorrect": false}]	io2 Block Express volumes with 20,000 provisioned IOPS	io2 Block Express provides consistent IOPS up to 256,000, supports volumes up to 64TB, and offers sub-millisecond latency for databases.	{"summary": "io2 Block Express advantages for databases:", "breakdown": ["Consistent IOPS: 20,000 IOPS guaranteed regardless of volume size", "Sub-millisecond latency: Optimal for transaction processing", "Scalability: Supports up to 64TB volumes for future growth", "Durability: 99.999% annual durability vs 99.9% for gp3"], "otherOptions": "gp3 IOPS can vary with volume size and burst credits\\nRAID 0 increases failure risk and management complexity\\nst1 optimized for throughput, not IOPS-intensive workloads"}	\N
43	143	43	AWS S3 Deep Dive	Application	Design Resilient Architectures	A media company requires 99.999999999% (11 9s) data durability for their video assets. They need to understand the difference between S3 durability and availability for their SLA reporting. Which statement is CORRECT about S3 durability vs availability?	[{"text": "Durability and availability are identical metrics across all S3 storage classes", "isCorrect": false}, {"text": "Durability (11 9s) represents data loss protection, while availability varies by storage class", "isCorrect": true}, {"text": "Availability is always higher than durability in S3", "isCorrect": false}, {"text": "Only S3 Standard provides 11 9s durability", "isCorrect": false}]	Durability (11 9s) represents data loss protection, while availability varies by storage class	S3 provides 11 9s durability (data loss protection) across ALL storage classes, but availability (uptime) varies by class.	{"summary": "S3 durability vs availability:", "breakdown": ["Durability (11 9s): Probability of NOT losing data over a year", "Availability varies: Standard (99.99%), IA (99.9%), Glacier (variable)", "All storage classes: Same durability guarantee", "Durability: Data integrity protection through redundancy"], "otherOptions": "Different metrics with different SLA percentages\\nAvailability percentages are typically lower than durability\\nAll S3 storage classes maintain 11 9s durability"}	\N
49	149	49	AWS File Systems Deep Dive	Application	Design High-Performing Architectures	A video editing company needs shared storage accessible from 50 EC2 instances across multiple AZs for collaborative editing projects. Files range from 1GB to 50GB, and editors require low-latency access. Performance needs vary from 1 GB/s during off-hours to 10 GB/s during production. Which file system solution provides the BEST performance and scalability?	[{"text": "Amazon EFS with General Purpose performance mode", "isCorrect": false}, {"text": "Amazon EFS with Max I/O performance mode and Provisioned Throughput", "isCorrect": true}, {"text": "Amazon FSx for Lustre with scratch file system", "isCorrect": false}, {"text": "Multiple EBS volumes shared using NFS on EC2", "isCorrect": false}]	Amazon EFS with Max I/O performance mode and Provisioned Throughput	EFS Max I/O mode supports higher IOPS for concurrent access, while Provisioned Throughput ensures consistent 10 GB/s performance regardless of file system size.	{"summary": "EFS configuration for video editing workloads:", "breakdown": ["Max I/O mode: Higher IOPS limit for 50 concurrent clients", "Provisioned Throughput: Guarantees 10 GB/s during peak production", "Multi-AZ access: Native support across availability zones", "POSIX compliance: Standard file system semantics for editing software"], "otherOptions": "General Purpose mode has lower IOPS limits for concurrent access\\nLustre scratch file system designed for HPC, not collaborative editing\\nEBS volumes can't be natively shared between instances"}	\N
1	101	1	AWS Architecture - High Availability	Application	Design Resilient Architectures	A company runs a critical e-commerce application on a single EC2 instance in us-east-1a. The application must achieve 99.99% uptime and automatically recover from AZ failures. Which solution provides the MOST cost-effective approach to meet these requirements?	[{"text": "Deploy a larger EC2 instance type in the same AZ", "isCorrect": false}, {"text": "Create an AMI and manually launch instances in other AZs when needed", "isCorrect": false}, {"text": "Use Auto Scaling Group with instances across multiple AZs behind an ALB", "isCorrect": true}, {"text": "Use Elastic Beanstalk with a single instance", "isCorrect": false}]	Use Auto Scaling Group with instances across multiple AZs behind an ALB	Auto Scaling Groups with multiple AZs provide automatic failure detection and recovery, while ALB distributes traffic and handles health checks.	{"summary": "Multi-AZ Auto Scaling provides automated resilience:", "breakdown": ["Auto Scaling Group automatically replaces failed instances", "Multiple AZs protect against entire AZ failures", "ALB provides health checks and traffic distribution", "Most cost-effective as it only runs minimum required instances"], "otherOptions": "Larger instance doesn't solve AZ failure\\nManual process doesn't meet automation requirement\\nSingle instance still has single point of failure"}	\N
48	148	48	AWS EBS Deep Dive	Expert	Design Secure Architectures	A financial application stores sensitive data on EBS volumes and requires encryption in transit and at rest. The application accesses data across multiple AZs and needs to ensure encryption keys remain under customer control with automatic rotation. Which configuration provides comprehensive encryption coverage?	[{"text": "EBS encryption with AWS managed keys and HTTPS application protocols", "isCorrect": false}, {"text": "EBS encryption with customer-managed KMS keys, automatic rotation enabled, and application-level TLS", "isCorrect": true}, {"text": "Instance store volumes with client-side encryption", "isCorrect": false}, {"text": "EBS encryption with customer-provided keys (SSE-C)", "isCorrect": false}]	EBS encryption with customer-managed KMS keys, automatic rotation enabled, and application-level TLS	Customer-managed KMS keys provide control with automatic rotation, EBS encryption handles data at rest, and TLS provides encryption in transit.	{"summary": "Comprehensive EBS encryption strategy:", "breakdown": ["Customer-managed KMS keys: Full control over encryption keys", "Automatic rotation: Annual key rotation without service interruption", "EBS encryption: All data at rest encrypted including snapshots", "Application TLS: Encrypts data in transit between instances"], "otherOptions": "AWS managed keys don't provide customer control\\nInstance store data is temporary, not suitable for persistent financial data\\nCustomer-provided keys require manual key management"}	\N
52	152	52	AWS Hybrid Storage Migration	Expert	Design High-Performing Architectures	A media company operates a hybrid architecture where video editors work on-premises but need seamless access to cloud storage for rendering jobs. Local performance must remain unaffected while providing transparent cloud storage scalability. Frequently accessed files should be available locally. Which hybrid storage solution provides the BEST user experience?	[{"text": "AWS Storage Gateway File Gateway with local cache", "isCorrect": true}, {"text": "AWS DataSync scheduled sync between on-premises and S3", "isCorrect": false}, {"text": "Direct Connect with EFS mounted on workstations", "isCorrect": false}, {"text": "S3 bucket mounted using third-party NFS gateway", "isCorrect": false}]	AWS Storage Gateway File Gateway with local cache	Storage Gateway File Gateway provides NFS interface with intelligent local caching, transparent S3 integration, and optimal performance for frequently accessed files.	{"summary": "File Gateway hybrid benefits:", "breakdown": ["Local cache: Frequently accessed files available at local speeds", "Transparent scaling: Files automatically stored in S3", "NFS interface: Standard file system access for editing applications", "Intelligent caching: Recently and frequently used files cached locally"], "otherOptions": "DataSync requires scheduled sync, not transparent access\\nEFS over Direct Connect has latency for large video files\\nThird-party solutions lack AWS service integration and support"}	\N
2	102	2	AWS Architecture - Data Resilience	Application	Design Resilient Architectures	A financial services company stores critical transaction data in RDS MySQL. They need to ensure data can be recovered with RPO of 5 minutes and RTO of 15 minutes in case of primary database failure. Which combination of solutions meets these requirements? (Choose TWO)	[{"text": "Enable automated backups with 5-minute backup frequency", "isCorrect": false}, {"text": "Configure RDS Multi-AZ deployment", "isCorrect": true}, {"text": "Create hourly manual snapshots", "isCorrect": false}, {"text": "Enable point-in-time recovery", "isCorrect": true}]	Configure RDS Multi-AZ deployment	Multi-AZ provides automatic failover within minutes, while point-in-time recovery enables restoration to any second within the backup retention period.	{"summary": "Meeting RPO/RTO requirements:", "breakdown": ["Multi-AZ: Automatic failover typically completes in 60-120 seconds", "Point-in-time recovery: Can restore to any second (meets 5-min RPO)", "Automated backups: Only daily, can't meet 5-minute RPO", "Manual snapshots: Too infrequent for RPO requirement"], "otherOptions": "Automated backups are daily, not every 5 minutes\\nHourly snapshots exceed 5-minute RPO requirement"}	\N
60	160	60	AWS Storage Resiliency Scalability	Application	Design Resilient Architectures	An e-commerce platform experiences seasonal traffic patterns with 50x increase during holiday sales. Their product catalog and user-generated content storage must scale automatically without performance degradation. Current EBS-based storage architecture requires manual intervention during peaks. Which storage architecture provides automatic scalability?	[{"text": "Auto Scaling EBS volumes with CloudWatch metrics", "isCorrect": false}, {"text": "Amazon S3 with CloudFront for content delivery and Lambda for processing", "isCorrect": true}, {"text": "Amazon EFS with Provisioned Throughput mode", "isCorrect": false}, {"text": "Multiple EBS volumes in RAID configuration", "isCorrect": false}]	Amazon S3 with CloudFront for content delivery and Lambda for processing	S3 provides unlimited automatic scaling, CloudFront handles traffic spikes through edge caching, and Lambda scales processing automatically with demand.	{"summary": "Auto-scaling storage architecture:", "breakdown": ["S3 unlimited scaling: Handles any amount of data and requests", "CloudFront edge caching: Reduces origin load during traffic spikes", "Lambda auto-scaling: Processing scales from 0 to thousands of concurrent executions", "No manual intervention: All components scale automatically"], "otherOptions": "EBS volumes cannot auto-scale capacity\\nEFS requires pre-provisioned throughput planning\\nRAID configuration still requires manual capacity planning"}	\N
65	165	65	AWS Storage Comprehensive	Expert	Design High-Performing Architectures	A large enterprise needs a comprehensive storage strategy for their cloud migration. Requirements include: 500TB database storage with 50,000 IOPS, 2PB file shares for 1000 users, 10PB object storage with global access, and hybrid connectivity to on-premises. Which architecture provides optimal performance, cost, and integration?	[{"text": "io2 Block Express EBS, EFS Max I/O, S3 with CloudFront, and Storage Gateway", "isCorrect": true}, {"text": "gp3 EBS, FSx for Lustre, S3 Intelligent-Tiering, and Direct Connect", "isCorrect": false}, {"text": "Aurora storage, EFS General Purpose, S3 Standard, and VPN connections", "isCorrect": false}, {"text": "Instance store, FSx for Windows, S3 Glacier, and AWS Outposts", "isCorrect": false}]	io2 Block Express EBS, EFS Max I/O, S3 with CloudFront, and Storage Gateway	io2 Block Express provides required IOPS, EFS Max I/O handles concurrent users, S3+CloudFront scales globally, Storage Gateway enables hybrid integration.	{"summary": "Enterprise storage architecture components:", "breakdown": ["io2 Block Express: 50,000 IOPS with sub-millisecond latency for databases", "EFS Max I/O: Scales to 1000+ concurrent users with higher IOPS", "S3 + CloudFront: Unlimited object storage with global low-latency access", "Storage Gateway: Seamless hybrid connectivity with caching"], "otherOptions": "gp3 may not provide consistent 50,000 IOPS, FSx Lustre not ideal for general file shares\\nAurora storage limits database choice, VPN insufficient for 500TB+ workloads\\nInstance store temporary, FSx Windows not mentioned as requirement, Glacier too slow"}	\N
66	166	66	AWS Storage Comprehensive	Expert	Design Secure Architectures	A financial services firm requires end-to-end security for their storage infrastructure handling sensitive customer data across S3, EBS, and EFS. Requirements include: customer-controlled encryption, automated compliance monitoring, data classification, and audit trails meeting regulatory standards. Which comprehensive security architecture meets all requirements?	[{"text": "AWS managed encryption, GuardDuty monitoring, and CloudTrail logging", "isCorrect": false}, {"text": "Customer-managed KMS keys, Amazon Macie, Config Conformance Packs, and CloudTrail data events", "isCorrect": true}, {"text": "CloudHSM encryption, Security Hub, and VPC Flow Logs", "isCorrect": false}, {"text": "Client-side encryption, Inspector scanning, and AWS Config rules", "isCorrect": false}]	Customer-managed KMS keys, Amazon Macie, Config Conformance Packs, and CloudTrail data events	Customer-managed KMS provides encryption control, Macie classifies sensitive data, Conformance Packs ensure compliance, CloudTrail data events provide complete audit trails.	{"summary": "Comprehensive security architecture for financial services:", "breakdown": ["Customer-managed KMS: Full control over encryption keys across all storage services", "Amazon Macie: Automated discovery and classification of sensitive financial data", "Config Conformance Packs: Continuous compliance monitoring for financial regulations", "CloudTrail data events: Complete audit trail of all data access activities"], "otherOptions": "AWS managed encryption lacks customer control required for financial services\\nCloudHSM overkill, Security Hub doesn't provide data classification\\nClient-side encryption complex to manage, Inspector focuses on vulnerabilities not data classification"}	\N
3	103	3	AWS Architecture - Cross-Region Resilience	Expert	Design Resilient Architectures	A media company streams video content globally and needs to ensure service availability even if an entire AWS region becomes unavailable. The application uses ALB, EC2 Auto Scaling, and RDS MySQL. Which design provides the MOST comprehensive disaster recovery solution?	[{"text": "Create read replicas in multiple regions with manual failover", "isCorrect": false}, {"text": "Use Route 53 health checks with cross-region ALBs and RDS cross-region automated backups", "isCorrect": false}, {"text": "Deploy identical infrastructure in two regions with Route 53 failover routing and RDS cross-region read replicas with promotion capability", "isCorrect": true}, {"text": "Use CloudFormation to deploy infrastructure in multiple regions manually when needed", "isCorrect": false}]	Deploy identical infrastructure in two regions with Route 53 failover routing and RDS cross-region read replicas with promotion capability	Full cross-region deployment with automated DNS failover and database replica promotion provides the fastest recovery from regional failures.	{"summary": "Comprehensive multi-region DR strategy:", "breakdown": ["Active-passive setup in multiple regions", "Route 53 automatic failover based on health checks", "RDS read replicas can be promoted to standalone databases", "Infrastructure already running in secondary region"], "otherOptions": "Manual failover too slow for video streaming\\nCross-region backups take too long to restore\\nManual deployment during disaster is too slow"}	\N
4	104	4	AWS Storage - Backup Strategy	Application	Design Resilient Architectures	A company needs to backup 50TB of data from on-premises to AWS. The data must be available within 4 hours during a disaster, and they want to minimize ongoing costs. The initial backup can take up to 2 weeks. Which solution is MOST appropriate?	[{"text": "AWS Storage Gateway with Tape Gateway to S3 Glacier", "isCorrect": false}, {"text": "AWS Snowball Edge to S3, then lifecycle policy to S3 Intelligent-Tiering", "isCorrect": true}, {"text": "Direct upload to S3 Standard over internet connection", "isCorrect": false}, {"text": "AWS DataSync to S3 Glacier Deep Archive", "isCorrect": false}]	AWS Snowball Edge to S3, then lifecycle policy to S3 Intelligent-Tiering	Snowball Edge handles the initial 50TB transfer efficiently, while S3 Intelligent-Tiering automatically optimizes costs and meets the 4-hour recovery requirement.	{"summary": "Large data backup strategy considerations:", "breakdown": ["Snowball Edge: Cost-effective for 50TB initial transfer", "S3 Intelligent-Tiering: Automatic cost optimization", "Standard tier availability: Immediate access for 4-hour RTO", "2-week initial timeline: Perfect for Snowball shipping"], "otherOptions": "Tape Gateway too slow for 4-hour recovery\\n50TB over internet too slow and expensive\\nGlacier Deep Archive takes 12+ hours to retrieve"}	\N
5	105	5	AWS Performance - Database Optimization	Application	Design High-Performing Architectures	An application experiences slow database performance during peak hours. The RDS MySQL instance shows 95% CPU utilization, and 80% of queries are read operations. The application cannot tolerate any downtime. Which solution provides the BEST performance improvement with minimal operational overhead?	[{"text": "Create a larger RDS instance and schedule maintenance window for upgrade", "isCorrect": false}, {"text": "Enable RDS Performance Insights to identify slow queries", "isCorrect": false}, {"text": "Create RDS read replicas and modify application to use them for read queries", "isCorrect": true}, {"text": "Migrate to Amazon Aurora MySQL with automatic scaling", "isCorrect": false}]	Create RDS read replicas and modify application to use them for read queries	Read replicas directly address the 80% read workload without downtime, providing immediate performance relief for read-heavy applications.	{"summary": "Read replica benefits for read-heavy workloads:", "breakdown": ["80% read queries can be offloaded to replicas", "No downtime required for setup", "Immediate performance improvement", "Cost-effective solution for read scaling"], "otherOptions": "Instance upgrade requires downtime\\nPerformance Insights only identifies issues, doesn't solve them\\nMigration requires significant effort and testing"}	\N
6	106	6	AWS Performance - Caching Strategy	Expert	Design High-Performing Architectures	A social media application serves user profiles stored in DynamoDB. The same profiles are accessed frequently by millions of users, causing high read costs and occasional throttling. The application requires sub-millisecond response times. Which caching strategy provides the BEST performance and cost optimization?	[{"text": "Implement application-level caching with Redis ElastiCache", "isCorrect": false}, {"text": "Enable DynamoDB DAX (DynamoDB Accelerator)", "isCorrect": true}, {"text": "Use CloudFront to cache DynamoDB API responses", "isCorrect": false}, {"text": "Increase DynamoDB provisioned read capacity", "isCorrect": false}]	Enable DynamoDB DAX (DynamoDB Accelerator)	DAX provides microsecond-level response times specifically designed for DynamoDB, with automatic cache management and no application changes required.	{"summary": "DAX advantages for DynamoDB caching:", "breakdown": ["Microsecond response times (sub-millisecond requirement)", "Transparent to application (no code changes)", "Automatic cache invalidation and management", "Reduces DynamoDB read costs significantly"], "otherOptions": "Redis requires application code changes and cache management\\nCloudFront can't cache DynamoDB API calls\\nIncreasing capacity doesn't solve latency, only costs more"}	\N
7	107	7	AWS Performance - Content Delivery	Comprehension	Design High-Performing Architectures	A global news website serves static content (images, videos, articles) to users worldwide. Users in Asia report slow page load times, while users in the US experience good performance. The origin servers are located in us-east-1. Which solution provides the MOST effective performance improvement for Asian users?	[{"text": "Deploy additional EC2 instances in ap-southeast-1", "isCorrect": false}, {"text": "Implement CloudFront distribution with edge locations in Asia", "isCorrect": true}, {"text": "Use Transfer Acceleration for S3 uploads", "isCorrect": false}, {"text": "Create Route 53 latency-based routing to multiple regions", "isCorrect": false}]	Implement CloudFront distribution with edge locations in Asia	CloudFront edge locations cache static content closer to Asian users, dramatically reducing latency for static content delivery.	{"summary": "CloudFront benefits for global content delivery:", "breakdown": ["Edge locations in Asia cache content locally", "Reduces latency from us-east-1 to Asia", "Handles static content (images, videos, articles) efficiently", "Automatic cache management and optimization"], "otherOptions": "Additional instances help dynamic content, not static\\nTransfer Acceleration for uploads, not downloads\\nLatency routing helps with dynamic content, not static"}	\N
12	112	12	AWS Cost Optimization - Instance Selection	Application	Design Cost-Optimized Architectures	A batch processing workload runs predictably every night from 2 AM to 6 AM for data analytics. The workload can tolerate interruptions and can resume from checkpoints. Current costs using On-Demand instances are $500/month. Which pricing model combination provides the MAXIMUM cost savings?	[{"text": "Reserved Instances for consistent baseline capacity", "isCorrect": false}, {"text": "Spot Instances with Auto Scaling groups across multiple AZs", "isCorrect": true}, {"text": "Savings Plans for compute usage commitment", "isCorrect": false}, {"text": "Dedicated Hosts for workload isolation", "isCorrect": false}]	Spot Instances with Auto Scaling groups across multiple AZs	Spot Instances can provide up to 90% savings for fault-tolerant batch workloads, and multiple AZs reduce interruption risk.	{"summary": "Spot Instances optimal for batch processing:", "breakdown": ["Up to 90% cost savings vs On-Demand", "Workload tolerates interruptions (checkpoint resume)", "Predictable 4-hour window reduces interruption risk", "Multiple AZs provide capacity diversification"], "otherOptions": "Reserved Instances only ~75% savings, overkill for 4-hour workload\\nSavings Plans better for consistent 24/7 usage\\nDedicated Hosts most expensive option"}	\N
13	113	13	AWS Cost Optimization - Storage Lifecycle	Application	Design Cost-Optimized Architectures	A company stores application logs in S3. Logs are actively analyzed for 30 days, occasionally accessed for 90 days, and must be retained for 7 years for compliance. Current storage costs are $1000/month in S3 Standard. Which lifecycle policy provides the BEST cost optimization?	[{"text": "Transition to S3 IA after 30 days, then S3 Glacier after 90 days, then Deep Archive after 1 year", "isCorrect": true}, {"text": "Keep all data in S3 Standard for quick access when needed", "isCorrect": false}, {"text": "Move everything to S3 Glacier immediately after upload", "isCorrect": false}, {"text": "Use S3 Intelligent-Tiering for automatic optimization", "isCorrect": false}]	Transition to S3 IA after 30 days, then S3 Glacier after 90 days, then Deep Archive after 1 year	Lifecycle policy matching access patterns: S3 Standard for active use, S3 IA for occasional access, Glacier for long-term retention, Deep Archive for compliance.	{"summary": "Optimal lifecycle transitions for log data:", "breakdown": ["Days 1-30: S3 Standard for active analysis", "Days 31-90: S3 IA for occasional access (50% savings)", "Days 91-365: S3 Glacier for backup (68% savings)", "Years 1-7: Deep Archive for compliance (77% savings)"], "otherOptions": "Standard storage most expensive for long-term retention\\nGlacier immediate transition prevents active analysis\\nIntelligent-Tiering adds overhead for predictable patterns"}	\N
14	114	14	AWS Cost Optimization - Database Costs	Expert	Design Cost-Optimized Architectures	A startup's MySQL database experiences unpredictable traffic patterns ranging from 5% to 95% CPU utilization throughout the day. Current RDS costs are $800/month with frequent over-provisioning during low-traffic periods. Which solution provides the BEST cost optimization while maintaining performance?	[{"text": "Migrate to Aurora Serverless v2 with automatic scaling", "isCorrect": true}, {"text": "Use RDS with scheduled Auto Scaling based on time patterns", "isCorrect": false}, {"text": "Switch to smaller RDS instance and add read replicas", "isCorrect": false}, {"text": "Migrate to DynamoDB with on-demand billing", "isCorrect": false}]	Migrate to Aurora Serverless v2 with automatic scaling	Aurora Serverless v2 automatically scales compute capacity based on actual demand, eliminating over-provisioning costs during low-traffic periods.	{"summary": "Aurora Serverless v2 benefits for variable workloads:", "breakdown": ["Automatic scaling from 0.5 to 128 ACUs based on demand", "Pay only for actual compute used (per-second billing)", "No over-provisioning during low-traffic periods", "MySQL compatibility maintains application compatibility"], "otherOptions": "Scheduled scaling can't handle unpredictable patterns\\nSmaller instance may cause performance issues during peaks\\nDynamoDB requires application rewrite from MySQL"}	\N
15	115	15	AWS Cost Optimization - Data Transfer	Application	Design Cost-Optimized Architectures	A company transfers 100TB of data monthly from EC2 instances to users worldwide. Current data transfer costs are $9,000/month. The data consists of software downloads that rarely change. Which solution provides the MOST significant cost reduction?	[{"text": "Use VPC endpoints to reduce data transfer charges", "isCorrect": false}, {"text": "Implement CloudFront CDN with S3 origin", "isCorrect": true}, {"text": "Move EC2 instances to regions with lower data transfer costs", "isCorrect": false}, {"text": "Compress all data before transmission", "isCorrect": false}]	Implement CloudFront CDN with S3 origin	CloudFront eliminates most internet data transfer charges and caches static content globally, dramatically reducing costs for software downloads.	{"summary": "CloudFront cost optimization for static content:", "breakdown": ["No data transfer charges between S3 and CloudFront", "Reduced CloudFront to internet rates vs EC2 to internet", "Caching reduces origin data transfer for repeated downloads", "Global edge locations improve performance and reduce costs"], "otherOptions": "VPC endpoints help AWS service costs, not internet transfer\\nRegional pricing differences minimal for internet transfer\\nCompression helps but doesn't address core transfer costs"}	\N
16	116	16	AWS Architecture - Decoupling	Application	Design Resilient Architectures	A photo processing application receives images via API, processes them, and stores results. During peak times, the processing queue becomes backlogged and API requests start failing. Which decoupling strategy provides the BEST resilience?	[{"text": "Increase EC2 instance size for faster processing", "isCorrect": false}, {"text": "Use SQS queue between API and processing with Auto Scaling based on queue depth", "isCorrect": true}, {"text": "Add more API Gateway throttling to reduce incoming requests", "isCorrect": false}, {"text": "Use Lambda functions instead of EC2 for processing", "isCorrect": false}]	Use SQS queue between API and processing with Auto Scaling based on queue depth	SQS decouples components allowing the API to accept requests even when processing is slow, with Auto Scaling adding capacity based on queue backlog.	{"summary": "SQS decoupling benefits:", "breakdown": ["API remains responsive during processing delays", "Queue acts as buffer for variable processing times", "Auto Scaling responds to actual demand (queue depth)", "No lost requests during traffic spikes"], "otherOptions": "Larger instances don't solve architecture coupling\\nThrottling causes request failures\\nLambda has 15-minute timeout limit for processing"}	\N
17	117	17	AWS Performance - Auto Scaling	Expert	Design High-Performing Architectures	An application experiences sudden traffic spikes that cause Auto Scaling to launch instances, but by the time instances are ready, traffic has decreased. This causes unnecessary costs. Which scaling strategy provides the BEST optimization?	[{"text": "Use predictive scaling with machine learning to anticipate traffic patterns", "isCorrect": true}, {"text": "Decrease Auto Scaling cooldown periods for faster scaling", "isCorrect": false}, {"text": "Use larger instance types that can handle traffic spikes", "isCorrect": false}, {"text": "Implement step scaling with smaller scaling increments", "isCorrect": false}]	Use predictive scaling with machine learning to anticipate traffic patterns	Predictive scaling uses machine learning to forecast traffic and pre-scale capacity, avoiding the lag time of reactive scaling.	{"summary": "Predictive scaling advantages:", "breakdown": ["ML forecasts traffic patterns to pre-scale capacity", "Reduces lag time between traffic spike and available capacity", "Prevents over-scaling by understanding traffic duration", "Works with historical patterns and scheduled events"], "otherOptions": "Faster scaling still reactive, doesn't solve core timing issue\\nOver-provisioning increases costs unnecessarily\\nStep scaling still reactive to traffic changes"}	\N
19	119	19	AWS Architecture - Service Limits	Expert	Design High-Performing Architectures	An application needs to upload files up to 500GB to S3. Users report upload failures and timeouts. The application currently uses single PUT requests. Which solution addresses the root cause of the upload failures?	[{"text": "Enable S3 Transfer Acceleration for faster uploads", "isCorrect": false}, {"text": "Implement S3 multipart upload for large files", "isCorrect": true}, {"text": "Use CloudFront for upload optimization", "isCorrect": false}, {"text": "Increase application timeout settings", "isCorrect": false}]	Implement S3 multipart upload for large files	S3 has a 5GB limit for single PUT operations. Files larger than 5GB require multipart upload, which is recommended for files over 100MB.	{"summary": "S3 upload limits and best practices:", "breakdown": ["Single PUT limit: 5GB maximum file size", "Multipart upload: Required for files >5GB, recommended >100MB", "500GB files: Must use multipart upload", "Improves reliability with retry capability per part"], "otherOptions": "Transfer Acceleration helps speed, doesn't solve size limit\\nCloudFront not designed for large file uploads\\nTimeouts don't solve the 5GB PUT limit"}	\N
20	120	20	AWS Architecture - Lambda Limitations	Expert	Design Resilient Architectures	A data processing function needs to run for 20 minutes to process large datasets. The current Lambda function times out after 15 minutes. Which solution provides the BEST architecture for this requirement?	[{"text": "Increase Lambda timeout to maximum 15 minutes and optimize code", "isCorrect": false}, {"text": "Break processing into smaller Lambda functions using Step Functions", "isCorrect": true}, {"text": "Use Lambda with SQS to process data in smaller chunks", "isCorrect": false}, {"text": "Migrate to ECS Fargate for longer-running tasks", "isCorrect": false}]	Break processing into smaller Lambda functions using Step Functions	Lambda has a hard 15-minute timeout limit. Step Functions can orchestrate multiple Lambda functions to handle longer workflows.	{"summary": "Lambda timeout limits and solutions:", "breakdown": ["Lambda maximum timeout: 15 minutes (hard limit)", "Step Functions: Can orchestrate multi-step workflows", "Break down processing: Multiple Lambda functions in sequence", "State management: Step Functions handles workflow state"], "otherOptions": "15 minutes is the maximum timeout\\nSQS helps with async processing but doesn't solve timeout\\nECS Fargate valid but Step Functions more serverless-native"}	\N
88	187	87	Performance - Database Read Scaling	Advanced	Design High-Performing Architectures	An analytics application runs complex read-heavy queries on a 10TB database. Read traffic varies from 1,000 to 50,000 queries per minute. Write traffic is minimal but requires immediate consistency. Which solution provides optimal read scaling?	[{"text": "Aurora with Auto Scaling read replicas and reader endpoint", "isCorrect": true}, {"text": "RDS with manual read replica provisioning", "isCorrect": false}, {"text": "DynamoDB with Global Secondary Indexes", "isCorrect": false}, {"text": "ElastiCache Redis cluster with read-through caching", "isCorrect": false}]	Aurora with Auto Scaling read replicas and reader endpoint	Aurora Auto Scaling automatically adjusts read replica count based on CPU utilization, reader endpoint distributes read traffic, and maintains strong consistency for writes.	{"summary": "Aurora read scaling advantages:", "breakdown": ["Auto Scaling replicas: Automatically adjusts from 1-15 replicas based on demand", "Reader endpoint: Automatic load balancing across read replicas", "Strong consistency: Immediate consistency for writes", "10TB capacity: Aurora scales storage automatically"], "otherOptions": "Manual scaling can't respond to 50x traffic variation efficiently\\nDynamoDB requires data model changes and lacks SQL compatibility\\nCaching adds complexity and potential consistency issues"}	\N
89	188	88	Performance - Edge Computing	Expert	Design High-Performing Architectures	A gaming platform needs sub-10ms response times for real-time multiplayer games globally. Traditional CloudFront caching cannot cache dynamic game state. Which solution provides the LOWEST latency for dynamic content?	[{"text": "Lambda@Edge with DynamoDB Global Tables", "isCorrect": true}, {"text": "CloudFront with origin shield and multiple origins", "isCorrect": false}, {"text": "Global Accelerator with multiple regional endpoints", "isCorrect": false}, {"text": "Route 53 latency-based routing with regional ALBs", "isCorrect": false}]	Lambda@Edge with DynamoDB Global Tables	Lambda@Edge runs code at CloudFront edge locations (400+ globally), DynamoDB Global Tables provide millisecond latency data access, enabling sub-10ms response times for dynamic content.	{"summary": "Edge computing for ultra-low latency:", "breakdown": ["Lambda@Edge: Code execution at 400+ edge locations", "DynamoDB Global Tables: Single-digit millisecond data access", "Dynamic content: Real-time game state processing at edge", "Global coverage: Reduces round-trip time to nearest edge"], "otherOptions": "Origin shield doesn't solve dynamic content latency\\nGlobal Accelerator improves network path but doesn't cache dynamic content\\nRoute 53 routing still requires round-trip to regional data centers"}	\N
90	189	89	Performance - Auto Scaling Optimization	Advanced	Design High-Performing Architectures	A video processing application experiences unpredictable spikes where demand can increase 20x within 5 minutes. Current Auto Scaling cannot respond fast enough, causing user timeouts. Which optimization provides the FASTEST scaling response?	[{"text": "Predictive scaling with custom CloudWatch metrics", "isCorrect": false}, {"text": "Pre-warm instances with warm pools and lifecycle hooks", "isCorrect": true}, {"text": "Larger instance types to handle peak load", "isCorrect": false}, {"text": "Scheduled scaling based on historical patterns", "isCorrect": false}]	Pre-warm instances with warm pools and lifecycle hooks	Warm pools maintain pre-initialized instances in stopped state, lifecycle hooks customize instance preparation, enabling sub-minute scaling response compared to cold instance launches.	{"summary": "Warm pool scaling optimization:", "breakdown": ["Warm pools: Pre-initialized instances ready in 30-60 seconds", "Lifecycle hooks: Custom preparation scripts for faster service startup", "Cost effective: Stopped instances have minimal EBS charges", "Unpredictable spikes: Always-ready capacity for immediate scaling"], "otherOptions": "Predictive scaling can't predict truly unpredictable spikes\\nOver-provisioning wastes cost during normal periods\\nScheduled scaling doesn't handle unpredictable demand"}	\N
91	190	90	Performance - Database Query Optimization	Expert	Design High-Performing Architectures	A data warehouse runs complex analytical queries that take 30+ minutes to complete. The queries scan 100TB+ datasets but only return aggregated results. Users need interactive query performance (<5 seconds). Which solution provides the BEST query acceleration?	[{"text": "Amazon Redshift with materialized views and result caching", "isCorrect": true}, {"text": "Athena with partitioned data and columnar formats", "isCorrect": false}, {"text": "Aurora with query plan caching and read replicas", "isCorrect": false}, {"text": "DynamoDB with pre-computed aggregation tables", "isCorrect": false}]	Amazon Redshift with materialized views and result caching	Redshift materialized views pre-compute complex aggregations, result caching returns identical queries instantly, and columnar storage with compression optimizes scan performance.	{"summary": "Data warehouse query acceleration:", "breakdown": ["Materialized views: Pre-computed aggregations refresh automatically", "Result caching: Identical queries return in milliseconds", "Columnar storage: Optimized for analytical query patterns", "Redshift Spectrum: Can query 100TB+ datasets efficiently"], "otherOptions": "Athena still requires scanning large datasets for complex aggregations\\nAurora optimized for OLTP, not analytical workloads\\nDynamoDB requires complete data model restructuring"}	\N
68	168	68	Security - Data Encryption Strategy	Advanced	Design Secure Architectures	A healthcare company processes PHI data and requires encryption at rest and in transit with full key management control. Which combination meets HIPAA compliance requirements? (Choose TWO)	[{"text": "S3 with SSE-S3 default encryption", "isCorrect": false}, {"text": "Customer-managed KMS keys with key rotation", "isCorrect": true}, {"text": "EBS volumes with AWS-managed encryption", "isCorrect": false}, {"text": "VPC endpoints for service communication", "isCorrect": true}, {"text": "Application Load Balancer with ACM certificates", "isCorrect": false}]	Customer-managed KMS keys with key rotation, VPC endpoints for service communication	HIPAA requires customer control over encryption keys (customer-managed KMS) and secure data transmission (VPC endpoints keep traffic off public internet). SSE-S3 and AWS-managed keys do not provide sufficient control.	{"summary": "HIPAA compliance requirements:", "breakdown": ["Customer-managed KMS: Full control over key policies and access", "VPC endpoints: Keep data traffic within AWS network", "Key rotation: Automatic rotation for compliance", "Audit trails: CloudTrail logs all key usage"], "otherOptions": "SSE-S3 doesn't provide customer key control\\nAWS-managed keys insufficient for HIPAA\\nACM helps but doesn't address data at rest"}	\N
77	177	77	Security - IAM Policy Conditions	Expert	Design Secure Architectures	A company needs developers to access S3 buckets only during business hours (9 AM - 5 PM) and only from corporate IP addresses. The policy must automatically deny access outside these conditions. Which IAM policy condition combination provides the MOST secure implementation?	[{"text": "aws:CurrentTime and aws:SourceIp conditions with explicit deny", "isCorrect": true}, {"text": "aws:RequestedTime and aws:VpcSourceIp with allow statements", "isCorrect": false}, {"text": "aws:TokenIssueTime and aws:userid conditions only", "isCorrect": false}, {"text": "aws:SecureTransport and aws:MultiFactorAuthAge conditions", "isCorrect": false}]	aws:CurrentTime and aws:SourceIp conditions with explicit deny	aws:CurrentTime restricts access to business hours, aws:SourceIp limits to corporate IPs, and explicit deny statements ensure security even if other policies allow access.	{"summary": "IAM conditional access controls:", "breakdown": ["aws:CurrentTime: Time-based access control (9 AM - 5 PM)", "aws:SourceIp: IP-based access restriction", "Explicit deny: Cannot be overridden by other allow policies", "Defense in depth: Multiple condition types for comprehensive security"], "otherOptions": "aws:RequestedTime is not a valid condition key\\nMissing time and IP restrictions required by business\\nAddresses different security concerns, not time/location"}	\N
79	179	79	Security - Data Loss Prevention	Expert	Design Secure Architectures	A healthcare company needs to prevent accidental exposure of PHI in S3 buckets across 100+ AWS accounts. They require automated remediation, real-time alerts, and compliance reporting. Which comprehensive solution provides the BEST data protection?	[{"text": "Amazon Macie + EventBridge + Config Remediation + Security Hub", "isCorrect": true}, {"text": "AWS Config + CloudWatch + SNS + Manual remediation", "isCorrect": false}, {"text": "GuardDuty + CloudTrail + Lambda + Custom dashboard", "isCorrect": false}, {"text": "Inspector + Systems Manager + CloudFormation + Cost Explorer", "isCorrect": false}]	Amazon Macie + EventBridge + Config Remediation + Security Hub	Macie detects PHI and exposure risks, EventBridge provides real-time alerting, Config Remediation automatically fixes violations, Security Hub centralizes compliance reporting.	{"summary": "Comprehensive data protection architecture:", "breakdown": ["Macie: ML-powered PHI detection and exposure analysis", "EventBridge: Real-time alerts for immediate response", "Config Remediation: Automatic policy enforcement", "Security Hub: Centralized compliance dashboard across 100+ accounts"], "otherOptions": "Lacks automated PHI detection and remediation\\nGuardDuty focuses on threats, not data classification\\nInspector and Cost Explorer irrelevant for data protection"}	\N
21	121	21	AWS Cost Optimization - Compute Savings	Expert	Design Cost-Optimized Architectures	A company runs a web application with the following pattern: 2 instances minimum 24/7, scales to 10 instances during business hours (9 AM-5 PM weekdays), and spikes to 50 instances during monthly sales (first 3 days). Current monthly On-Demand costs are $15,000. Which purchasing strategy provides MAXIMUM cost savings while maintaining performance?	[{"text": "All-Upfront Reserved Instances for 10 instances, On-Demand for the rest", "isCorrect": false}, {"text": "Reserved Instances for 2 instances, Savings Plans for business hours capacity, Spot for sales spikes", "isCorrect": true}, {"text": "Compute Savings Plans for all capacity needs", "isCorrect": false}, {"text": "Scheduled Reserved Instances for business hours, On-Demand for spikes", "isCorrect": false}]	Reserved Instances for 2 instances, Savings Plans for business hours capacity, Spot for sales spikes	Layered purchasing strategy: RIs for baseline (70% savings), Savings Plans for predictable scaling (66% savings), Spot for spikes (up to 90% savings).	{"summary": "Optimal cost strategy for variable workloads:", "breakdown": ["Reserved Instances: 2 baseline instances (24/7) - 70% savings", "Savings Plans: 8 additional instances for business hours - 66% savings", "Spot Instances: 40 instances for monthly sales - up to 90% savings", "Total savings: Approximately 75% reduction from $15,000"], "otherOptions": "Over-provisioning RIs for 10 instances wastes money nights/weekends\\nSavings Plans less discount than RIs for baseline\\nScheduled RIs discontinued, wouldn't cover spikes"}	\N
22	122	22	AWS Cost Optimization - Data Processing	Application	Design Cost-Optimized Architectures	A data analytics company processes 10TB of log files daily using EMR clusters. Processing takes 4 hours and runs once daily at 2 AM. The current approach uses On-Demand instances costing $8,000/month. Which architecture change provides the BEST cost optimization?	[{"text": "Migrate to AWS Glue for serverless ETL processing", "isCorrect": false}, {"text": "Use EMR on Spot Instances with diverse instance types across multiple AZs", "isCorrect": true}, {"text": "Pre-process data with Lambda before EMR to reduce cluster size", "isCorrect": false}, {"text": "Use smaller EMR cluster running 24/7 with Reserved Instances", "isCorrect": false}]	Use EMR on Spot Instances with diverse instance types across multiple AZs	EMR on Spot Instances can provide 50-80% cost savings for batch processing workloads that can tolerate interruptions.	{"summary": "EMR Spot Instance optimization:", "breakdown": ["Spot savings: 50-80% reduction from On-Demand pricing", "Instance diversity: Reduces interruption risk", "Multiple AZs: Ensures capacity availability", "4-hour processing window: Perfect for Spot reliability"], "otherOptions": "Glue more expensive for 10TB daily processing\\nLambda timeout and cost limitations for TB-scale data\\n24/7 cluster wasteful for 4-hour daily job"}	\N
23	123	23	AWS Cost Optimization - Storage Optimization	Expert	Design Cost-Optimized Architectures	A media company stores 500TB of video content in S3 Standard. Analytics show: 10% accessed daily (hot), 30% accessed weekly (warm), 60% accessed rarely but unpredictably. Retrieval must complete within 12 hours. Current cost is $11,500/month. Which storage strategy provides optimal cost reduction?	[{"text": "S3 Intelligent-Tiering for all content with archive configuration", "isCorrect": true}, {"text": "S3 Standard for hot, S3 IA for warm, S3 Glacier for cold data", "isCorrect": false}, {"text": "S3 One Zone-IA for all content to reduce costs", "isCorrect": false}, {"text": "S3 Standard for hot, S3 Glacier Instant Retrieval for all other content", "isCorrect": false}]	S3 Intelligent-Tiering for all content with archive configuration	S3 Intelligent-Tiering automatically optimizes storage costs for unpredictable access patterns without retrieval fees, perfect for this use case.	{"summary": "Intelligent-Tiering benefits for unpredictable access:", "breakdown": ["Automatic tiering: No manual lifecycle policies needed", "No retrieval fees: Critical for unpredictable access", "Archive configuration: Moves rarely accessed objects to archive tiers", "Cost savings: Up to 70% reduction while maintaining performance"], "otherOptions": "Manual lifecycle rules don't handle unpredictable access well\\nOne Zone-IA risky for 500TB of valuable content\\nGlacier Instant Retrieval has retrieval fees for 60% of content"}	\N
26	126	26	AWS Architecture - Event-Driven Resilience	Expert	Design Resilient Architectures	An e-commerce platform must process orders from multiple channels (web, mobile, partners) with different formats. The system must handle 50,000 orders/hour during sales, ensure no order loss, and enable new channel integration without affecting existing ones. Which architecture provides the BEST resilience and scalability?	[{"text": "API Gateway with Lambda functions writing directly to DynamoDB", "isCorrect": false}, {"text": "Amazon EventBridge with channel-specific rules routing to SQS queues and containerized processors", "isCorrect": true}, {"text": "Kinesis Data Streams with Lambda consumers for each channel", "isCorrect": false}, {"text": "Application Load Balancer with Auto Scaling EC2 instances", "isCorrect": false}]	Amazon EventBridge with channel-specific rules routing to SQS queues and containerized processors	EventBridge provides event routing with schema registry, SQS ensures message durability, and containerized processors enable independent scaling per channel.	{"summary": "Event-driven architecture benefits:", "breakdown": ["EventBridge: Central event router with content-based filtering", "Schema Registry: Validates events and enables discovery", "SQS queues: Guaranteed message delivery and buffering", "Container isolation: Each channel processor scales independently"], "otherOptions": "Direct writes risk data loss during high load\\nKinesis overkill for transactional data, complex scaling\\nMonolithic approach, difficult to add new channels"}	\N
27	127	27	AWS Architecture - Disaster Recovery	Application	Design Resilient Architectures	A financial services application requires RPO of 1 hour and RTO of 4 hours. The application uses Aurora PostgreSQL, EC2 instances with custom AMIs, and stores documents in S3. The DR solution must be cost-effective. Which approach meets these requirements?	[{"text": "Aurora Global Database with standby EC2 instances in secondary region", "isCorrect": false}, {"text": "Aurora backups to secondary region, AMIs copied cross-region, S3 cross-region replication, and AWS Backup", "isCorrect": true}, {"text": "Multi-region active-active deployment with Route 53 failover", "isCorrect": false}, {"text": "Database snapshots and EC2 snapshots copied hourly to secondary region", "isCorrect": false}]	Aurora backups to secondary region, AMIs copied cross-region, S3 cross-region replication, and AWS Backup	Backup-based DR meets RPO/RTO requirements cost-effectively: Aurora automated backups (continuous), AMIs ready for quick launch, S3 CRR for documents.	{"summary": "Cost-effective DR strategy components:", "breakdown": ["Aurora backups: Point-in-time recovery within 5 minutes (meets 1-hour RPO)", "Cross-region AMI copies: Ready for quick EC2 launch", "S3 CRR: Near real-time replication for documents", "AWS Backup: Centralized backup management and compliance"], "otherOptions": "Global Database expensive for 4-hour RTO requirement\\nActive-active overkill and complex for these requirements\\nManual snapshots miss continuous data changes"}	\N
28	128	28	AWS Performance - API Optimization	Application	Design High-Performing Architectures	A mobile app backend serves personalized content to 10 million users globally. The API Gateway shows high latency for data aggregation from multiple microservices. Each request makes 5-7 service calls with 200ms average latency per call. Which solution provides the BEST performance improvement?	[{"text": "Implement API Gateway caching for all endpoints", "isCorrect": false}, {"text": "Use AWS AppSync with GraphQL to batch and parallelize service calls", "isCorrect": true}, {"text": "Add ElastiCache in front of each microservice", "isCorrect": false}, {"text": "Increase Lambda memory allocation for faster processing", "isCorrect": false}]	Use AWS AppSync with GraphQL to batch and parallelize service calls	AppSync with GraphQL reduces API calls through query batching and parallel resolution, dramatically reducing overall latency for aggregated data.	{"summary": "AppSync optimization benefits:", "breakdown": ["Query batching: Single request instead of 5-7 calls", "Parallel resolution: Service calls execute simultaneously", "Reduced latency: From 1000-1400ms to 200-300ms total", "Caching built-in: Further performance improvements"], "otherOptions": "Caching doesn't help with personalized content\\nStill requires serial service calls\\nLambda memory doesn't solve aggregation latency"}	\N
29	129	29	AWS Performance - ML Inference	Expert	Design High-Performing Architectures	A video streaming platform needs real-time content moderation using computer vision ML models. They process 1,000 concurrent streams with <100ms inference latency requirement. Current SageMaker endpoint shows 300ms latency. Which optimization provides the required performance?	[{"text": "Use larger SageMaker instance types with more GPUs", "isCorrect": false}, {"text": "Deploy models to Lambda with Provisioned Concurrency", "isCorrect": false}, {"text": "Implement SageMaker multi-model endpoints with edge deployment using AWS IoT Greengrass", "isCorrect": false}, {"text": "Use Amazon EC2 Inf1 instances with AWS Neuron for optimized inference", "isCorrect": true}]	Use Amazon EC2 Inf1 instances with AWS Neuron for optimized inference	EC2 Inf1 instances with AWS Inferentia chips provide the lowest latency and highest throughput for real-time ML inference at scale.	{"summary": "Inf1 instance optimization benefits:", "breakdown": ["AWS Inferentia chips: Purpose-built for ML inference", "Sub-100ms latency: Achievable with optimized models", "High throughput: Handle 1,000 concurrent streams", "Cost-effective: Up to 70% lower cost than GPU instances"], "otherOptions": "Larger instances don't guarantee latency reduction\\nLambda cold starts and limits prevent consistent <100ms\\nEdge deployment adds complexity without meeting core requirement"}	\N
30	130	30	AWS Cost Optimization - Hybrid Architecture	Expert	Design Cost-Optimized Architectures	A company wants to modernize their on-premises data warehouse (50Tto AWS. They need to maintain some on-premises reporting tools while enabling cloud analytics. Budget is limited to $5,000/month. Query patterns: 20% hot data (daily), 30% warm (weekly), 50% cold (monthly). Which architecture provides the BEST cost optimization?	[{"text": "Migrate everything to Redshift with Reserved Instances", "isCorrect": false}, {"text": "Use Redshift for hot data, Redshift Spectrum for warm/cold data in S3, with AWS Direct Connect", "isCorrect": true}, {"text": "Keep all data on-premises and use Athena federated queries", "isCorrect": false}, {"text": "Use RDS PostgreSQL with read replicas for all data", "isCorrect": false}]	Use Redshift for hot data, Redshift Spectrum for warm/cold data in S3, with AWS Direct Connect	Tiered approach with Redshift for hot data and Spectrum for S3-based warm/cold data provides optimal price-performance within budget constraints.	{"summary": "Hybrid data warehouse optimization:", "breakdown": ["Redshift (10TB): ~$2,500/month for hot data performance", "S3 + Spectrum (40TB): ~$1,000/month for warm/cold storage", "Direct Connect: ~$1,500/month for reliable hybrid connectivity", "Total: ~$5,000/month within budget with optimal performance"], "otherOptions": "Full Redshift for 50TB exceeds budget significantly\\nAthena federated queries too slow for hot data needs\\nRDS not optimized for analytics workloads, would exceed budget"}	\N
31	131	31	AWS Cost Optimization - Serverless Economics	Expert	Design Cost-Optimized Architectures	A news aggregation service processes 50 million articles monthly with unpredictable spikes during breaking news (up to 10x normal load). Current EC2-based architecture costs $12,000/month and struggles with spike handling. Articles average 5KB, require text analysis, and must be searchable within 1 minute. Which serverless architecture provides the best cost optimization while meeting requirements?	[{"text": "API Gateway  Lambda  DynamoDB with ElasticSearch", "isCorrect": false}, {"text": "EventBridge  SQS  Lambda with concurrent execution limits  S3  Athena with Glue crawlers", "isCorrect": true}, {"text": "Kinesis Data Streams  Kinesis Analytics  RDS", "isCorrect": false}, {"text": "Step Functions orchestrating multiple Lambda functions  Aurora Serverless", "isCorrect": false}]	EventBridge  SQS  Lambda with concurrent execution limits  S3  Athena with Glue crawlers	EventBridge with SQS provides event buffering for spikes, Lambda with concurrency limits controls costs, S3 offers cheap storage, and Athena enables SQL searches without database costs.	{"summary": "Serverless architecture cost optimization:", "breakdown": ["EventBridge + SQS: Handles 10x spikes without over-provisioning", "Lambda concurrency limits: Prevents runaway costs during spikes", "S3 storage: $0.023/GB vs database storage at $0.10/GB", "Athena queries: Pay-per-query ($5/TB scanned) vs always-on database"], "otherOptions": "ElasticSearch cluster costs $3000+/month minimum\\nKinesis Analytics expensive for sporadic spikes\\nAurora Serverless still has minimum capacity costs"}	\N
32	132	32	AWS Cost Optimization - Container Right-Sizing	Application	Design Cost-Optimized Architectures	A microservices application runs 40 containers on ECS with varying resource needs: 10 containers need 4 vCPU/8GB (API servers), 20 need 1 vCPU/2GB (workers), and 10 need 0.5 vCPU/1GB (sidecars). Current costs using m5.4xlarge instances are $8,000/month with 35% CPU utilization. Which optimization strategy provides maximum cost savings?	[{"text": "Switch to larger instances for better CPU:memory ratio", "isCorrect": false}, {"text": "Use ECS capacity providers with mixed instance types and Spot for workers", "isCorrect": true}, {"text": "Migrate all containers to Fargate with right-sized task definitions", "isCorrect": false}, {"text": "Implement cluster auto-scaling based on CPU utilization", "isCorrect": false}]	Use ECS capacity providers with mixed instance types and Spot for workers	Capacity providers enable mixing instance types optimized for different workloads, while Spot instances for stateless workers can reduce costs by 70-90%.	{"summary": "Container cost optimization strategy:", "breakdown": ["Mixed instances: c5 for CPU-intensive, r5 for memory-intensive", "Spot for workers: 70% savings on 20 containers (stateless)", "Better bin packing: Increases utilization from 35% to 75%", "Total savings: Approximately 60% reduction from $8,000"], "otherOptions": "Larger instances worsen utilization problems\\nFargate more expensive for predictable workloads\\nAuto-scaling doesn't address instance type mismatch"}	\N
34	134	34	AWS Architecture - IoT and Edge	Expert	Design Resilient Architectures	A manufacturing company operates 500 factories with 100 IoT sensors each, generating 1KB messages every second. They need local anomaly detection with sub-second response, cloud-based analytics, and must operate during internet outages. Which architecture provides the required edge computing capabilities?	[{"text": "Direct IoT Core ingestion with Kinesis Analytics for anomaly detection", "isCorrect": false}, {"text": "AWS IoT Greengrass with Lambda functions, local message routing, and intermittent cloud sync", "isCorrect": true}, {"text": "EC2 instances in each factory with VPN connections to cloud", "isCorrect": false}, {"text": "AWS Outposts in each factory for local processing", "isCorrect": false}]	AWS IoT Greengrass with Lambda functions, local message routing, and intermittent cloud sync	IoT Greengrass enables local Lambda execution for sub-second anomaly detection, continues operating offline, and synchronizes with cloud when connected.	{"summary": "Edge computing with IoT Greengrass benefits:", "breakdown": ["Local Lambda: Sub-second anomaly detection at edge", "Offline operation: Continues processing during outages", "Efficient sync: Batches data for cloud analytics when connected", "Cost-effective: No need for servers in 500 locations"], "otherOptions": "Requires constant internet, no local processing\\nManaging 500 EC2 instances operationally complex\\nOutposts overkill and expensive for IoT workloads"}	\N
70	170	70	Resilience - Disaster Recovery Strategy	Intermediate	Design Resilient Architectures	A financial application requires RTO of 4 hours and RPO of 1 hour. The application uses EC2, RDS MySQL, and S3. Which DR strategy meets these requirements MOST cost-effectively?	[{"text": "Pilot light with automated failover scripts", "isCorrect": true}, {"text": "Warm standby with Multi-AZ RDS in secondary region", "isCorrect": false}, {"text": "Hot standby with real-time replication", "isCorrect": false}, {"text": "Backup and restore with scheduled snapshots", "isCorrect": false}]	Pilot light with automated failover scripts	Pilot light maintains core infrastructure (like RDS read replica) in secondary region, can meet 4-hour RTO with automation, and is more cost-effective than warm/hot standby while exceeding backup/restore capabilities.	{"summary": "Pilot light DR strategy:", "breakdown": ["Maintains core infrastructure (database replicas)", "Automated deployment scripts for rapid scaling", "Meets 4-hour RTO with proper automation", "More cost-effective than hot standby"], "otherOptions": "Warm standby is expensive and exceeds requirements\\nHot standby is expensive and exceeds requirements\\nBackup/restore may not meet 4-hour RTO"}	\N
35	135	35	AWS Cost Optimization - Streaming Data	Application	Design Cost-Optimized Architectures	A ride-sharing app ingests 100,000 location updates per second during peak hours (6-9 AM, 5-8 PM) but only 5,000 per second during off-peak. Current Kinesis Data Streams with provisioned shards costs $15,000/month. Real-time processing is required only during peak hours; batch processing is acceptable off-peak. Which architecture reduces costs while meeting requirements?	[{"text": "Kinesis Data Streams with auto-scaling shards based on traffic", "isCorrect": false}, {"text": "Kinesis Data Streams On-Demand for peaks, Kinesis Firehose to S3 for off-peak batch", "isCorrect": true}, {"text": "Replace with SQS queues and Lambda functions", "isCorrect": false}, {"text": "DynamoDB Streams with Lambda triggers", "isCorrect": false}]	Kinesis Data Streams On-Demand for peaks, Kinesis Firehose to S3 for off-peak batch	Kinesis On-Demand eliminates over-provisioning costs while Firehose to S3 provides cost-effective batch processing during off-peak hours.	{"summary": "Streaming cost optimization strategy:", "breakdown": ["Kinesis On-Demand: Pay only for actual throughput", "Peak hours: Real-time processing as required", "Off-peak Firehose: 90% cheaper than real-time streams", "Estimated savings: 65% reduction from $15,000/month"], "otherOptions": "Auto-scaling still requires minimum shards\\nSQS lacks streaming semantics for real-time\\nDynamoDB Streams tied to table operations"}	\N
36	136	36	AWS Architecture - Multi-Region	Expert	Design Resilient Architectures	A global trading platform requires <10ms latency for 99% of users across US, EU, and APAC regions. The platform processes 1 million trades daily with strict consistency requirements. Each trade must be reflected globally within 100ms. Current single-region deployment shows 150ms+ latency for distant users. Which multi-region architecture best meets these requirements?	[{"text": "Deploy full stack in each region with eventual consistency", "isCorrect": false}, {"text": "Aurora Global Database with local read replicas, Route 53 geolocation routing, and DynamoDB Global Tables for session data", "isCorrect": true}, {"text": "Single region with CloudFront caching for static content", "isCorrect": false}, {"text": "Active-active regions with asynchronous replication", "isCorrect": false}]	Aurora Global Database with local read replicas, Route 53 geolocation routing, and DynamoDB Global Tables for session data	Aurora Global Database provides <1 second replication with strong consistency, local read replicas ensure <10ms reads, while DynamoDB Global Tables handle session state with automatic conflict resolution.	{"summary": "Multi-region architecture for low latency trading:", "breakdown": ["Aurora Global: <1 second cross-region replication", "Local read replicas: <10ms read latency per region", "Write forwarding: Maintains consistency for trades", "DynamoDB Global Tables: Multi-master for session data"], "otherOptions": "Eventual consistency violates trade requirements\\nCloudFront doesn't help with dynamic trade data\\nAsync replication can't guarantee 100ms consistency"}	\N
38	138	38	AWS Performance - Hybrid Networking	Application	Design High-Performing Architectures	A media company needs to transfer 50TB of daily video content from on-premises editing stations to S3 for processing. Current internet upload takes 20 hours, causing production delays. The solution must complete transfers within 4 hours and support 100 concurrent editor workstations. Which approach best meets these performance requirements?	[{"text": "Multiple Site-to-Site VPN connections with ECMP", "isCorrect": false}, {"text": "AWS Direct Connect with Virtual Interfaces and S3 Transfer Acceleration", "isCorrect": true}, {"text": "Storage Gateway File Gateway with local cache", "isCorrect": false}, {"text": "Daily AWS Snowball Edge shipments", "isCorrect": false}]	AWS Direct Connect with Virtual Interfaces and S3 Transfer Acceleration	Direct Connect provides dedicated bandwidth for consistent transfer speeds, VIFs enable private S3 connectivity, and Transfer Acceleration optimizes the upload path for concurrent workstations.	{"summary": "High-performance hybrid transfer solution:", "breakdown": ["Direct Connect: 10Gbps dedicated bandwidth", "VIF to S3: Private connectivity, no internet congestion", "Transfer Acceleration: Optimizes for 100 concurrent uploads", "50TB in 4 hours: Requires ~3.5Gbps sustained throughput"], "otherOptions": "VPN limited to ~1.25Gbps aggregate bandwidth\\nFile Gateway adds latency and cache limitations\\nSnowball shipping time exceeds 4-hour requirement"}	\N
39	139	39	AWS Architecture - Event-Driven Serverless	Expert	Design High-Performing Architectures	An e-commerce platform needs to process order events with multiple downstream systems: inventory (must process once), shipping (at-least-once), analytics (can tolerate duplicates), and email (exactly-once). Order volume varies from 100/minute to 10,000/minute during flash sales. Which event-driven architecture ensures correct delivery semantics for each consumer?	[{"text": "SNS with SQS queues for all consumers", "isCorrect": false}, {"text": "EventBridge with rules routing to SQS FIFO for inventory, standard SQS for shipping, Kinesis for analytics, and Step Functions for email", "isCorrect": true}, {"text": "Kinesis Data Streams with Lambda consumers", "isCorrect": false}, {"text": "Direct Lambda invocations with error handling", "isCorrect": false}]	EventBridge with rules routing to SQS FIFO for inventory, standard SQS for shipping, Kinesis for analytics, and Step Functions for email	EventBridge provides content-based routing, SQS FIFO ensures exactly-once processing, standard SQS handles at-least-once, Kinesis supports analytics replay, and Step Functions manages email workflow state.	{"summary": "Event delivery semantics per consumer:", "breakdown": ["SQS FIFO: Exactly-once for critical inventory updates", "Standard SQS: At-least-once with retry for shipping", "Kinesis: Replay capability for analytics reprocessing", "Step Functions: Manages email state to prevent duplicates"], "otherOptions": "SNS+SQS doesn't provide exactly-once semantics\\nKinesis requires all consumers to process in order\\nDirect invocations lack delivery guarantees"}	\N
40	140	40	AWS Cost Optimization - Modern Architectures	Expert	Design Cost-Optimized Architectures	A SaaS company runs 200 customer environments, each with identical architecture: ALB, 2-10 EC2 instances, RDS MySQL, and 50GB of S3 storage. Monthly costs are $400,000 with only 30% resource utilization. Customers require isolation for compliance. Which modernization approach provides maximum cost reduction while maintaining isolation?	[{"text": "Containerize applications and run all customers on shared EKS cluster", "isCorrect": false}, {"text": "AWS App Runner with customer-specific environments, Aurora Serverless v2, and S3 bucket policies", "isCorrect": true}, {"text": "Lambda functions with RDS Proxy and separate VPCs", "isCorrect": false}, {"text": "Consolidate all customers into a single multi-tenant application", "isCorrect": false}]	AWS App Runner with customer-specific environments, Aurora Serverless v2, and S3 bucket policies	App Runner provides serverless compute with environment isolation, Aurora Serverless v2 scales database resources based on actual usage, eliminating overprovisioning while maintaining compliance isolation.	{"summary": "Serverless multi-tenant cost optimization:", "breakdown": ["App Runner: Pay-per-request pricing, no idle EC2 costs", "Aurora Serverless v2: Scales down to 0.5 ACU when idle", "Environment isolation: Separate App Runner services per customer", "Estimated savings: 70% reduction through utilization-based pricing"], "otherOptions": "Shared cluster violates compliance isolation requirements\\nLambda cold starts problematic for web applications\\nMulti-tenant architecture breaks compliance requirements"}	\N
45	145	45	AWS S3 Deep Dive	Expert	Design Cost-Optimized Architectures	A content delivery platform stores 200TB of video files with unpredictable access patterns. Analytics show some videos remain popular for months while others are rarely accessed after the first week. They need cost optimization without performance penalties for popular content. Which S3 storage strategy provides optimal cost savings?	[{"text": "Lifecycle policy transitioning all content to S3 IA after 30 days", "isCorrect": false}, {"text": "S3 Intelligent-Tiering with Deep Archive Access tier enabled", "isCorrect": true}, {"text": "Keep popular content in Standard, move others to Glacier based on view counts", "isCorrect": false}, {"text": "Use S3 One Zone-IA for all content to reduce costs", "isCorrect": false}]	S3 Intelligent-Tiering with Deep Archive Access tier enabled	S3 Intelligent-Tiering automatically optimizes costs based on access patterns without retrieval fees for frequent/infrequent tiers, perfect for unpredictable patterns.	{"summary": "Intelligent-Tiering benefits for unpredictable access:", "breakdown": ["Automatic optimization: No manual lifecycle policies needed", "No retrieval fees: For frequent and infrequent access tiers", "Deep Archive tier: Additional savings for rarely accessed content", "Performance maintained: Popular content stays in frequent tier"], "otherOptions": "Fixed lifecycle doesn't handle unpredictable patterns\\nManual management based on view counts is complex and error-prone\\nOne Zone-IA lacks resilience for valuable content"}	\N
50	150	50	AWS File Systems Deep Dive	Expert	Design High-Performing Architectures	A Windows-based application requires high-performance file storage with Active Directory integration, SMB protocol support, and sub-millisecond latencies. The workload involves intensive random I/O operations on small files. Current on-premises Windows File Server achieves 100,000 IOPS. Which AWS solution provides equivalent performance?	[{"text": "Amazon FSx for Windows File Server with SSD storage", "isCorrect": true}, {"text": "Amazon EFS with Windows file gateway", "isCorrect": false}, {"text": "EC2 Windows instance with attached io2 EBS volumes", "isCorrect": false}, {"text": "Amazon FSx for Lustre with Windows connectivity", "isCorrect": false}]	Amazon FSx for Windows File Server with SSD storage	FSx for Windows File Server provides native SMB, Active Directory integration, SSD storage for high IOPS, and sub-millisecond latencies specifically optimized for Windows workloads.	{"summary": "FSx for Windows File Server benefits:", "breakdown": ["Native SMB: Full Windows file system compatibility", "Active Directory: Seamless user authentication and authorization", "SSD storage: Up to 100,000+ IOPS for random I/O workloads", "Sub-millisecond latency: Optimized for Windows application performance"], "otherOptions": "EFS doesn't natively support SMB or Active Directory\\nSelf-managed solution lacks native SMB optimization\\nFSx for Lustre designed for Linux HPC workloads, not Windows"}	\N
51	151	51	AWS Hybrid Storage Migration	Application	Design Resilient Architectures	A company needs to migrate 500TB of archival data from on-premises tape storage to AWS. The data is rarely accessed but must be retrievable within 12 hours when needed. The migration must complete within 3 months with minimal impact on existing network bandwidth. Which migration strategy is MOST efficient?	[{"text": "AWS DataSync over Direct Connect for gradual migration", "isCorrect": false}, {"text": "Multiple AWS Snowball Edge devices shipped in batches to S3 Glacier Deep Archive", "isCorrect": true}, {"text": "Storage Gateway Tape Gateway for gradual cloud migration", "isCorrect": false}, {"text": "AWS Transfer Family for secure file transfer", "isCorrect": false}]	Multiple AWS Snowball Edge devices shipped in batches to S3 Glacier Deep Archive	Snowball Edge devices handle large data volumes efficiently without network impact, while Glacier Deep Archive provides cost-effective storage with 12-hour retrieval.	{"summary": "Large-scale archival migration strategy:", "breakdown": ["Snowball Edge capacity: 80TB per device, 7 devices for 500TB", "No network impact: Physical data transfer bypasses internet", "Glacier Deep Archive: Lowest cost storage for rarely accessed data", "12-hour retrieval: Meets business requirement for archive access"], "otherOptions": "DataSync over network would take months and impact bandwidth\\nTape Gateway for ongoing hybrid, not one-time migration\\nTransfer Family not optimized for 500TB bulk migration"}	\N
76	176	76	Cost Optimization - Serverless Architecture	Advanced	Design Cost-Optimized Architectures	An image processing application receives 500 uploads daily with unpredictable timing. Processing takes 3-8 minutes per image and requires 4GB memory. Current EC2 instances run idle 85% of the time. Which solution optimizes costs?	[{"text": "Increase Lambda memory and extend timeout to 15 minutes", "isCorrect": false}, {"text": "Use SQS to queue jobs and process with ECS Fargate auto-scaling", "isCorrect": true}, {"text": "Implement Step Functions to coordinate multiple Lambda functions", "isCorrect": false}, {"text": "Use Spot Instances with Auto Scaling based on SQS queue depth", "isCorrect": false}]	Use SQS to queue jobs and process with ECS Fargate auto-scaling	Fargate provides right-sizing for memory-intensive, long-running tasks without server management, eliminating idle time costs with auto-scaling.	{"summary": "Serverless solution for long-running tasks:", "breakdown": ["SQS: reliable job queuing", "Fargate: no server management, pay per use", "Auto-scaling: eliminates idle time", "4GB memory support: handles resource requirements"], "otherOptions": "Lambda has 15-minute maximum execution time\\nStep Functions still limited by Lambda constraints\\nSpot Instances can be interrupted during processing"}	\N
61	161	61	AWS Data Lifecycle	Application	Design Cost-Optimized Architectures	A research institution stores genomics data with the following access pattern: analyzed intensively for 30 days, occasionally referenced for 6 months, and archived for 10 years for compliance. Current costs are $50,000/month in S3 Standard. Which lifecycle policy provides maximum cost optimization while meeting access requirements?	[{"text": "Standard for 30 days  IA at 30 days  Glacier at 180 days  Deep Archive at 1 year", "isCorrect": true}, {"text": "Intelligent-Tiering for all data with Deep Archive enabled", "isCorrect": false}, {"text": "Standard for 30 days  Glacier immediately at 30 days", "isCorrect": false}, {"text": "One Zone-IA for all data to reduce costs", "isCorrect": false}]	Standard for 30 days  IA at 30 days  Glacier at 180 days  Deep Archive at 1 year	Lifecycle transitions match access patterns: Standard for intensive analysis, IA for occasional access, Glacier for long-term storage, Deep Archive for compliance.	{"summary": "Optimized lifecycle transitions for research data:", "breakdown": ["Days 1-30: S3 Standard for intensive analysis", "Days 31-180: S3 IA for occasional reference (68% cost reduction)", "Days 181-365: S3 Glacier for backup storage (77% cost reduction)", "Years 1-10: Deep Archive for compliance (80% cost reduction)"], "otherOptions": "Intelligent-Tiering adds overhead for predictable access patterns\\nEarly Glacier transition makes occasional access expensive\\nOne Zone-IA lacks resilience for valuable research data"}	\N
62	162	62	AWS Data Lifecycle	Expert	Design Cost-Optimized Architectures	A media company has complex data lifecycle needs: news content (hot for 7 days, cold afterwards), evergreen content (unpredictable access), and archived footage (rare access but immediate retrieval when needed). They want automated optimization without management overhead. Which strategy handles all three patterns optimally?	[{"text": "Separate buckets with different lifecycle policies for each content type", "isCorrect": false}, {"text": "S3 Intelligent-Tiering with object tagging and tag-based lifecycle policies", "isCorrect": true}, {"text": "Manual lifecycle management based on content metadata", "isCorrect": false}, {"text": "Single lifecycle policy with shortest transition times", "isCorrect": false}]	S3 Intelligent-Tiering with object tagging and tag-based lifecycle policies	Intelligent-Tiering with object tags enables different optimization strategies per content type while maintaining automation and handling unpredictable access patterns.	{"summary": "Tag-based intelligent lifecycle management:", "breakdown": ["Object tagging: Classify content types (news, evergreen, archive)", "Intelligent-Tiering: Automatic optimization for unpredictable evergreen content", "Tag-based policies: Specific rules for news (7-day pattern) and archive content", "No management overhead: All transitions automated based on access patterns"], "otherOptions": "Multiple buckets increase management complexity\\nManual management doesn't scale and introduces errors\\nSingle policy can't optimize for different access patterns"}	\N
63	163	63	AWS Storage Cost Management	Expert	Design Cost-Optimized Architectures	A data analytics company spends $100,000/month on storage across S3, EBS, and EFS for various workloads. They need comprehensive cost optimization while maintaining performance. Analysis shows: 40% rarely accessed, 30% predictable patterns, 20% unpredictable access, 10% high-performance needs. Which multi-service optimization strategy provides maximum savings?	[{"text": "Move all data to cheapest storage classes regardless of access patterns", "isCorrect": false}, {"text": "S3 lifecycle policies for rarely accessed, Intelligent-Tiering for unpredictable, gp3 optimization for EBS, EFS IA for infrequent file access", "isCorrect": true}, {"text": "Consolidate all storage to S3 with single lifecycle policy", "isCorrect": false}, {"text": "Implement Reserved Capacity for all storage services", "isCorrect": false}]	S3 lifecycle policies for rarely accessed, Intelligent-Tiering for unpredictable, gp3 optimization for EBS, EFS IA for infrequent file access	Tailored optimization per access pattern: lifecycle for predictable patterns, Intelligent-Tiering for unpredictable, gp3 for cost-effective EBS performance, EFS IA for infrequent access.	{"summary": "Multi-service cost optimization strategy:", "breakdown": ["40% rarely accessed: S3 lifecycle to Glacier/Deep Archive (75% savings)", "20% unpredictable: S3 Intelligent-Tiering (40% average savings)", "EBS optimization: gp3 volumes with right-sized IOPS (20% savings)", "EFS optimization: IA storage class for infrequent access (85% savings)"], "otherOptions": "Ignoring access patterns can cause performance issues and retrieval costs\\nNot all workloads suit object storage architecture\\nReserved Capacity not available for all storage types and may not match usage patterns"}	\N
64	164	64	AWS Storage Cost Management	Application	Design Cost-Optimized Architectures	A startup's S3 storage costs have grown to $25,000/month with 80% of objects never accessed after 90 days. They also have high data transfer costs from direct client downloads. The application serves user-uploaded images and documents globally. Which combination provides the most comprehensive cost reduction?	[{"text": "S3 lifecycle policy to Glacier and CloudFront distribution", "isCorrect": true}, {"text": "S3 Intelligent-Tiering and S3 Transfer Acceleration", "isCorrect": false}, {"text": "Move all data to S3 One Zone-IA with direct client access", "isCorrect": false}, {"text": "Compress all objects and implement client-side caching", "isCorrect": false}]	S3 lifecycle policy to Glacier and CloudFront distribution	Lifecycle policy to Glacier reduces storage costs by 68% for unused objects, while CloudFront eliminates data transfer costs and provides global acceleration.	{"summary": "Comprehensive S3 cost optimization:", "breakdown": ["Lifecycle to Glacier: 68% storage cost reduction for 80% of objects", "CloudFront: Eliminates data transfer costs from S3 to internet", "Global caching: Improves performance while reducing origin costs", "Combined savings: Approximately 70% total cost reduction"], "otherOptions": "Intelligent-Tiering adds overhead for predictable 90-day pattern\\nOne Zone-IA lacks resilience and doesn't address transfer costs\\nCompression helps but doesn't address core storage lifecycle and transfer costs"}	\N
92	191	91	Cost Optimization - Reserved Capacity Strategy	Expert	Design Cost-Optimized Architectures	A company runs diverse workloads: 40% steady-state (3-year predictable), 30% seasonal (6-month cycles), 20% development (weekdays only), 10% experimental (unpredictable). Current monthly costs are $200,000. Which purchasing strategy maximizes savings?	[{"text": "3-year Standard RIs for 40%, 1-year Convertible RIs for 30%, Scheduled RIs for 20%, On-Demand for 10%", "isCorrect": false}, {"text": "Compute Savings Plans for 70%, Spot Instances for 20%, On-Demand for 10%", "isCorrect": false}, {"text": "3-year Standard RIs for 40%, Compute Savings Plans for 30%, EC2 Instance Savings Plans for 20%, Spot for 10%", "isCorrect": true}, {"text": "All Reserved Instances with different term lengths", "isCorrect": false}]	3-year Standard RIs for 40%, Compute Savings Plans for 30%, EC2 Instance Savings Plans for 20%, Spot for 10%	This strategy optimizes for each workload pattern: Standard RIs for steady-state (maximum savings), Compute Savings Plans for seasonal flexibility, Instance Savings Plans for development consistency, Spot for experimental cost efficiency.	{"summary": "Optimized purchasing mix strategy:", "breakdown": ["Standard RIs (40%): Maximum 70% savings for predictable workloads", "Compute Savings Plans (30%): 66% savings with seasonal flexibility", "Instance Savings Plans (20%): Weekday development with family flexibility", "Spot Instances (10%): Up to 90% savings for fault-tolerant experiments"], "otherOptions": "Scheduled RIs are discontinued\\nSavings Plans alone don't maximize savings for steady-state workloads\\nRIs lack flexibility for seasonal and development workloads"}	\N
93	192	92	Cost Optimization - Data Transfer	Advanced	Design Cost-Optimized Architectures	A content platform transfers 500TB monthly: 60% to end users globally, 25% between AWS regions for processing, 15% to on-premises for archival. Current data transfer costs are $45,000/month. Which optimization provides maximum cost reduction?	[{"text": "CloudFront for user content, VPC Peering for inter-region, DirectConnect for on-premises", "isCorrect": true}, {"text": "S3 Transfer Acceleration for all transfers", "isCorrect": false}, {"text": "Regional S3 buckets with Cross-Region Replication", "isCorrect": false}, {"text": "Compress all data and use multiple smaller transfers", "isCorrect": false}]	CloudFront for user content, VPC Peering for inter-region, DirectConnect for on-premises	CloudFront eliminates internet transfer costs for 60% of traffic, VPC Peering reduces inter-region costs, DirectConnect provides predictable pricing for on-premises transfers.	{"summary": "Data transfer cost optimization strategy:", "breakdown": ["CloudFront (60%): Eliminates $27,000/month in internet transfer costs", "VPC Peering (25%): Reduces inter-region transfer by 50%", "DirectConnect (15%): Predictable pricing vs internet gateway", "Total savings: Approximately 70% reduction in transfer costs"], "otherOptions": "Transfer Acceleration adds cost and doesn't optimize for user downloads\\nCRR doesn't reduce transfer costs, adds replication costs\\nCompression helps but doesn't address underlying transfer pricing"}	\N
94	193	93	Cost Optimization - Serverless vs Container Economics	Expert	Design Cost-Optimized Architectures	A microservices platform runs 200 services with varying traffic patterns: 50 services get constant low traffic, 100 services have predictable business-hour spikes, 50 services have unpredictable traffic. Current container costs are $300,000/month. Which architecture mix optimizes costs?	[{"text": "All services migrate to Lambda for serverless benefits", "isCorrect": false}, {"text": "Fargate for all services with auto-scaling enabled", "isCorrect": false}, {"text": "Lambda for unpredictable traffic, Fargate Spot for spiky services, ECS EC2 Reserved for constant traffic", "isCorrect": true}, {"text": "EKS with cluster auto-scaling for all services", "isCorrect": false}]	Lambda for unpredictable traffic, Fargate Spot for spiky services, ECS EC2 Reserved for constant traffic	This hybrid approach optimizes for traffic patterns: Lambda eliminates idle costs for unpredictable traffic, Fargate Spot reduces costs for spiky workloads, Reserved instances provide maximum savings for constant traffic.	{"summary": "Hybrid architecture cost optimization:", "breakdown": ["Lambda (50 services): Pay-per-request eliminates idle time costs", "Fargate Spot (100 services): 70% savings for fault-tolerant spiky workloads", "ECS Reserved (50 services): 70% savings for predictable constant traffic", "Estimated total savings: 60% reduction from current $300,000"], "otherOptions": "Lambda cold starts and execution time limits problematic for all services\\nFargate On-Demand expensive for constant traffic\\nEKS has control plane costs and doesn't optimize for traffic patterns"}	\N
74	174	74	Cost Optimization - EC2 Purchasing Strategy	Intermediate	Design Cost-Optimized Architectures	A web application runs 24/7 with predictable baseline traffic and occasional spikes. Current setup uses 12 m5.large On-Demand instances. Which purchasing strategy provides maximum cost savings while maintaining availability?	[{"text": "Convert all instances to 3-year Reserved Instances", "isCorrect": false}, {"text": "Use 8 Reserved Instances + Auto Scaling with On-Demand for spikes", "isCorrect": true}, {"text": "Replace all instances with Spot Instances and diversified types", "isCorrect": false}, {"text": "Use Savings Plans for all compute requirements", "isCorrect": false}]	Use 8 Reserved Instances + Auto Scaling with On-Demand for spikes	Reserved Instances for baseline capacity provide significant savings (~70%), while Auto Scaling with On-Demand handles unpredictable traffic spikes.	{"summary": "Hybrid purchasing strategy benefits:", "breakdown": ["Reserved Instances: ~70% savings for baseline", "Auto Scaling: handles traffic variability", "On-Demand: flexibility for spikes", "Maintains availability: no interruption risk"], "otherOptions": "Over-provisioning with RIs wastes money during low traffic\\nSpot Instances can be interrupted, affecting availability\\nSavings Plans alone don't optimize for traffic patterns"}	\N
75	175	75	Cost Optimization - Storage Lifecycle	Advanced	Design Cost-Optimized Architectures	A backup system stores 200TB monthly. Data accessed frequently for 30 days, occasionally for 6 months, rarely for 2 years, then archived permanently. Which S3 storage strategy minimizes costs?	[{"text": "S3 Standard for all data with intelligent tiering", "isCorrect": false}, {"text": "S3 Standard  S3-IA (30 days)  S3 Glacier (6 months)  S3 Glacier Deep Archive (2 years)", "isCorrect": true}, {"text": "S3 Standard  S3 One Zone-IA (30 days)  S3 Glacier Deep Archive (6 months)", "isCorrect": false}, {"text": "S3 Intelligent-Tiering for all data regardless of access patterns", "isCorrect": false}]	S3 Standard  S3-IA (30 days)  S3 Glacier (6 months)  S3 Glacier Deep Archive (2 years)	This lifecycle progression optimizes costs by moving data through appropriate storage classes based on access patterns and retrieval requirements.	{"summary": "Optimal S3 lifecycle progression:", "breakdown": ["S3 Standard: frequent access (0-30 days)", "S3-IA: occasional access (30 days-6 months)", "S3 Glacier: rare access (6 months-2 years)", "S3 Deep Archive: long-term archival (2+ years)"], "otherOptions": "Intelligent-Tiering has monitoring costs for predictable patterns\\nSkips optimal intermediate storage classes\\nNot cost-effective for known access patterns"}	\N
10	110	10	AWS Security - Network Security	Application	Design Secure Applications	A three-tier web application (web, app, database) needs to be secured according to the principle of least privilege. The web tier must be accessible from the internet, app tier only from web tier, and database tier only from app tier. Which security group configuration is CORRECT?	[{"text": "Web SG: 0.0.0.0/0:80,443 | App SG: Web-SG:8080 | DB SG: App-SG:3306", "isCorrect": true}, {"text": "Web SG: 0.0.0.0/0:80,443 | App SG: 0.0.0.0/0:8080 | DB SG: 0.0.0.0/0:3306", "isCorrect": false}, {"text": "All tiers: VPC CIDR:All ports for internal communication", "isCorrect": false}, {"text": "Web SG: 0.0.0.0/0:All | App SG: VPC CIDR:All | DB SG: VPC CIDR:All", "isCorrect": false}]	Web SG: 0.0.0.0/0:80,443 | App SG: Web-SG:8080 | DB SG: App-SG:3306	Security groups should reference other security groups for internal communication, implementing true least privilege access between tiers.	{"summary": "Proper three-tier security group design:", "breakdown": ["Web tier: Internet access on standard web ports (80, 443)", "App tier: Only accepts traffic from web security group", "Database tier: Only accepts traffic from app security group", "Security group references: More secure than IP-based rules"], "otherOptions": "App and DB exposed to internet\\nToo broad access within VPC\\nOverly permissive on all tiers"}	\N
44	144	44	AWS S3 Deep Dive	Application	Design Secure Architectures	A healthcare organization stores patient records in S3 and needs to maintain full control over encryption keys while ensuring HIPAA compliance. They want to minimize key management overhead but need complete audit trails. Which encryption approach provides the BEST balance?	[{"text": "SSE-S3 with AWS-managed keys", "isCorrect": false}, {"text": "SSE-KMS with customer-managed keys", "isCorrect": true}, {"text": "SSE-C with customer-provided keys", "isCorrect": false}, {"text": "Client-side encryption with S3 Encryption Client", "isCorrect": false}]	SSE-KMS with customer-managed keys	SSE-KMS with customer-managed keys provides control over keys, audit trails via CloudTrail, and AWS handles key management infrastructure.	{"summary": "SSE-KMS benefits for HIPAA compliance:", "breakdown": ["Customer control: Create, rotate, and manage encryption keys", "Audit trails: CloudTrail logs all key usage and access", "AWS management: Infrastructure and availability handled by AWS", "HIPAA compliant: Meets regulatory requirements for key control"], "otherOptions": "AWS-managed keys don't provide sufficient customer control\\nSSE-C requires managing key storage and availability\\nClient-side encryption adds complexity without AWS integration benefits"}	\N
53	153	53	AWS Data Security Concepts	Application	Design Secure Architectures	A financial services company needs to automatically discover and protect sensitive data across 200 S3 buckets containing customer documents. They require automated classification, policy enforcement, and immediate alerts for data exposure risks. Which combination provides comprehensive data security automation?	[{"text": "AWS Config rules for bucket policies and CloudWatch for monitoring", "isCorrect": false}, {"text": "Amazon Macie for discovery and classification, EventBridge for alerts, and S3 Block Public Access", "isCorrect": true}, {"text": "AWS Inspector for vulnerability scanning and GuardDuty for threats", "isCorrect": false}, {"text": "Custom Lambda functions with CloudTrail for access monitoring", "isCorrect": false}]	Amazon Macie for discovery and classification, EventBridge for alerts, and S3 Block Public Access	Macie uses ML to discover sensitive data, provides automatic classification, EventBridge enables real-time alerts, and Block Public Access prevents exposure.	{"summary": "Automated data security components:", "breakdown": ["Amazon Macie: ML-powered sensitive data discovery and classification", "EventBridge integration: Real-time alerts for policy violations", "S3 Block Public Access: Prevents accidental public exposure", "Automated findings: Continuous monitoring across all 200 buckets"], "otherOptions": "Config and CloudWatch don't provide sensitive data discovery\\nInspector and GuardDuty focus on infrastructure, not data classification\\nCustom solutions require significant development and maintenance"}	\N
54	154	54	AWS Data Security Concepts	Expert	Design Secure Architectures	A healthcare organization must implement data classification and protection for patient records across multiple AWS services (S3, RDS, DynamoDB). They need automated discovery of PHI, policy-based access controls, and audit trails meeting HIPAA requirements. Which architecture provides comprehensive data protection?	[{"text": "Amazon Macie for S3, database activity streams for RDS, and VPC Flow Logs", "isCorrect": false}, {"text": "Amazon Macie for S3, AWS CloudTrail data events, and customer-managed KMS keys across all services", "isCorrect": true}, {"text": "AWS Config for compliance monitoring and GuardDuty for threat detection", "isCorrect": false}, {"text": "Custom data loss prevention (DLP) solution with third-party tools", "isCorrect": false}]	Amazon Macie for S3, AWS CloudTrail data events, and customer-managed KMS keys across all services	Macie discovers PHI in S3, CloudTrail data events provide comprehensive audit trails, and customer-managed KMS keys ensure encryption control across all services.	{"summary": "HIPAA-compliant data protection architecture:", "breakdown": ["Macie: Automated PHI discovery and classification in S3", "CloudTrail data events: Complete audit trail of data access", "Customer-managed KMS: Encryption control across S3, RDS, DynamoDB", "Policy-based access: IAM policies with resource-based conditions"], "otherOptions": "Missing encryption control and comprehensive audit coverage\\nConfig and GuardDuty don't provide PHI discovery and classification\\nCustom solutions require extensive compliance validation"}	\N
55	155	55	AWS Encryption	Expert	Design Secure Architectures	A multinational corporation needs end-to-end encryption for data stored across AWS services in multiple regions. They require customer control over encryption keys, automatic rotation, cross-region access, and integration with their existing HSM infrastructure. Which encryption strategy provides the MOST comprehensive solution?	[{"text": "AWS KMS customer-managed keys with automatic rotation in each region", "isCorrect": false}, {"text": "AWS CloudHSM cluster with custom key management application", "isCorrect": true}, {"text": "Client-side encryption with application-managed keys", "isCorrect": false}, {"text": "AWS KMS with imported key material from on-premises HSM", "isCorrect": false}]	AWS CloudHSM cluster with custom key management application	CloudHSM provides dedicated hardware security modules with full customer control, integrates with existing HSM infrastructure, and supports cross-region key management.	{"summary": "CloudHSM comprehensive encryption benefits:", "breakdown": ["Dedicated HSM: Full customer control over encryption operations", "FIPS 140-2 Level 3: Highest security certification available", "Cross-region clustering: Keys available across multiple regions", "HSM integration: Compatible with existing on-premises HSM infrastructure"], "otherOptions": "KMS keys are region-specific and don't integrate with existing HSM\\nApplication-managed keys lack HSM security and are difficult to manage at scale\\nImported key material in KMS doesn't provide full HSM integration"}	\N
56	156	56	AWS Encryption	Application	Design Secure Architectures	A development team needs to encrypt application data before storing it in DynamoDB. They want to minimize performance impact while ensuring field-level encryption for sensitive columns (SSN, credit card numbers) but allow searching on encrypted email addresses. Which encryption approach provides the BEST balance of security and functionality?	[{"text": "DynamoDB encryption at rest with AWS managed keys", "isCorrect": false}, {"text": "AWS DynamoDB Encryption Client with deterministic encryption for emails, probabilistic for sensitive fields", "isCorrect": true}, {"text": "Application-level AES encryption for all fields", "isCorrect": false}, {"text": "AWS KMS encryption for entire DynamoDB table", "isCorrect": false}]	AWS DynamoDB Encryption Client with deterministic encryption for emails, probabilistic for sensitive fields	DynamoDB Encryption Client provides field-level encryption with deterministic encryption for searchable fields and probabilistic encryption for maximum security on sensitive data.	{"summary": "Field-level encryption strategy:", "breakdown": ["Deterministic encryption: Same email always produces same ciphertext (searchable)", "Probabilistic encryption: Different ciphertext each time (maximum security)", "Client-side encryption: Data encrypted before sending to DynamoDB", "Performance optimized: Only sensitive fields encrypted, not entire records"], "otherOptions": "Table-level encryption doesn't provide field-level control\\nApplication encryption without library optimization impacts performance\\nKMS table encryption encrypts all data, doesn't enable selective searching"}	\N
57	157	57	AWS Governance Compliance	Expert	Design Secure Architectures	A regulated financial institution must demonstrate continuous compliance with SOC 2 requirements for data storage. They need automated policy enforcement, compliance reporting, and evidence collection across 50 AWS accounts. Non-compliant resources must be automatically remediated. Which governance framework provides comprehensive compliance automation?	[{"text": "AWS Config with custom rules and SNS notifications", "isCorrect": false}, {"text": "AWS Organizations with SCPs, Config Conformance Packs, and Systems Manager Automation", "isCorrect": true}, {"text": "AWS CloudFormation with compliance templates", "isCorrect": false}, {"text": "AWS Security Hub with manual remediation workflows", "isCorrect": false}]	AWS Organizations with SCPs, Config Conformance Packs, and Systems Manager Automation	Organizations provides account governance, Conformance Packs enable SOC 2 compliance checks, and Systems Manager Automation handles remediation across all accounts.	{"summary": "Comprehensive compliance automation:", "breakdown": ["Organizations + SCPs: Account-level policy enforcement", "Config Conformance Packs: Pre-built SOC 2 compliance rules", "Systems Manager Automation: Automatic remediation of non-compliant resources", "Centralized reporting: Compliance status across all 50 accounts"], "otherOptions": "Config alone lacks account-level governance and automated remediation\\nCloudFormation provides deployment compliance, not ongoing monitoring\\nSecurity Hub requires manual remediation, not automated"}	\N
58	158	58	AWS Governance Compliance	Application	Design Secure Architectures	A global company must ensure S3 buckets across all regions comply with data residency requirements. EU data must stay in EU regions, US data in US regions, with automatic policy enforcement and compliance reporting. Which solution provides the MOST effective governance?	[{"text": "IAM policies restricting S3 actions based on user location", "isCorrect": false}, {"text": "S3 bucket policies with aws:RequestedRegion condition keys", "isCorrect": false}, {"text": "AWS Organizations SCPs with region restrictions based on data classification tags", "isCorrect": true}, {"text": "AWS Config rules monitoring bucket creation across regions", "isCorrect": false}]	AWS Organizations SCPs with region restrictions based on data classification tags	Organizations SCPs provide account-level enforcement that cannot be overridden, with tag-based conditions enabling automatic data residency compliance.	{"summary": "Data residency governance with SCPs:", "breakdown": ["SCPs: Cannot be overridden by account-level permissions", "Tag-based conditions: Automatically enforce based on data classification", "Region restrictions: Prevent EU data creation in non-EU regions", "Organization-wide: Applies to all current and future accounts"], "otherOptions": "IAM policies can be overridden by account administrators\\nBucket policies only apply after bucket creation\\nConfig rules detect violations but don't prevent them"}	\N
67	167	67	Security - IAM Cross-Account Access	Intermediate	Design Secure Architectures	A development team needs access to production AWS resources from their development account. The security team requires auditable, temporary access with no permanent credentials. Which solution provides the MOST secure approach?	[{"text": "Create IAM users in production account with temporary passwords", "isCorrect": false}, {"text": "Configure cross-account IAM roles with AssumeRole permissions", "isCorrect": true}, {"text": "Share access keys between accounts using AWS Secrets Manager", "isCorrect": false}, {"text": "Use AWS SSO with permanent group assignments", "isCorrect": false}]	Configure cross-account IAM roles with AssumeRole permissions	Cross-account roles provide temporary credentials via STS, enable auditing through CloudTrail, and follow security best practices. The trust policy defines who can assume the role, while permissions policy controls what they can do.	{"summary": "Cross-account roles for secure access:", "breakdown": ["Temporary credentials via STS", "Complete audit trail in CloudTrail", "No permanent credentials to manage", "Trust policy controls who can assume role"], "otherOptions": "Users still require permanent management\\nAccess keys are permanent credentials\\nPermanent access violates least privilege"}	\N
78	178	78	Security - Secrets Rotation	Advanced	Design Secure Architectures	A microservices application uses 20+ different databases with rotating credentials every 7 days. The application experiences downtime during credential rotation. Which solution provides zero-downtime credential rotation?	[{"text": "Use AWS Secrets Manager with automatic rotation and version staging", "isCorrect": true}, {"text": "Store credentials in Parameter Store with manual rotation scripts", "isCorrect": false}, {"text": "Use IAM roles with temporary credentials for database access", "isCorrect": false}, {"text": "Implement client-side credential caching with TTL expiration", "isCorrect": false}]	Use AWS Secrets Manager with automatic rotation and version staging	Secrets Manager supports multi-version secrets during rotation (AWSPENDING, AWSCURRENT) allowing applications to gracefully transition to new credentials without downtime.	{"summary": "Zero-downtime rotation with Secrets Manager:", "breakdown": ["Version staging: AWSPENDING and AWSCURRENT versions available simultaneously", "Automatic rotation: Lambda-based rotation functions", "Application compatibility: Gradual transition between credential versions", "Multiple databases: Supports various database engines"], "otherOptions": "Parameter Store lacks automatic rotation capabilities\\nIAM roles don't provide database credentials\\nClient caching doesn't solve rotation downtime"}	\N
69	169	69	Security - Zero Trust Architecture	Expert	Design Secure Architectures	A financial services company implements zero trust where every request must be verified regardless of source. Microservices communicate across VPCs and on-premises. Which combination provides comprehensive zero trust? (Choose THREE)	[{"text": "AWS PrivateLink for service-to-service communication", "isCorrect": true}, {"text": "IAM roles for service authentication with short-lived tokens", "isCorrect": true}, {"text": "VPC security groups allowing traffic from any source", "isCorrect": false}, {"text": "AWS Certificate Manager for mutual TLS authentication", "isCorrect": true}, {"text": "Shared service accounts across all microservices", "isCorrect": false}]	AWS PrivateLink for service-to-service communication, IAM roles for service authentication with short-lived tokens, AWS Certificate Manager for mutual TLS authentication	Zero trust requires: private connectivity (PrivateLink), verified identity (IAM roles), and encrypted communication (mTLS) for every interaction.	{"summary": "Zero trust implementation components:", "breakdown": ["PrivateLink: Private service connectivity", "IAM roles: Verified service identity", "Short-lived tokens: Limit credential exposure", "mTLS: Encrypted and authenticated communication"], "otherOptions": "Open security groups violate zero trust principles\\nShared accounts prevent proper identity verification"}	\N
80	180	80	Security - Network Micro-segmentation	Advanced	Design Secure Architectures	A financial application requires micro-segmentation where each microservice can only communicate with specific other services. Traditional security groups become unmanageable with 50+ services. Which approach provides scalable micro-segmentation?	[{"text": "AWS App Mesh with service-to-service mTLS and traffic policies", "isCorrect": true}, {"text": "VPC security groups with service discovery automation", "isCorrect": false}, {"text": "Network ACLs with subnet-level service isolation", "isCorrect": false}, {"text": "WAF rules with API Gateway for each microservice", "isCorrect": false}]	AWS App Mesh with service-to-service mTLS and traffic policies	App Mesh provides service mesh architecture with automatic mTLS, traffic policies for fine-grained access control, and service discovery integration for scalable micro-segmentation.	{"summary": "Service mesh micro-segmentation benefits:", "breakdown": ["Automatic mTLS: Service-to-service encryption and authentication", "Traffic policies: Fine-grained communication rules", "Service discovery: Dynamic service endpoint management", "Scalability: Manages 50+ service communications efficiently"], "otherOptions": "Security groups don't scale well for complex service-to-service communication\\nNetwork ACLs too coarse-grained for microservice communication\\nWAF operates at HTTP level, not service mesh communication"}	\N
81	181	81	Security - Compliance Automation	Expert	Design Secure Architectures	A multinational bank must demonstrate continuous compliance with SOX, PCI-DSS, and GDPR across 200 AWS accounts. They need automated evidence collection, policy enforcement, and violation remediation. Which solution provides comprehensive compliance automation?	[{"text": "AWS Audit Manager + Config Conformance Packs + Security Hub + Systems Manager Automation", "isCorrect": true}, {"text": "CloudTrail + Config + CloudWatch + Manual compliance reports", "isCorrect": false}, {"text": "Well-Architected Tool + Trusted Advisor + Cost Explorer", "isCorrect": false}, {"text": "Inspector + GuardDuty + Macie + Custom Lambda functions", "isCorrect": false}]	AWS Audit Manager + Config Conformance Packs + Security Hub + Systems Manager Automation	Audit Manager automates evidence collection for SOX/PCI/GDPR, Conformance Packs implement compliance rules, Security Hub centralizes findings, Systems Manager automates remediation.	{"summary": "Multi-regulation compliance automation:", "breakdown": ["Audit Manager: Automated evidence collection for SOX, PCI-DSS, GDPR", "Conformance Packs: Pre-built compliance rule sets", "Security Hub: Centralized compliance dashboard", "Systems Manager: Automated remediation of non-compliant resources"], "otherOptions": "Manual processes don't scale for 200 accounts\\nThese tools don't address compliance requirements\\nSecurity tools but lack compliance framework integration"}	\N
82	182	82	Security - Advanced Threat Detection	Advanced	Design Secure Architectures	A SaaS platform experiences sophisticated attacks including credential stuffing, API abuse, and account takeovers. They need behavioral analysis and adaptive security responses. Which solution provides the MOST comprehensive threat detection?	[{"text": "GuardDuty + Detective + Security Hub with custom response automation", "isCorrect": true}, {"text": "WAF + Shield + CloudTrail with manual analysis", "isCorrect": false}, {"text": "Config + Inspector + Trusted Advisor with reporting", "isCorrect": false}, {"text": "VPC Flow Logs + CloudWatch + SNS with basic alerting", "isCorrect": false}]	GuardDuty + Detective + Security Hub with custom response automation	GuardDuty detects threats using ML, Detective provides behavioral analysis and investigation graphs, Security Hub correlates findings, automation enables adaptive responses.	{"summary": "Advanced threat detection and response:", "breakdown": ["GuardDuty: ML-based threat detection for credential stuffing and API abuse", "Detective: Behavioral analysis and investigation workflows", "Security Hub: Threat correlation and centralized management", "Response automation: Adaptive security measures based on threat intelligence"], "otherOptions": "WAF and Shield address different attack vectors, lack behavioral analysis\\nThese services focus on compliance, not threat detection\\nBasic monitoring without advanced threat detection capabilities"}	\N
47	147	47	AWS EBS Deep Dive	Application	Design Resilient Architectures	A production database uses EBS volumes and requires point-in-time recovery capability with minimal data loss. The current snapshot strategy creates daily snapshots, but the RPO requirement is 4 hours. Which enhancement provides the BEST backup strategy improvement?	[{"text": "Increase snapshot frequency to every 4 hours using scheduled snapshots", "isCorrect": true}, {"text": "Enable EBS Fast Snapshot Restore on existing daily snapshots", "isCorrect": false}, {"text": "Configure EBS Multi-Attach to create live replicas", "isCorrect": false}, {"text": "Use AWS Backup service with continuous backup enabled", "isCorrect": false}]	Increase snapshot frequency to every 4 hours using scheduled snapshots	Scheduling snapshots every 4 hours directly meets the RPO requirement by ensuring maximum data loss is limited to 4 hours.	{"summary": "Snapshot strategy for 4-hour RPO:", "breakdown": ["Scheduled snapshots: Every 4 hours meets exact RPO requirement", "Incremental backups: Only changed blocks stored, cost-effective", "Point-in-time recovery: Can restore to any snapshot timestamp", "Automated lifecycle: Can retain snapshots based on retention policy"], "otherOptions": "Fast Snapshot Restore improves RTO, not RPO\\nMulti-Attach doesn't provide backup functionality\\nContinuous backup overkill for 4-hour RPO requirement"}	\N
59	159	59	AWS Storage Resiliency Scalability	Expert	Design Resilient Architectures	A video streaming platform stores 1PB of content with global distribution requirements. They need 99.99% availability, automatic failover, and the ability to handle 10x traffic spikes during major events. Current single-region S3 architecture shows latency issues for international users. Which architecture provides optimal resiliency and global performance?	[{"text": "S3 Cross-Region Replication with CloudFront distribution", "isCorrect": true}, {"text": "Multi-region S3 buckets with Route 53 latency-based routing", "isCorrect": false}, {"text": "S3 Transfer Acceleration with single-region storage", "isCorrect": false}, {"text": "EFS with VPC peering across regions", "isCorrect": false}]	S3 Cross-Region Replication with CloudFront distribution	S3 CRR provides data resilience and regional availability, while CloudFront edge locations ensure global low-latency access and can handle traffic spikes automatically.	{"summary": "Global content delivery architecture:", "breakdown": ["S3 CRR: Automatic replication across regions for resilience", "CloudFront: 400+ edge locations for global low-latency delivery", "Auto-scaling: CloudFront handles 10x traffic spikes automatically", "99.99% availability: S3 + CloudFront combined SLA exceeds requirement"], "otherOptions": "Route 53 doesn't provide edge caching for content delivery\\nTransfer Acceleration only helps uploads, not global content delivery\\nEFS not designed for 1PB content delivery workloads"}	\N
84	183	83	Resilience - Chaos Engineering	Expert	Design Resilient Architectures	A streaming service wants to implement chaos engineering to test system resilience. They need to simulate various failure scenarios including AZ outages, service degradation, and network partitions. Which approach provides systematic chaos engineering capabilities?	[{"text": "AWS Fault Injection Simulator with pre-built experiment templates", "isCorrect": true}, {"text": "Manual instance termination and service disruption scripts", "isCorrect": false}, {"text": "Auto Scaling stress testing with load generation tools", "isCorrect": false}, {"text": "Blue-green deployments with rollback procedures", "isCorrect": false}]	AWS Fault Injection Simulator with pre-built experiment templates	AWS FIS provides controlled chaos engineering experiments with built-in safety mechanisms, experiment templates for common failure scenarios, and integration with AWS services.	{"summary": "Systematic chaos engineering with AWS FIS:", "breakdown": ["Controlled experiments: Safe failure injection with stop conditions", "Pre-built templates: Common failure scenarios like AZ outages", "Service integration: Works with EC2, ECS, RDS, and other AWS services", "Monitoring integration: Tracks system behavior during experiments"], "otherOptions": "Manual approaches lack safety controls and systematic methodology\\nLoad testing doesn't simulate infrastructure failures\\nBlue-green deployments are for safe releases, not failure testing"}	\N
85	184	84	Resilience - Database Failover	Advanced	Design Resilient Architectures	A financial trading application requires automatic database failover with zero data loss and sub-60 second RTO. The database processes 100,000 transactions per second. Which solution provides the BEST performance and reliability?	[{"text": "Aurora Global Database with cross-region automated failover", "isCorrect": true}, {"text": "RDS Multi-AZ with read replicas and manual promotion", "isCorrect": false}, {"text": "DynamoDB with Global Tables and eventual consistency", "isCorrect": false}, {"text": "Self-managed PostgreSQL with streaming replication", "isCorrect": false}]	Aurora Global Database with cross-region automated failover	Aurora Global Database provides synchronous replication within region (zero data loss), automatic failover in <60 seconds, and can handle high transaction volumes with read replica scaling.	{"summary": "High-performance database failover:", "breakdown": ["Synchronous replication: Zero data loss within region", "Automatic failover: Sub-60 second RTO with health monitoring", "High throughput: Designed for 100,000+ TPS workloads", "Global availability: Cross-region disaster recovery"], "otherOptions": "Manual promotion exceeds 60-second RTO requirement\\nEventual consistency inappropriate for financial trading\\nSelf-managed solutions require complex operational overhead"}	\N
86	185	85	Resilience - Circuit Breaker Pattern	Advanced	Design Resilient Architectures	A microservices application experiences cascading failures when one service becomes unavailable. Dependencies continue to call the failing service, causing system-wide outages. Which implementation provides the BEST circuit breaker pattern?	[{"text": "Application Load Balancer with health checks and connection draining", "isCorrect": false}, {"text": "AWS App Mesh with circuit breaker and retry policies", "isCorrect": true}, {"text": "API Gateway with throttling and request timeout settings", "isCorrect": false}, {"text": "Lambda functions with SQS dead letter queues", "isCorrect": false}]	AWS App Mesh with circuit breaker and retry policies	App Mesh provides service mesh capabilities with built-in circuit breaker patterns, configurable failure thresholds, automatic service isolation, and intelligent retry policies.	{"summary": "Service mesh circuit breaker implementation:", "breakdown": ["Circuit breaker: Automatic isolation of failing services", "Failure detection: Configurable thresholds and timeouts", "Graceful degradation: Prevents cascading failures", "Retry policies: Intelligent backoff and circuit restoration"], "otherOptions": "Load balancers provide basic health checks, not circuit breaker patterns\\nAPI Gateway throttling doesn't implement circuit breaker logic\\nDead letter queues handle failed messages, not circuit breaking"}	\N
87	186	86	Resilience - Multi-Region Backup Strategy	Expert	Design Resilient Architectures	A global SaaS platform stores 50TB of customer data daily across multiple regions. They need automated backup with RPO of 15 minutes, cross-region replication, and long-term retention (7 years) for compliance. Which solution provides optimal backup resilience?	[{"text": "AWS Backup with cross-region copy, lifecycle policies, and compliance reports", "isCorrect": true}, {"text": "EBS snapshots with Lambda automation and S3 replication", "isCorrect": false}, {"text": "Database native backup tools with manual cross-region copy", "isCorrect": false}, {"text": "Third-party backup software with tape gateway integration", "isCorrect": false}]	AWS Backup with cross-region copy, lifecycle policies, and compliance reports	AWS Backup provides centralized backup across services, automated cross-region replication, lifecycle management for long-term retention, and compliance reporting for audit requirements.	{"summary": "Comprehensive backup strategy components:", "breakdown": ["Centralized backup: Manages EC2, RDS, DynamoDB, EFS across regions", "15-minute RPO: Continuous and frequent backup scheduling", "Cross-region copy: Automated replication for disaster recovery", "7-year retention: Lifecycle policies with compliance reporting"], "otherOptions": "Manual automation doesn't scale for 50TB daily across multiple services\\nManual processes don't meet 15-minute RPO requirements\\nThird-party solutions add complexity without native AWS integration"}	\N
71	171	71	Resilience - Multi-AZ High Availability	Advanced	Design Resilient Architectures	A critical web application must survive an entire AZ failure with zero data loss and < 2 minute recovery time. Current architecture: ALB  EC2  RDS MySQL. Which modification provides the highest availability?	[{"text": "Auto Scaling Group across 3 AZs + RDS Multi-AZ + ElastiCache", "isCorrect": true}, {"text": "Single large EC2 instance + RDS with automated backups", "isCorrect": false}, {"text": "Auto Scaling Group in 1 AZ + RDS read replicas across AZs", "isCorrect": false}, {"text": "Lambda functions + DynamoDB Global Tables", "isCorrect": false}]	Auto Scaling Group across 3 AZs + RDS Multi-AZ + ElastiCache	Multi-AZ Auto Scaling provides instance-level resilience, RDS Multi-AZ offers automatic failover with zero data loss, and ElastiCache reduces database load during recovery scenarios.	{"summary": "Multi-AZ high availability design:", "breakdown": ["Auto Scaling Group: automatic instance replacement across AZs", "RDS Multi-AZ: synchronous replication with automatic failover", "Zero data loss: synchronous database replication", "Sub-2 minute recovery: automated failover processes"], "otherOptions": "Single instance creates single point of failure\\nSingle AZ Auto Scaling doesn't protect against AZ failure\\nMajor architecture change, may not meet recovery time"}	\N
95	199	98	Database High Availability	Intermediate	Design Resilient Architectures	A financial application uses Amazon RDS MySQL with Multi-AZ deployment. During a planned maintenance window, AWS performs a failover to the standby instance. How long should the application expect to be unavailable, and what happens to the database endpoint?	[{"text": "The application experiences 60-120 seconds of downtime while DNS propagates to the standby instance; the endpoint remains unchanged", "isCorrect": true}, {"text": "The application must update its connection string to point to the new primary instance after failover", "isCorrect": false}, {"text": "The failover is instantaneous with zero downtime because Multi-AZ uses synchronous replication", "isCorrect": false}, {"text": "The application experiences 5-10 minutes of downtime while the standby instance is promoted and initialized", "isCorrect": false}]	The application experiences 60-120 seconds of downtime while DNS propagates to the standby instance; the endpoint remains unchanged	RDS Multi-AZ failover typically completes in 60-120 seconds. During this time, AWS automatically updates the DNS record to point to the standby instance, which is promoted to primary. The database endpoint (connection string) remains unchanged, so applications automatically reconnect to the new primary without code changes. The standby instance is already running and synchronized, so no lengthy initialization is required.	{"summary": "Multi-AZ failover takes 60-120 seconds with automatic DNS update to standby instance", "breakdown": ["Synchronous replication: Standby is always up-to-date (zero data loss)", "DNS-based failover: Endpoint stays same, DNS record updated automatically", "Typical failover time: 60-120 seconds for DNS propagation and connection re-establishment", "Application reconnection: Most DB clients automatically retry failed connections", "No endpoint change: Applications don't need configuration updates", "Standby promotion: New primary immediately accepts connections", "Health checks: RDS monitors primary and triggers failover on failure"], "otherOptions": "Applications don't need endpoint updates (DNS handled automatically); Failover isn't instantaneous (60-120s required); 5-10 minutes is too long for standard failover"}	t
96	200	99	Storage Optimization	Advanced	Design Cost-Optimized Architectures	A media company stores 500TB of video content in S3 Standard. Analysis shows that 80% of videos are not accessed after 30 days, and 95% are never accessed after 90 days. They need to retain all content for 7 years for compliance. Which lifecycle policy provides maximum cost savings while meeting requirements?	[{"text": "Transition to S3 Standard-IA after 30 days, then to S3 Glacier Flexible Retrieval after 90 days, then to S3 Glacier Deep Archive after 1 year", "isCorrect": true}, {"text": "Transition to S3 Intelligent-Tiering immediately and let AWS automatically optimize storage classes", "isCorrect": false}, {"text": "Transition to S3 One Zone-IA after 30 days, then to S3 Glacier Deep Archive after 90 days", "isCorrect": false}, {"text": "Keep all content in S3 Standard and enable S3 Lifecycle policies for automatic deletion after 7 years", "isCorrect": false}]	Transition to S3 Standard-IA after 30 days, then to S3 Glacier Flexible Retrieval after 90 days, then to S3 Glacier Deep Archive after 1 year	This multi-tier approach maximizes savings by matching storage class to access patterns. Standard-IA (30+ days) reduces costs by ~50% for infrequently accessed data while maintaining quick retrieval. Glacier Flexible Retrieval (90+ days) saves ~80% for rarely accessed data. Glacier Deep Archive (1+ years) offers 95% cost reduction for long-term compliance storage. This strategy balances cost savings with retrieval capabilities based on actual usage patterns.	{"summary": "Multi-tier lifecycle transitions optimize costs by matching storage class to access patterns", "breakdown": ["Days 0-30: S3 Standard ($0.023/GB) - frequent access period", "Days 30-90: Standard-IA ($0.0125/GB) - 46% cost reduction", "Days 90-365: Glacier Flexible ($0.0036/GB) - 84% cost reduction", "Years 1-7: Glacier Deep Archive ($0.00099/GB) - 96% cost reduction", "Total estimated savings: ~85% over 7 years vs keeping in Standard", "Compliance maintained: All content retained for required 7 years", "Retrieval times: Standard-IA (milliseconds), Glacier (minutes-hours), Deep Archive (12-48 hours)"], "otherOptions": "Intelligent-Tiering has monitoring fees and may not be optimal for predictable patterns; One Zone-IA lacks durability for compliance data; Keeping in Standard wastes 85% in unnecessary costs"}	t
97	201	100	Network Architecture	Advanced	Design High-Performing Architectures	A company has 15 VPCs across multiple AWS regions that need to communicate with each other. They anticipate adding 10 more VPCs over the next year. Currently using VPC peering, they are experiencing management complexity. What is the PRIMARY advantage of migrating to AWS Transit Gateway?	[{"text": "Transit Gateway eliminates the need for VPC peering connections, reducing from 105 peering connections to a hub-and-spoke model with centralized routing", "isCorrect": true}, {"text": "Transit Gateway provides faster network performance between VPCs compared to VPC peering", "isCorrect": false}, {"text": "Transit Gateway automatically encrypts all inter-VPC traffic without requiring additional configuration", "isCorrect": false}, {"text": "Transit Gateway reduces data transfer costs by 50% compared to VPC peering", "isCorrect": false}]	Transit Gateway eliminates the need for VPC peering connections, reducing from 105 peering connections to a hub-and-spoke model with centralized routing	With 15 VPCs, full mesh VPC peering requires n(n-1)/2 = 105 connections. Adding 10 more VPCs would require 300 connections total. Transit Gateway uses a hub-and-spoke model where each VPC connects once to the central gateway, reducing to 15 connections (eventually 25). This dramatically simplifies routing, management, and scaling while providing centralized route control and inter-region connectivity via Transit Gateway peering.	{"summary": "Transit Gateway replaces complex mesh topology with centralized hub-and-spoke architecture", "breakdown": ["VPC Peering mesh: 15 VPCs = 105 connections; 25 VPCs = 300 connections (scales at n)", "Transit Gateway: 15 VPCs = 15 attachments; 25 VPCs = 25 attachments (scales linearly)", "Management reduction: Single routing table vs hundreds of route entries", "Centralized control: One place to manage inter-VPC routing policies", "Transitive routing: Supported (not possible with VPC peering)", "Multi-region support: Transit Gateway peering for cross-region connectivity", "Scaling: Can attach up to 5,000 VPCs per gateway"], "otherOptions": "Performance is comparable, not faster; Encryption is same as peering (not automatic advantage); Data transfer costs are similar, not reduced by 50%"}	t
98	202	101	Serverless Compute	Intermediate	Design High-Performing Architectures	A company's AWS Lambda function processes incoming webhooks from a third-party service. During peak hours, they receive errors indicating "Rate Exceeded" and "Service Unavailable." What is the MOST likely cause, and what should be implemented to resolve it?	[{"text": "The Lambda function is reaching the account-level concurrent execution limit; implement reserved concurrency for the function and request a limit increase from AWS Support", "isCorrect": true}, {"text": "The Lambda function timeout is too short; increase the timeout value to allow longer execution times", "isCorrect": false}, {"text": "The API Gateway throttling limit is too restrictive; increase the API Gateway rate limits", "isCorrect": false}, {"text": "The Lambda function memory allocation is insufficient; increase memory to improve performance and reduce errors", "isCorrect": false}]	The Lambda function is reaching the account-level concurrent execution limit; implement reserved concurrency for the function and request a limit increase from AWS Support	AWS Lambda has a default account-level concurrent execution limit of 1,000 across all functions in a region. "Rate Exceeded" errors indicate the function is hitting this limit during peak loads. Reserved concurrency guarantees a portion of the account limit for critical functions, preventing other functions from consuming all available concurrency. For sustained high-volume workloads, request a limit increase through AWS Support to raise the regional concurrency limit.	{"summary": "Lambda concurrency limits cause throttling; use reserved concurrency and request limit increases", "breakdown": ["Account limit: Default 1,000 concurrent executions per region", "Reserved concurrency: Guarantees capacity for critical functions", "Unreserved pool: Minimum 100 concurrency always available for other functions", "Rate exceeded error: Indicates concurrency limit reached", "Throttling behavior: New invocations rejected when limit exceeded", "Async invocations: Automatically retried (may cause delays)", "Sync invocations: Return 429 error (caller must retry)", "Solution: Set reserved concurrency + request permanent limit increase"], "otherOptions": "Timeout doesn't cause Rate Exceeded errors; API Gateway throttling shows different error messages; Memory allocation affects performance, not concurrency limits"}	t
99	203	102	Storage Performance	Intermediate	Design High-Performing Architectures	A database application requires 20,000 IOPS with consistent low latency for transaction processing. The database size is 500GB. Which EBS volume type provides the MOST cost-effective solution while meeting performance requirements?	[{"text": "Provisioned IOPS SSD (io2) with 20,000 IOPS provisioned", "isCorrect": false}, {"text": "General Purpose SSD (gp3) with 16,000 IOPS included baseline and 4,000 IOPS provisioned", "isCorrect": true}, {"text": "Throughput Optimized HDD (st1) with enhanced networking enabled", "isCorrect": false}, {"text": "General Purpose SSD (gp2) with 1,500 IOPS baseline (3 IOPS per GB  500GB)", "isCorrect": false}]	General Purpose SSD (gp3) with 16,000 IOPS included baseline and 4,000 IOPS provisioned	GP3 volumes provide 3,000 IOPS baseline (sufficient for most workloads) and allow provisioning up to 16,000 IOPS at no extra cost. Additional IOPS can be provisioned beyond 16,000 at lower cost than io2. For this scenario, provisioning 20,000 IOPS on gp3 costs significantly less than io2 while providing the same performance. GP3 also offers consistent low latency suitable for transactional databases, making it the most cost-effective choice.	{"summary": "GP3 offers best cost-performance ratio for most database workloads under 64,000 IOPS", "breakdown": ["GP3 pricing: $0.08/GB + $0.005/provisioned IOPS over 3,000", "GP3 for 20,000 IOPS: (500GB  $0.08) + (17,000  $0.005) = $40 + $85 = $125/month", "io2 pricing: $0.125/GB + $0.065/IOPS = (500  $0.125) + (20,000  $0.065) = $62.50 + $1,300 = $1,362.50/month", "Cost savings: GP3 is ~91% cheaper than io2 for this workload", "Performance: Both provide consistent sub-millisecond latency", "GP3 max: 16,000 IOPS included, up to 64,000 total provisioned", "Use io2 when: Need >64,000 IOPS or 99.999% durability"], "otherOptions": "io2 is unnecessarily expensive for this requirement; st1 is for throughput, not IOPS; gp2 only provides 1,500 IOPS (insufficient)"}	t
100	204	103	Content Delivery - High Availability	Advanced	Design Resilient Architectures	A media streaming company uses CloudFront to deliver video content from an S3 bucket. They want to implement automatic failover to a secondary S3 bucket in another region if the primary origin becomes unavailable. What configuration is required?	[{"text": "Create a CloudFront origin group with primary and secondary origins, configure health checks for the primary origin", "isCorrect": true}, {"text": "Set up S3 Cross-Region Replication and use Route 53 failover routing to switch between origins", "isCorrect": false}, {"text": "Enable S3 Transfer Acceleration on both buckets and configure CloudFront to use the fastest origin", "isCorrect": false}, {"text": "Configure multiple CloudFront distributions with Route 53 health checks to switch DNS between distributions", "isCorrect": false}]	Create a CloudFront origin group with primary and secondary origins, configure health checks for the primary origin	CloudFront origin groups provide native high availability by automatically failing over to a secondary origin when the primary origin fails health checks or returns specific HTTP status codes (500, 502, 503, 504, 404, 403). This happens transparently at the CloudFront edge without requiring DNS changes or manual intervention. You configure an origin group with primary and secondary origins (S3 buckets), and CloudFront automatically routes requests to the secondary when the primary is unhealthy.	{"summary": "CloudFront origin groups enable automatic failover between primary and secondary origins", "breakdown": ["Origin group: Contains 2 origins (primary and secondary)", "Failover triggers: HTTP 500, 502, 503, 504 status codes from primary", "Health checks: CloudFront monitors origin health automatically", "Automatic failover: Transparent to end users (no DNS change required)", "Failback: Returns to primary when health restored", "Configuration: Set up via CloudFront console or API", "Secondary origin: Can be S3 bucket in different region", "Best practice: Use S3 CRR to keep secondary bucket synchronized"], "otherOptions": "Route 53 failover adds unnecessary complexity and slower DNS TTL; Transfer Acceleration is for upload performance, not failover; Multiple distributions require DNS switching (slower)"}	t
101	205	104	Database Design - Performance	Advanced	Design High-Performing Architectures	An e-commerce application stores order data in DynamoDB with UserID as the partition key and OrderDate as the sort key. During flash sales, a few power users place hundreds of orders, causing throttling issues. What is the BEST solution to prevent hot partitions while maintaining query efficiency?	[{"text": "Add a random suffix (1-10) to UserID to distribute writes across multiple partitions, and query all suffixes to retrieve user orders", "isCorrect": false}, {"text": "Change the partition key to OrderID (UUID) and create a Global Secondary Index (GSI) with UserID as the partition key", "isCorrect": true}, {"text": "Use OrderDate as the partition key and UserID as the sort key to distribute writes over time", "isCorrect": false}, {"text": "Enable DynamoDB auto scaling and increase provisioned write capacity units", "isCorrect": false}]	Change the partition key to OrderID (UUID) and create a Global Secondary Index (GSI) with UserID as the partition key	Using a unique OrderID (UUID) as the partition key ensures even write distribution across all partitions, eliminating hot partitions. A GSI with UserID as the partition key enables efficient queries for all orders by a specific user. This architecture separates write distribution (via random OrderID) from query access patterns (via GSI on UserID), providing both high write throughput and efficient queries without throttling.	{"summary": "UUID partition key distributes writes; GSI on UserID maintains query efficiency", "breakdown": ["Hot partition problem: Few UserIDs receive disproportionate writes", "UUID partition key: Guarantees random distribution across all partitions", "Write distribution: No single partition receives excessive traffic", "GSI purpose: Enables querying by UserID despite different partition key", "Query pattern: Query GSI with UserID to retrieve all user orders", "GSI cost: Additional storage and query capacity, but solves throttling", "Scalability: Supports unlimited users and orders without hot partitions", "Alternative pattern: Event-driven architecture with composite key"], "otherOptions": "Random suffixes require multiple queries (inefficient); OrderDate as partition key creates time-based hot partitions; Auto scaling doesn't solve hot partition issue (throttling persists)"}	t
102	206	105	Governance - Access Control	Advanced	Design Secure Applications and Architectures	A company uses AWS Organizations with multiple accounts. They want to prevent all accounts in the Development OU from launching EC2 instances larger than t3.large, while allowing the Production OU full access. Despite having IAM permissions, developers report they cannot launch t3.xlarge instances. What is the evaluation order that explains this behavior?	[{"text": "SCP deny at OU level blocks the action regardless of IAM Allow policies, as SCPs set the maximum permissions boundary", "isCorrect": true}, {"text": "IAM policies are evaluated first, then SCPs, so the IAM Allow should override the SCP deny", "isCorrect": false}, {"text": "Resource-based policies on EC2 can override SCP restrictions for specific instance types", "isCorrect": false}, {"text": "Identity-based IAM policies attached to users take precedence over OU-level SCPs", "isCorrect": false}]	SCP deny at OU level blocks the action regardless of IAM Allow policies, as SCPs set the maximum permissions boundary	Service Control Policies (SCPs) define the maximum permissions for accounts in an organization, acting as a guardrail. Even if an IAM policy grants permission, an SCP deny will prevent the action. The evaluation order is: SCP (defines maximum allowed permissions)  IAM policies (grant specific permissions within SCP boundaries)  Resource-based policies. SCPs filter what actions are possible; IAM policies then determine what is actually permitted within those boundaries.	{"summary": "SCPs set permission boundaries that override IAM Allow policies", "breakdown": ["Policy evaluation order: SCPs  Identity-based IAM  Resource-based policies", "SCP as boundary: Defines maximum possible permissions for entire account", "IAM cannot exceed: IAM Allow policies only work within SCP boundaries", "Explicit deny: SCP deny blocks action even with IAM Allow", "OU inheritance: SCPs applied to OU affect all accounts within it", "Production OU: Different SCP allowing all instance types", "Use case: Prevent costly resource launches, enforce compliance", "Testing: Developers see access denied despite having IAM permissions"], "otherOptions": "IAM policies evaluated after SCPs (not first); Resource-based policies can't override SCP denies; User IAM policies must work within SCP boundaries"}	t
103	207	106	Container Orchestration	Intermediate	Design High-Performing Architectures	A containerized application runs on ECS with 20 tasks across 5 EC2 instances. During deployment, they notice that some instances run 8 tasks while others run only 2, causing uneven resource utilization. Which task placement strategy should be configured to distribute tasks evenly across instances?	[{"text": "Use the spread placement strategy with the attribute ecs.availability-zone to distribute tasks across AZs", "isCorrect": false}, {"text": "Use the binpack placement strategy with memory to maximize resource utilization on each instance", "isCorrect": false}, {"text": "Use the spread placement strategy with the attribute instance-id to distribute tasks evenly across instances", "isCorrect": true}, {"text": "Use the random placement strategy to distribute tasks without specific constraints", "isCorrect": false}]	Use the spread placement strategy with the attribute instance-id to distribute tasks evenly across instances	The spread placement strategy distributes tasks across specified attributes. Using "instance-id" ensures tasks are placed evenly across all container instances, preventing any single instance from being overloaded. This strategy maximizes fault tolerance and resource distribution. With 20 tasks and 5 instances, spread will place approximately 4 tasks per instance, ensuring even resource utilization across the cluster.	{"summary": "Spread strategy with instance-id attribute ensures even task distribution across instances", "breakdown": ["Spread strategy: Distributes tasks based on specified attribute (AZ, instance-id, etc.)", "Instance-id attribute: Ensures even distribution across all instances", "Result: 20 tasks  5 instances = 4 tasks per instance (approximately)", "Fault tolerance: No single instance overloaded or under-utilized", "Resource utilization: CPU and memory distributed evenly", "AZ spread: Use ecs.availability-zone for cross-AZ distribution", "Combination: Can use multiple strategies (spread for AZ, then instance)", "Best practice: Spread for HA, binpack for cost optimization"], "otherOptions": "AZ spread distributes across zones, not instances; Binpack intentionally concentrates tasks (opposite of even distribution); Random doesn't guarantee even distribution"}	t
104	208	107	Secure Data Access	Intermediate	Design Secure Applications and Architectures	A web application needs to allow users to upload profile photos directly to S3 without exposing AWS credentials in the client-side code. The uploads should be restricted to specific users for a limited time. What is the MOST secure approach?	[{"text": "Generate S3 presigned URLs on the backend with PUT permissions and short expiration times (e.g., 15 minutes), provide to authenticated users", "isCorrect": true}, {"text": "Create an IAM user with S3 write permissions and embed the access keys in the client application", "isCorrect": false}, {"text": "Make the S3 bucket public with a bucket policy that allows uploads from the application's IP address", "isCorrect": false}, {"text": "Use S3 Access Points with a resource policy that grants upload permissions to all users", "isCorrect": false}]	Generate S3 presigned URLs on the backend with PUT permissions and short expiration times (e.g., 15 minutes), provide to authenticated users	Presigned URLs allow temporary access to S3 objects without requiring AWS credentials in the client. The backend generates a URL signed with its AWS credentials that grants specific permissions (e.g., PUT object) for a limited time. Users can upload directly to S3 using this URL, which automatically expires, ensuring time-limited access. This keeps credentials secure on the backend while enabling direct client-to-S3 uploads, reducing backend bandwidth and improving performance.	{"summary": "Presigned URLs provide secure, time-limited direct access to S3 without exposing credentials", "breakdown": ["Backend generation: Server creates presigned URL using its IAM role credentials", "Time-limited: URL expires after specified duration (e.g., 15 minutes)", "Permission control: URL grants specific operation (PUT for upload, GET for download)", "Direct upload: Client uploads directly to S3, not through backend", "No credentials exposed: Client never sees AWS access keys", "Authentication: Backend verifies user identity before generating URL", "Bandwidth savings: Upload traffic doesn't go through application servers", "Security: URL expires automatically, limiting exposure window"], "otherOptions": "Embedded credentials in client code is extremely insecure; Public bucket violates least privilege principle; S3 Access Points don't provide time-limited access like presigned URLs"}	t
125	229	128	Serverless - Event-Driven Architecture	Expert	Design High-Performing Architectures	An e-commerce company processes orders through a multi-step workflow: payment validation, inventory check, shipping label generation, and customer notification. Each step can fail and requires retry with exponential backoff. The solution must handle 10,000 orders/hour during peak times, provide visibility into each workflow execution, and support parallel processing where possible. Which architecture provides the most robust implementation?	[{"text": "A) Lambda functions triggered by SQS with DLQ for failures", "isCorrect": false}, {"text": "B) EventBridge rules triggering Step Functions state machines with parallel states", "isCorrect": true}, {"text": "C) ECS tasks orchestrated by custom scheduler in RDS", "isCorrect": false}, {"text": "D) Kinesis Data Streams with Lambda consumers", "isCorrect": false}]	B) EventBridge rules triggering Step Functions state machines with parallel states	Step Functions provides built-in error handling with retry/backoff, parallel execution for independent steps, visual workflow monitoring, and automatic state management. EventBridge decouples order creation from processing.	{"summary": "Event-driven workflow orchestration:", "breakdown": ["Step Functions: Built-in retry with exponential backoff", "Parallel states: Inventory and shipping labels process simultaneously", "EventBridge: Decouples order creation from processing", "Execution history: Complete visibility into each workflow", "Automatic scaling: Handles variable load without management"], "otherOptions": "A) SQS lacks workflow visibility and parallel processing\\nC) Custom scheduler adds complexity and maintenance overhead\\nD) Kinesis doesn't provide workflow orchestration or retry logic"}	f
126	230	129	Database - Connection Pooling	Advanced	Design High-Performing Architectures	A serverless application uses Lambda functions to query an Aurora MySQL database. During traffic spikes, the database reaches max_connections limit (1000), causing Lambda timeouts. The application makes 5,000+ concurrent connections during peak periods. Database CPU usage is only 20%. Which solution resolves the connection exhaustion issue while maintaining serverless benefits?	[{"text": "A) Increase Aurora instance size to support more connections", "isCorrect": false}, {"text": "B) Implement RDS Proxy with connection pooling between Lambda and Aurora", "isCorrect": true}, {"text": "C) Use DynamoDB instead of Aurora for better Lambda integration", "isCorrect": false}, {"text": "D) Implement connection pooling in Lambda using global variables", "isCorrect": false}]	B) Implement RDS Proxy with connection pooling between Lambda and Aurora	RDS Proxy multiplexes thousands of Lambda connections to a smaller pool of database connections, preventing connection exhaustion. It maintains connection health, reduces Lambda cold start times by reusing connections, and requires no application code changes.	{"summary": "RDS Proxy connection pooling benefits:", "breakdown": ["Connection multiplexing: 5000 Lambda  100 DB connections", "Connection reuse: Reduces cold start overhead", "Automatic failover: Maintains connection during DB failover", "IAM authentication: Eliminates credential management in Lambda", "Low CPU impact: Solves connection issue without scaling compute"], "otherOptions": "A) Wastes resources when low CPU shows connection limits, not compute\\nC) Requires complete application rewrite and may not fit data model\\nD) Lambda container reuse is unpredictable, doesn't solve concurrent spikes"}	f
127	231	130	Storage - Cost Optimization	Intermediate	Design Cost-Optimized Architectures	A data analytics company stores 500TB of customer data in S3. Access patterns are unpredictable: some objects accessed frequently for weeks, then rarely for months. Current S3 Standard costs are $11,500/month. The team wants to optimize costs without impacting data availability or requiring application changes. Which storage strategy provides maximum cost savings with minimal operational overhead?	[{"text": "A) S3 Intelligent-Tiering with Archive Access tier enabled", "isCorrect": true}, {"text": "B) Lifecycle policies moving to Infrequent Access after 30 days", "isCorrect": false}, {"text": "C) Store all data in S3 Glacier with expedited retrieval", "isCorrect": false}, {"text": "D) Use Lambda to analyze CloudTrail and move objects to IA", "isCorrect": false}]	A) S3 Intelligent-Tiering with Archive Access tier enabled	S3 Intelligent-Tiering automatically moves objects between access tiers based on actual usage patterns, requires no application changes, and includes Archive tiers (90-180 days) for maximum savings. Monitoring fees are offset by storage savings.	{"summary": "Intelligent-Tiering automatic optimization:", "breakdown": ["Automatic tiering: Frequent  IA  Archive based on usage", "No retrieval fees: Unlike Glacier, immediate access to all tiers", "Archive tiers: 90-180 day archives reduce costs by 95%", "No application changes: S3 API remains identical", "Estimated savings: 40-70% reduction for unpredictable patterns"], "otherOptions": "B) Fixed 30-day policy can't adapt to varying patterns, causes retrieval fees\\nC) Glacier requires restore requests, impacts availability\\nD) Custom solution adds complexity, costs, and potential errors"}	f
128	232	131	Networking - Private Connectivity	Advanced	Design Secure Architectures	A healthcare company has strict compliance requirements prohibiting any data from traversing the public internet. Applications in private subnets need to access S3, DynamoDB, and a third-party SaaS API. NAT Gateways currently route traffic through IGW. Which combination ensures all traffic remains on the AWS private network? (Choose TWO)	[{"text": "A) VPC Gateway Endpoints for S3 and DynamoDB", "isCorrect": true}, {"text": "B) NAT Gateway in private subnet", "isCorrect": false}, {"text": "C) VPC Interface Endpoint (PrivateLink) for SaaS API", "isCorrect": true}, {"text": "D) Internet Gateway with security group restrictions", "isCorrect": false}, {"text": "E) Transit Gateway with VPN attachment", "isCorrect": false}]	A) VPC Gateway Endpoints for S3 and DynamoDB, C) VPC Interface Endpoint (PrivateLink) for SaaS API	Gateway Endpoints route S3/DynamoDB traffic privately through AWS backbone. Interface Endpoints (PrivateLink) enable private connectivity to third-party services. Neither requires NAT Gateway or internet exposure.	{"summary": "Private AWS connectivity architecture:", "breakdown": ["Gateway Endpoints: Free, private S3/DynamoDB access", "PrivateLink: Private connection to SaaS via AWS network", "No IGW traffic: Eliminates public internet exposure", "Route table updates: Automatic routing to endpoints", "Compliance: Meets healthcare data transmission requirements"], "otherOptions": "B) NAT Gateway routes traffic through IGW to public internet\\nD) IGW allows public internet access, violates compliance\\nE) Transit Gateway doesn't provide service endpoint access"}	t
129	233	132	CDN - Edge Computing	Expert	Design High-Performing Architectures	A global media streaming platform serves personalized video content to 50M users. They need to implement A/B testing for UI features, add user authentication headers, and perform device-based content optimizationall with <50ms latency. Origin in us-east-1 shows 300ms latency for Asia users. Which solution provides optimal global performance?	[{"text": "A) CloudFront with Lambda@Edge for request manipulation and viewer response", "isCorrect": true}, {"text": "B) Global Accelerator with ALB in multiple regions", "isCorrect": false}, {"text": "C) Route 53 geolocation routing to regional ALBs", "isCorrect": false}, {"text": "D) CloudFront with origin request to regional Lambda functions", "isCorrect": false}]	A) CloudFront with Lambda@Edge for request manipulation and viewer response	Lambda@Edge executes code at 450+ global edge locations with <50ms latency. Viewer request events handle auth/A/B testing, origin request events optimize content, viewer response adds headersall before content reaches origin, eliminating 300ms origin latency for cached content.	{"summary": "Edge computing benefits:", "breakdown": ["Lambda@Edge: Runs at all CloudFront edge locations", "Viewer request: Auth and A/B testing before cache lookup", "Origin request: Content optimization before origin hit", "Viewer response: Add headers/transforms after cache", "50ms latency: Process at edge, eliminate origin roundtrips"], "otherOptions": "B) Global Accelerator improves TCP performance, doesn't provide edge computing\\nC) Geolocation routing can't manipulate requests at edge\\nD) Origin request only runs on cache miss, doesn't help cached content"}	f
130	234	133	Containers - Auto-scaling	Expert	Design Cost-Optimized Architectures	A microservices platform on EKS runs 100+ services with varying resource requirements: some need GPU, others need large memory, many need only small CPU. Cluster autoscaler takes 5+ minutes to add nodes during traffic spikes, causing pod pending states. Monthly EC2 costs are $80K with 40% waste from oversized nodes. Which solution provides fastest scaling and cost optimization?	[{"text": "A) Replace Cluster Autoscaler with Karpenter for intelligent node provisioning", "isCorrect": true}, {"text": "B) Use EKS Fargate for all pods to eliminate node management", "isCorrect": false}, {"text": "C) Implement HPA with smaller node pools across multiple instance types", "isCorrect": false}, {"text": "D) Use EC2 Spot Fleet with manual node group configuration", "isCorrect": false}]	A) Replace Cluster Autoscaler with Karpenter for intelligent node provisioning	Karpenter provisions optimal instance types in seconds (vs 5 minutes), consolidates underutilized nodes, bins packs pods efficiently, supports Spot instances automatically, and eliminates node group management. It right-sizes nodes to actual pod requirements.	{"summary": "Karpenter intelligent provisioning:", "breakdown": ["Sub-60 second scaling: Provisions nodes in parallel with pod scheduling", "Instance diversity: Automatically selects optimal instance type per pod", "Consolidation: Continuously replaces nodes to maximize utilization", "Spot integration: Automatic spot instance diversification", "Cost savings: 40-60% reduction through right-sizing and spot"], "otherOptions": "B) Fargate adds 25-40% cost premium, doesn't support GPU/DaemonSets\\nC) HPA scales pods, not nodes; multiple node pools increase complexity\\nD) Manual Spot Fleet management adds operational overhead"}	f
131	235	134	Security - Secrets Management	Intermediate	Design Secure Architectures	An application uses 200 configuration parameters (API endpoints, feature flags, cache TTLs) and 5 database passwords that must rotate every 30 days. Current approach hardcodes values in environment variables. Which secrets management strategy provides optimal cost and functionality?	[{"text": "A) Store all 205 values in Secrets Manager with automatic rotation", "isCorrect": false}, {"text": "B) Use Parameter Store Standard for configs, Secrets Manager for passwords with Lambda rotation", "isCorrect": true}, {"text": "C) Store everything in Parameter Store Advanced with rotation notifications", "isCorrect": false}, {"text": "D) Use AWS AppConfig for all values with deployment validation", "isCorrect": false}]	B) Use Parameter Store Standard for configs, Secrets Manager for passwords with Lambda rotation	Parameter Store Standard tier is free for 10,000 parameters and sufficient for static configs. Secrets Manager provides automatic rotation for databases (RDS, Aurora, DocumentDB) and costs only $0.40/secret/month + rotation Lambda costs. This hybrid approach optimizes cost while enabling automatic rotation where needed.	{"summary": "Cost-optimized secrets architecture:", "breakdown": ["Parameter Store Standard: Free for configs, 10K parameter limit", "Secrets Manager: $0.40/secret/month  5 = $2/month", "Automatic rotation: Native RDS/Aurora integration", "Versioning: Both services track parameter history", "Cost comparison: $2/month vs $82/month for all in Secrets Manager"], "otherOptions": "A) Secrets Manager costs $82/month for 205 secrets vs $2 for optimal approach\\nC) Parameter Store lacks native database rotation capabilities\\nD) AppConfig designed for feature flags, not credential management"}	f
132	236	135	Database - Multi-Region	Expert	Design Resilient Architectures	A gaming company operates globally with users in US, EU, and APAC. They need <100ms read/write latency in all regions, automatic failover if a region fails, and ability to handle traffic spikes from 10K to 500K concurrent users within minutes. Game session data must be immediately consistent across regions to prevent duplicate rewards. Which architecture meets these requirements?	[{"text": "A) DynamoDB Global Tables with On-Demand capacity mode", "isCorrect": true}, {"text": "B) Aurora Global Database with read replicas in each region", "isCorrect": false}, {"text": "C) S3 Cross-Region Replication with Transfer Acceleration", "isCorrect": false}, {"text": "D) ElastiCache Global Datastore with Redis", "isCorrect": false}]	A) DynamoDB Global Tables with On-Demand capacity mode	DynamoDB Global Tables provide multi-master writes in all regions with <100ms replication, last-writer-wins conflict resolution, and automatic failover. On-Demand mode handles 10K to 500K spikes instantly with no capacity planning.	{"summary": "Global multi-master architecture:", "breakdown": ["Global Tables: Multi-region, multi-master writes", "Sub-second replication: <100ms cross-region consistency", "On-Demand: Automatic scaling from 0 to millions of requests", "Last-writer-wins: Prevents duplicate rewards through timestamps", "Automatic failover: Survive complete region failure"], "otherOptions": "B) Aurora Global has single write region, 1-second replication lag\\nC) S3 eventual consistency violates gaming session requirements\\nD) ElastiCache is cache layer, not primary database for critical data"}	f
133	237	136	Monitoring - Security Analytics	Advanced	Design Secure Architectures	A security team needs to identify the top IP addresses making failed API authentication attempts, track which IAM users generate the most AccessDenied errors, and find resources receiving the highest volumes of unauthorized access attempts. They need real-time visibility without building custom log analysis infrastructure. Which AWS service provides this capability most efficiently?	[{"text": "A) CloudWatch Logs Insights with scheduled queries", "isCorrect": false}, {"text": "B) CloudWatch Contributor Insights analyzing CloudTrail logs", "isCorrect": true}, {"text": "C) GuardDuty with custom threat intelligence feeds", "isCorrect": false}, {"text": "D) Athena querying S3-stored CloudTrail logs", "isCorrect": false}]	B) CloudWatch Contributor Insights analyzing CloudTrail logs	Contributor Insights automatically analyzes CloudTrail logs to find top contributors (IPs, users, resources) for specific events like AccessDenied. It provides real-time rankings, built-in rules for security events, and visual displays without custom queries.	{"summary": "Automated security analytics:", "breakdown": ["Contributor Insights: Pre-built rules for CloudTrail analysis", "Top contributors: Automatic ranking by IP, user, resource", "Real-time: Results available within minutes", "No infrastructure: Fully managed service, no servers", "Built-in patterns: AccessDenied, UnauthorizedOperation rules"], "otherOptions": "A) Logs Insights requires writing custom queries and scheduling\\nC) GuardDuty detects threats but doesn't provide top contributor analytics\\nD) Athena requires custom queries, table management, and isn't real-time"}	f
134	238	137	Disaster Recovery - Backup Strategy	Advanced	Design Resilient Architectures	An enterprise runs 500 EC2 instances, 50 RDS databases, and 200 EBS volumes across 20 AWS accounts. They need centralized backup management with 7-day retention in source region and 90-day retention in DR region. Compliance requires backups in separate AWS account to protect against account compromise. Which solution provides centralized management with minimal operational overhead?	[{"text": "A) AWS Backup with cross-region, cross-account backup policies using AWS Organizations", "isCorrect": true}, {"text": "B) Lambda functions in each account calling snapshot APIs with cross-region copy", "isCorrect": false}, {"text": "C) Data Lifecycle Manager policies per account with SNS notifications", "isCorrect": false}, {"text": "D) CloudFormation StackSets deploying backup scripts across accounts", "isCorrect": false}]	A) AWS Backup with cross-region, cross-account backup policies using AWS Organizations	AWS Backup with Organizations enables centralized backup policies across all accounts, automatic cross-region copying, vault lock for immutability, and consolidated monitoring. Single pane of glass management for 750+ resources.	{"summary": "Centralized enterprise backup:", "breakdown": ["Organizations integration: Policy applies to all accounts", "Cross-account vaults: Backups in separate security boundary", "Cross-region copy: Automatic DR region replication", "Vault Lock: Immutable backups prevent ransomware deletion", "Unified monitoring: Single dashboard for 20 accounts"], "otherOptions": "B) Custom Lambda solution requires extensive code, error handling per account\\nC) DLM doesn't support RDS, lacks cross-account capability\\nD) StackSets deploys infrastructure, doesn't provide backup service"}	f
135	239	138	Serverless - GraphQL API	Expert	Design High-Performing Architectures	A mobile app needs a flexible API allowing clients to request exactly the data they need, reducing mobile bandwidth usage by 60%. Backend consists of DynamoDB tables, S3 objects, and complex business logic requiring Lambda. The API needs real-time subscriptions for chat features and automatic caching. Which API architecture provides optimal functionality?	[{"text": "A) API Gateway REST API with Lambda integrations for all operations", "isCorrect": false}, {"text": "B) AppSync GraphQL API with DynamoDB resolvers and Lambda for complex logic", "isCorrect": true}, {"text": "C) API Gateway HTTP API with Lambda handling GraphQL parsing", "isCorrect": false}, {"text": "D) ALB with Lambda targets implementing custom GraphQL server", "isCorrect": false}]	B) AppSync GraphQL API with DynamoDB resolvers and Lambda for complex logic	AppSync provides managed GraphQL with direct DynamoDB resolvers (eliminating Lambda for simple queries), built-in real-time WebSocket subscriptions, automatic response caching, and Lambda integration for complex business logic. GraphQL clients request only needed fields, reducing bandwidth.	{"summary": "AppSync GraphQL benefits:", "breakdown": ["Direct DynamoDB resolvers: No Lambda overhead for simple queries", "Flexible queries: Clients request specific fields, reduce bandwidth", "Real-time subscriptions: WebSocket support for chat features", "Automatic caching: Response caching at edge and API level", "Unified API: Single endpoint for DynamoDB, S3, Lambda data"], "otherOptions": "A) REST API requires multiple endpoints, overfetches data\\nC) HTTP API lacks built-in subscriptions, manual GraphQL parsing in Lambda\\nD) Custom GraphQL server requires management, scaling, caching implementation"}	f
136	240	139	Containers - Service Mesh	Advanced	Design High-Performing Architectures	A microservices application on ECS has 50 services communicating via ALBs. Teams complain about complex service discovery (updating ALB DNS in env vars), lack of traffic metrics between services, and need for connection retry logic in every service. They want simplified networking without adopting Kubernetes. Which ECS feature addresses these concerns?	[{"text": "A) ECS Service Connect with automatic service mesh", "isCorrect": true}, {"text": "B) ECS Service Discovery with Route 53 private hosted zones", "isCorrect": false}, {"text": "C) App Mesh with manual Envoy proxy injection", "isCorrect": false}, {"text": "D) Internal NLB per service with Lambda for service registry", "isCorrect": false}]	A) ECS Service Connect with automatic service mesh	ECS Service Connect provides service mesh capabilities (traffic routing, retries, timeouts, circuit breaking) without Envoy sidecar management. It includes automatic service discovery using simple namespace names, built-in CloudWatch metrics for service-to-service traffic, and integrates natively with ECS.	{"summary": "ECS Service Connect features:", "breakdown": ["Service mesh: Automatic retries, timeouts, circuit breakers", "Simple naming: Reference services by namespace name, not ALB DNS", "Traffic metrics: Service-to-service latency, success rates", "No sidecars to manage: AWS-managed infrastructure", "ECS native: No migration to Kubernetes required"], "otherOptions": "B) Service Discovery provides DNS but no traffic management or metrics\\nC) App Mesh requires manual Envoy sidecar deployment and management\\nD) Custom solution adds complexity, lacks retry/circuit breaker logic"}	f
137	241	140	Serverless - Scheduled Tasks	Intermediate	Design Cost-Optimized Architectures	A company needs to schedule 1 million individual reminder tasks (email/SMS) based on customer preferences: some daily, some weekly, some at specific times. Current Lambda + DynamoDB polling solution costs $5K/month and misses time-sensitive reminders. They need accurate scheduling with per-customer customization. Which service provides cost-effective, scalable scheduling?	[{"text": "A) EventBridge Scheduler with one-time and recurring schedules", "isCorrect": true}, {"text": "B) CloudWatch Events rules for each schedule pattern", "isCorrect": false}, {"text": "C) Step Functions with Wait states for each reminder", "isCorrect": false}, {"text": "D) SQS with message delay and Lambda polling", "isCorrect": false}]	A) EventBridge Scheduler with one-time and recurring schedules	EventBridge Scheduler supports millions of schedules with flexible recurrence patterns, one-time schedules, and pay-per-invocation pricing ($1.00 per million invocations). It targets 170+ AWS services and eliminates polling overhead.	{"summary": "Scalable scheduling solution:", "breakdown": ["Million+ schedules: Designed for high-volume scheduling", "Flexible patterns: One-time, cron expressions, custom intervals", "Pay-per-invocation: $1.00 per million vs $5K/month polling", "Direct targets: Invoke Lambda, SQS, SNS, EventBridge bus", "Time windows: Flexible schedule with configurable windows"], "otherOptions": "B) CloudWatch Events limited to fixed intervals, not per-customer schedules\\nC) Step Functions charges per state transition, expensive at scale\\nD) SQS max delay 15 minutes, requires continuous polling"}	f
138	242	141	Networking - Hub and Spoke	Expert	Design Secure Architectures	An enterprise has 50 VPCs across multiple regions requiring centralized egress filtering, intrusion prevention, and URL filtering for all internet-bound traffic. Current approach uses NAT Gateway per VPC with no filtering. Security team needs stateful inspection, domain-based filtering, and centralized rule management. Which architecture provides required security controls cost-effectively?	[{"text": "A) Transit Gateway with centralized Network Firewall in dedicated security VPC", "isCorrect": true}, {"text": "B) Security groups and NACLs in each VPC", "isCorrect": false}, {"text": "C) Third-party firewall instances in each VPC with Auto Scaling", "isCorrect": false}, {"text": "D) AWS WAF on CloudFront distributions", "isCorrect": false}]	A) Transit Gateway with centralized Network Firewall in dedicated security VPC	Transit Gateway enables hub-and-spoke topology routing all internet traffic through central security VPC. Network Firewall provides stateful inspection, Suricata-compatible IPS rules, domain filtering, and centralized managementall egress traffic filtered before reaching IGW.	{"summary": "Centralized egress security architecture:", "breakdown": ["Transit Gateway: Hub-and-spoke routes all VPC traffic centrally", "Network Firewall: Stateful inspection, IPS, domain filtering", "Single management: Rules apply to all 50 VPCs", "Cost effective: One firewall deployment vs 50 per-VPC solutions", "Logging: Centralized flow logs and alert aggregation"], "otherOptions": "B) Security groups/NACLs lack stateful inspection and URL filtering\\nC) Third-party firewalls in 50 VPCs expensive, complex management\\nD) WAF protects web applications, not general egress traffic"}	f
139	243	142	Analytics - Data Lake Security	Expert	Design Secure Architectures	A healthcare company builds a data lake with sensitive PHI in S3. Data science team needs row-level and column-level access controls based on user department. Current S3 bucket policies are too coarse-grained. They need to query with Athena while enforcing fine-grained permissions without modifying queries. Which service provides required access controls?	[{"text": "A) AWS Lake Formation with cell-level security and data filters", "isCorrect": true}, {"text": "B) S3 Access Points with separate policies per department", "isCorrect": false}, {"text": "C) IAM policies with condition keys for S3 objects", "isCorrect": false}, {"text": "D) AWS Glue with custom ETL jobs filtering data", "isCorrect": false}]	A) AWS Lake Formation with cell-level security and data filters	Lake Formation provides row-level and column-level security for data lake S3 data. It integrates with Athena, automatically filtering query results based on user permissions without query modification. Cell-level security masks specific columns based on IAM principal.	{"summary": "Fine-grained data lake security:", "breakdown": ["Row-level filters: Users see only authorized rows", "Column-level security: Sensitive columns hidden per user", "Athena integration: Transparent filtering in queries", "Centralized permissions: Single policy for all query engines", "Audit: CloudTrail logs all data access attempts"], "otherOptions": "B) Access Points provide bucket-level controls, not row/column filtering\\nC) IAM condition keys can't filter query results at row/column level\\nD) ETL filtering requires separate datasets per department, data duplication"}	f
140	244	143	DevOps - Deployment Strategy	Advanced	Design Resilient Architectures	An application on ECS requires zero-downtime deployments with automatic rollback if error rate exceeds 2% or latency exceeds 500ms after deployment. Deployments must complete in under 10 minutes. Current rolling deployments take 30 minutes and caused a production outage from a bad deployment. Which deployment strategy with automated validation provides the safest deployments?	[{"text": "A) CodeDeploy blue/green deployment with CloudWatch alarms for auto-rollback", "isCorrect": true}, {"text": "B) ECS rolling updates with health checks", "isCorrect": false}, {"text": "C) CodeDeploy linear deployment with manual approval", "isCorrect": false}, {"text": "D) Lambda aliases with traffic shifting and alarms", "isCorrect": false}]	A) CodeDeploy blue/green deployment with CloudWatch alarms for auto-rollback	CodeDeploy blue/green for ECS creates new task set alongside old, shifts traffic via ALB, monitors CloudWatch alarms (error rate, latency), and automatically rolls back if thresholds breached. Complete traffic shift in <10 minutes with instant rollback capability.	{"summary": "Safe deployment with validation:", "breakdown": ["Blue/green: New version deployed alongside old", "Traffic shifting: Gradual shift from blue to green via ALB", "Alarm monitoring: Error rate and latency thresholds", "Automatic rollback: Revert traffic if alarms trigger", "Fast deployment: Traffic shift in minutes vs 30-minute rolling"], "otherOptions": "B) Rolling updates lack automatic rollback capability\\nC) Manual approval doesn't detect runtime issues like latency\\nD) Question asks about ECS, not Lambda deployments"}	f
141	245	144	Machine Learning - Model Hosting	Expert	Design Cost-Optimized Architectures	An ML platform hosts 500 recommendation models (one per customer) averaging 2 requests/minute per model. Current approach deploys one SageMaker endpoint per model costing $180K/month with 90% idle time. Models are small (200MB) with similar inference latency requirements. Which deployment strategy reduces costs while maintaining performance?	[{"text": "A) SageMaker Multi-Model Endpoints hosting all 500 models on shared instances", "isCorrect": true}, {"text": "B) Lambda functions with models in EFS", "isCorrect": false}, {"text": "C) ECS tasks with models in S3, one task per model", "isCorrect": false}, {"text": "D) SageMaker Serverless Inference per model", "isCorrect": false}]	A) SageMaker Multi-Model Endpoints hosting all 500 models on shared instances	Multi-Model Endpoints load models dynamically from S3 into memory as needed, share infrastructure across models, and automatically cache frequently used models. Perfect for low-traffic scenarios with many models. Reduces costs by 90% compared to dedicated endpoints.	{"summary": "Multi-model hosting optimization:", "breakdown": ["Shared infrastructure: 500 models on few instances", "Dynamic loading: Models loaded from S3 on-demand", "Automatic caching: Frequently used models stay in memory", "Cost savings: $18K/month vs $180K (90% reduction)", "Invocation routing: Specify model in request path"], "otherOptions": "B) Lambda cold starts problematic for ML inference, 15-minute timeout limit\\nC) ECS tasks still provision one per model, minimal cost savings\\nD) Serverless Inference costs more at 2 req/min than multi-model shared"}	f
142	246	145	Networking - Advanced Routing	Advanced	Design High-Performing Architectures	A global e-commerce platform has infrastructure in us-east-1, eu-west-1, and ap-southeast-1. They want to route users to nearest region but direct 20% of North American traffic to eu-west-1 for A/B testing. If primary region fails, traffic should automatically shift to next nearest healthy region. Which Route 53 routing strategy provides this sophisticated traffic management?	[{"text": "A) Geoproximity routing with bias adjustment and health checks", "isCorrect": true}, {"text": "B) Geolocation routing with failover records", "isCorrect": false}, {"text": "C) Latency-based routing with weighted records", "isCorrect": false}, {"text": "D) Multivalue answer routing with health checks", "isCorrect": false}]	A) Geoproximity routing with bias adjustment and health checks	Geoproximity routes based on user and resource location, supports bias values to expand/shrink geographic regions (for A/B testing), and integrates with health checks for automatic failover to next nearest region.	{"summary": "Advanced traffic management:", "breakdown": ["Geoproximity: Routes to nearest region by default", "Bias adjustment: Shift traffic (e.g., -30 bias sends 20% elsewhere)", "Health checks: Automatic failover to next nearest healthy region", "Traffic flow visualization: Visual editor for complex policies", "Dynamic routing: Considers both distance and bias"], "otherOptions": "B) Geolocation uses fixed geographic boundaries, can't do percentage splits\\nC) Latency-based ignores geographic preference, weighted can't shift by region\\nD) Multivalue returns multiple IPs, doesn't provide geographic routing control"}	f
143	247	146	Analytics - Stream Processing	Expert	Design High-Performing Architectures	An IoT platform ingests 10M sensor events/second requiring real-time complex event processing: 5-minute tumbling windows for aggregations, joins across 3 streams, and stateful pattern detection. Lambda cannot maintain state across invocations, causing incorrect aggregations. Which streaming solution supports complex stateful processing at this scale?	[{"text": "A) Kinesis Data Analytics for Apache Flink with managed state", "isCorrect": true}, {"text": "B) Lambda with DynamoDB for state storage", "isCorrect": false}, {"text": "C) EMR running Spark Streaming with Parquet checkpointing", "isCorrect": false}, {"text": "D) Kinesis Data Analytics for SQL", "isCorrect": false}]	A) Kinesis Data Analytics for Apache Flink with managed state	Flink provides native support for complex event processing: tumbling/sliding windows, multi-stream joins, stateful operations with managed state backend, exactly-once semantics, and automatic scaling. Designed for high-throughput stream processing with complex logic.	{"summary": "Complex stream processing capabilities:", "breakdown": ["Stateful processing: Managed state backend for aggregations", "Tumbling windows: Built-in 5-minute window support", "Stream joins: Join multiple Kinesis streams with event-time", "Exactly-once: Consistent state and output", "Auto-scaling: Handles 10M events/sec with parallelism"], "otherOptions": "B) Lambda DynamoDB state leads to race conditions at high concurrency\\nC) EMR Spark requires manual cluster management, higher operational cost\\nD) Kinesis SQL limited to basic aggregations, lacks complex pattern matching"}	f
144	248	147	Governance - Multi-Account Strategy	Intermediate	Design Secure Architectures	A growing startup provisions 2-3 new AWS accounts monthly for different teams/projects. Each account needs identical baseline: VPC with standard subnets, CloudTrail enabled, GuardDuty activated, and Config rules for compliance. Current manual setup takes 4 hours per account with configuration drift. Which service automates account provisioning with governance guardrails?	[{"text": "A) AWS Control Tower with Account Factory and customizations", "isCorrect": true}, {"text": "B) Organizations with CloudFormation StackSets", "isCorrect": false}, {"text": "C) Service Catalog with account vending machine", "isCorrect": false}, {"text": "D) AWS Systems Manager automation documents", "isCorrect": false}]	A) AWS Control Tower with Account Factory and customizations	Control Tower Account Factory provisions new accounts in minutes with pre-configured networking, guardrails (SCPs + Config rules), centralized logging, and customizations via Terraform/CloudFormation. Ensures consistent governance across all accounts.	{"summary": "Automated account provisioning:", "breakdown": ["Account Factory: New account in <10 minutes vs 4 hours", "Customizations: Apply standard VPC/networking templates", "Guardrails: Mandatory SCPs and Config rules auto-applied", "Drift detection: Control Tower detects configuration changes", "Centralized logging: CloudTrail logs aggregate in log archive account"], "otherOptions": "B) StackSets require manual trigger per account, no governance framework\\nC) Service Catalog provides self-service but lacks multi-account governance\\nD) Systems Manager documents don't provide account-level governance guardrails"}	f
\.


--
-- Data for Name: comptia_cloud_plus_questions; Type: TABLE DATA; Schema: prepper; Owner: postgres
--

COPY prepper.comptia_cloud_plus_questions (question_id, question_number, category, difficulty, domain, question_text, options, correct_answer, explanation, explanation_details, multiple_answers, correct_answers, id, cognitive_level, skill_level) FROM stdin;
217	217	DevOps - CI/CD Pipelines	Intermediate	DevOps Fundamentals	A development team wants to implement continuous integration for their cloud application. Which component is MOST critical for a successful CI pipeline?	[{"text": "Automated testing at each commit", "isCorrect": true}, {"text": "Manual code review process", "isCorrect": false}, {"text": "Weekly deployment schedule", "isCorrect": false}, {"text": "Database backup automation", "isCorrect": false}]	Automated testing at each commit	Continuous Integration requires automated testing to catch issues early. Tests run automatically on every code commit, providing immediate feedback to developers.	{"summary": "CI Pipeline Requirements:", "breakdown": ["Automated builds on every commit", "Automated unit and integration tests", "Fast feedback loops (under 10 minutes)", "Version control integration"], "otherOptions": "Manual reviews are good but slow down CI\\\\nWeekly deployments are too infrequent\\\\nBackups are important but not CI-specific"}	\N	\N	1	\N	\N
221	221	DevOps - Container Orchestration	Beginner	DevOps Fundamentals	What is the primary purpose of Kubernetes in a cloud environment?	[{"text": "To replace virtual machines entirely", "isCorrect": false}, {"text": "To orchestrate and manage containerized applications at scale", "isCorrect": true}, {"text": "To provide serverless computing capabilities", "isCorrect": false}, {"text": "To eliminate the need for load balancers", "isCorrect": false}]	To orchestrate and manage containerized applications at scale	Kubernetes automates deployment, scaling, and management of containerized applications across clusters of hosts. It handles container lifecycle, networking, and resource allocation.	{"summary": "Kubernetes Key Functions:", "breakdown": ["Automatic container scheduling and placement", "Self-healing: restart failed containers", "Horizontal scaling based on metrics", "Service discovery and load balancing"], "otherOptions": "Kubernetes runs on VMs, doesn't replace them\\\\nServerless is a different paradigm (e.g., AWS Lambda)\\\\nLoad balancing is a Kubernetes feature, not eliminated"}	\N	\N	2	\N	\N
220	220	DevOps - Infrastructure as Code	Advanced	DevOps Fundamentals	A DevOps team uses Terraform to manage cloud infrastructure. They need to ensure that infrastructure changes are reviewed and tested before applying to production. What is the BEST workflow?	[{"text": "Apply changes directly to production after local testing", "isCorrect": false}, {"text": "Use Terraform plan in CI, apply after peer review and approval", "isCorrect": true}, {"text": "Manually review Terraform files without using plan", "isCorrect": false}, {"text": "Apply changes during low-traffic hours without review", "isCorrect": false}]	Use Terraform plan in CI, apply after peer review and approval	Terraform plan shows proposed changes without applying them. Running in CI with peer review ensures changes are validated before production deployment.	{"summary": "IaC Best Practices:", "breakdown": ["Run 'terraform plan' in CI pipeline", "Require peer review for infrastructure changes", "Test in non-production environments first", "Use state locking to prevent concurrent modifications"], "otherOptions": "Direct production changes are risky\\\\nManual review can miss configuration errors\\\\nTiming doesn't replace proper review process"}	\N	\N	3	\N	\N
219	219	DevOps - Infrastructure as Code	Intermediate	DevOps Fundamentals	What is the primary benefit of using Infrastructure as Code (IaC) tools like Terraform or CloudFormation?	[{"text": "Eliminates the need for cloud expertise", "isCorrect": false}, {"text": "Provides version-controlled, repeatable infrastructure deployments", "isCorrect": true}, {"text": "Automatically optimizes cloud costs", "isCorrect": false}, {"text": "Removes the need for security configurations", "isCorrect": false}]	Provides version-controlled, repeatable infrastructure deployments	IaC treats infrastructure configuration as code, enabling version control, peer review, automated testing, and consistent deployments across environments.	{"summary": "Infrastructure as Code Benefits:", "breakdown": ["Version control for infrastructure changes", "Consistent deployments across environments", "Self-documenting infrastructure", "Enables disaster recovery and environment replication"], "otherOptions": "IaC requires cloud knowledge to write templates\\\\nCost optimization requires separate tools\\\\nSecurity must still be explicitly configured"}	\N	\N	4	\N	\N
222	222	DevOps - Container Orchestration	Advanced	DevOps Fundamentals	A microservices application deployed on Kubernetes experiences intermittent connection failures between services. Which Kubernetes feature should be investigated FIRST?	[{"text": "Ingress controller configuration", "isCorrect": false}, {"text": "Service mesh implementation", "isCorrect": false}, {"text": "Service discovery and DNS resolution", "isCorrect": true}, {"text": "Pod CPU limits", "isCorrect": false}]	Service discovery and DNS resolution	Intermittent connection issues between services typically indicate DNS resolution problems or service discovery failures. Kubernetes DNS (CoreDNS) enables service-to-service communication.	{"summary": "Kubernetes Networking Troubleshooting:", "breakdown": ["Check CoreDNS pod health and logs", "Verify Service definitions and selectors", "Test DNS resolution from within pods", "Ensure network policies don't block traffic"], "otherOptions": "Ingress handles external traffic, not inter-service\\\\nService mesh is optional and adds complexity\\\\nCPU limits affect performance, not connectivity"}	\N	\N	5	\N	\N
223	223	DevOps - Configuration Management	Intermediate	DevOps Fundamentals	An organization needs to ensure consistent server configurations across 500 cloud instances. Which approach BEST aligns with DevOps principles?	[{"text": "Manually configure each server following a checklist", "isCorrect": false}, {"text": "Create a golden image and deploy copies", "isCorrect": false}, {"text": "Use configuration management tools like Ansible or Chef", "isCorrect": true}, {"text": "Write custom shell scripts for each server", "isCorrect": false}]	Use configuration management tools like Ansible or Chef	Configuration management tools provide idempotent, automated configuration that can be version-controlled and tested. They ensure consistency and enable configuration drift detection.	{"summary": "Configuration Management Benefits:", "breakdown": ["Idempotent operations: safe to run multiple times", "Version-controlled configurations", "Automated compliance and drift detection", "Scalable to thousands of nodes"], "otherOptions": "Manual configuration is error-prone and slow\\\\nGolden images become stale and lack flexibility\\\\nCustom scripts are hard to maintain and test"}	\N	\N	6	\N	\N
224	224	DevOps - GitOps Workflows	Advanced	DevOps Fundamentals	A team wants to implement GitOps for their Kubernetes deployments. What is the core principle of GitOps?	[{"text": "All deployment automation scripts stored in Git", "isCorrect": false}, {"text": "Git repository serves as the single source of truth for declarative infrastructure", "isCorrect": true}, {"text": "Using GitHub for code reviews", "isCorrect": false}, {"text": "Automatic merging of all Git branches", "isCorrect": false}]	Git repository serves as the single source of truth for declarative infrastructure	GitOps uses Git as the single source of truth. Desired system state is declared in Git, and automated agents ensure the actual state matches. All changes go through Git workflows (PR, review, merge).	{"summary": "GitOps Principles:", "breakdown": ["Declarative infrastructure and application definitions", "Git as single source of truth", "Automated synchronization from Git to clusters", "Pull-based deployment model for security"], "otherOptions": "It's about the workflow, not just storage location\\\\nCode review is part of it but not the core principle\\\\nAutomatic merging contradicts proper review processes"}	\N	\N	7	\N	\N
173	188	Automation	Knowledge	DevOps and Automation	Which of the following is a declarative Infrastructure as Code (IaC) tool?	[{"text": "A bash script with a series of CLI commands.", "isCorrect": false}, {"text": "An Ansible playbook.", "isCorrect": false}, {"text": "A Terraform configuration file.", "isCorrect": true}, {"text": "A Python script using a cloud SDK.", "isCorrect": false}]	A Terraform configuration file.	Terraform is a prime example of a declarative IaC tool. You define the desired end state of your infrastructure in HCL (HashiCorp Configuration Language), and Terraform's engine figures out the necessary API calls to create, update, or delete resources to achieve that state.	{"summary": "Terraform is a declarative IaC tool.", "breakdown": ["You declare *what* you want, not *how* to create it.", "Terraform builds a dependency graph and executes actions in the correct order.", "It is idempotent, meaning you can apply the same configuration multiple times with no changes after the first successful run."], "otherOptions": "A, Bash and Python scripts are imperative; they define the specific, step-by-step commands to execute.\\nAnsible is largely declarative but is primarily a configuration management tool, not an infrastructure provisioning tool, although it can do both."}	1	{"A Terraform configuration file."}	8	\N	\N
148	164	Operations	Knowledge	Operations and Support	What does it mean for an Infrastructure as Code (IaC) template to be idempotent?	[{"text": "It can only be used one time before it needs to be rewritten.", "isCorrect": false}, {"text": "It will result in the same defined end state regardless of how many times it is applied.", "isCorrect": true}, {"text": "It can be used to deploy infrastructure to multiple cloud providers simultaneously.", "isCorrect": false}, {"text": "It automatically encrypts all resources that it creates.", "isCorrect": false}]	It will result in the same defined end state regardless of how many times it is applied.	Idempotence is a key principle of modern IaC and configuration management. It means that running the operation multiple times will have the same result as running it once. For IaC, this means if you apply a template to create a server, and then apply the same template again, it will recognize the server already exists and make no changes.	{"summary": "Idempotency means repeated applications result in the same state.", "breakdown": ["It makes deployments predictable and safe to re-run.", "If a deployment fails midway, you can simply run it again to complete the setup.", "It is the core principle that allows IaC tools to manage the state of infrastructure over time."], "otherOptions": "This is incorrect; the goal is reusability.\\nThis describes a cloud-agnostic tool, which is a different concept.\\nEncryption is a configuration choice within the template, not an inherent property of idempotency."}	1	{"It will result in the same defined end state regardless of how many times it is applied."}	9	\N	\N
22	24	Deployments - Infrastructure as Code	Application	Cloud Deployment	Your team needs to deploy identical environments across development, staging, and production. Which approach ensures consistency and reduces manual errors?	[{"text": "Copying virtual machine images", "isCorrect": false}, {"text": "Using different configurations for each environment", "isCorrect": false}, {"text": "Infrastructure as Code (IaC) templates", "isCorrect": true}, {"text": "Manual deployment through cloud console", "isCorrect": false}]	Infrastructure as Code (IaC) templates	IaC templates ensure consistent, repeatable, and version-controlled infrastructure deployments.	{"summary": "IaC benefits for environment consistency:", "breakdown": ["Version-controlled infrastructure definitions", "Automated and repeatable deployments", "Eliminates configuration drift", "Enables testing of infrastructure changes"], "otherOptions": "Manual processes are error-prone\\nVM images don't cover full infrastructure\\nDifferent configs create inconsistency"}	\N	\N	10	\N	\N
80	80	Infrastructure as Code	Application	Cloud Deployment	A DevOps team manages infrastructure across development, staging, and production environments. They experience configuration drift and inconsistencies between environments, leading to deployment failures. Which approach would BEST solve these consistency issues?	[{"text": "Use configuration management scripts", "isCorrect": false}, {"text": "Create golden images for all server configurations", "isCorrect": false}, {"text": "Implement Infrastructure as Code (IaC) with version control", "isCorrect": true}, {"text": "Document all configurations in detailed runbooks", "isCorrect": false}]	Implement Infrastructure as Code (IaC) with version control	Infrastructure as Code ensures consistent, repeatable deployments across environments by defining infrastructure in version-controlled code templates.	{"summary": "IaC benefits for environment consistency:", "breakdown": ["Declarative definitions: Infrastructure defined as code templates", "Version control: Track and rollback infrastructure changes", "Consistency: Identical deployments across all environments", "Automation: Eliminates manual configuration errors"], "otherOptions": "Documentation doesn't prevent manual configuration errors\\nGolden images don't address infrastructure configuration drift\\nScripts can vary in execution and may not be declarative"}	\N	\N	11	\N	\N
20	22	Deployments - Migration Types	Application	Cloud Deployment	Your legacy application needs significant changes to take advantage of cloud-native features like auto-scaling and managed services. Which migration approach is most appropriate?	[{"text": "Retaining", "isCorrect": false}, {"text": "Refactoring", "isCorrect": true}, {"text": "Retiring", "isCorrect": false}, {"text": "Rehosting", "isCorrect": false}]	Refactoring	Refactoring involves modifying applications to take advantage of cloud-native features and services.	{"summary": "Refactoring characteristics:", "breakdown": ["Modifies applications for cloud optimization", "Enables use of managed services", "Improves scalability and performance", "Higher effort than rehosting but better ROI"], "otherOptions": "Rehosting doesn't modify applications\\nRetiring eliminates the application\\nRetaining keeps app on-premises"}	\N	\N	12	\N	\N
72	71	DevOps - Automation	Application	DevOps and Automation	A DevOps team wants to automate the provisioning of new virtual machines, network configurations, and security groups whenever a new project starts. Which practice is best suited for this task?	[{"text": "Implementing Infrastructure as Code (IaC) with templating tools.", "isCorrect": true}, {"text": "Using shell scripts for server setup and manual network configuration.", "isCorrect": false}, {"text": "Manual configuration via cloud console for each project.", "isCorrect": false}, {"text": "Creating a comprehensive manual checklist for infrastructure setup.", "isCorrect": false}]	Implementing Infrastructure as Code (IaC) with templating tools.	Infrastructure as Code (Iaallows defining and provisioning infrastructure using code, ensuring repeatability, consistency, and reduced manual errors for new project environments.	{"summary": "IaC for automated provisioning:", "breakdown": ["**Repeatability:** Ensures identical environments are deployed every time.", "**Consistency:** Eliminates configuration drift between environments.", "**Reduced Errors:** Automates complex setup processes, minimizing human error.", "**Version Control:** Infrastructure definitions can be versioned and managed like application code."], "otherOptions": "Manual configuration is prone to errors and inconsistencies, especially for complex setups. \\nShell scripts can automate some tasks but typically lack the comprehensive state management and idempotency of IaC tools for full infrastructure provisioning. Network configuration would still likely be manual or poorly managed. \\nA manual checklist helps but does not automate or guarantee consistency, nor does it reduce the time spent on manual setup."}	\N	\N	13	\N	\N
218	217	Operations - Cost Management	Comprehension	Operations and Support	Which of the following BEST summarizes key cost considerations for cloud usage?	[{"text": "A) The main focus of cloud cost management is on dedicated host billing models", "isCorrect": false}, {"text": "B) Cloud cost management is primarily focused on minimizing upfront costs rather than ongoing usage optimization", "isCorrect": false}, {"text": "C) Cloud costs involve implementing tagging for continuously rightsizing resources", "isCorrect": true}, {"text": "D) Most cloud costs are predictable, making real-time monitoring and usage optimization less critical for cost management", "isCorrect": false}]	C) Cloud costs involve implementing tagging for continuously rightsizing resources	Effective cloud cost management requires comprehensive tagging for cost allocation and visibility, combined with continuous rightsizing to ensure resources match actual workload requirements. These practices enable organizations to track spending by team/project and eliminate waste from over-provisioned resources.	{"summary": "Key cloud cost management practices:", "breakdown": ["Tagging strategy: Label all resources with project, team, environment, cost-center", "Cost allocation: Track spending by department, application, or business unit", "Continuous rightsizing: Regularly analyze and adjust resource sizes to match demand", "Eliminate waste: Identify and remove idle, unused, or over-provisioned resources", "Reserved capacity: Use commitments for predictable workloads (30-70% savings)", "Real-time monitoring: Set budgets, alerts, and anomaly detection for cost spikes"], "otherOptions": "A) Cloud cost management covers all pricing models, not just dedicated hosts\\\\nB) Cloud advantage is low upfront costs; challenge is managing ongoing usage\\\\nD) Cloud costs are often unpredictable due to elasticity - monitoring IS critical"}	\N	\N	14	\N	\N
232	335	Cloud Architecture - Multi-tenant Design	Intermediate	Cloud Architecture and Design	A healthcare SaaS provider needs to ensure their multi-tenant application isolates customer data while maximizing resource efficiency. They want to prevent one tenant's workload from impacting another's performance. What architecture pattern should they implement?	[{"text": "Single database with shared tables and tenant ID filtering", "isCorrect": false}, {"text": "Separate database instances for each tenant", "isCorrect": false}, {"text": "Resource quotas and container-based isolation per tenant", "isCorrect": true}, {"text": "Single application instance serving all tenants without isolation", "isCorrect": false}]	Resource quotas and container-based isolation per tenant	Resource quotas combined with container-based isolation provide the optimal balance between security, performance isolation, and resource efficiency in multi-tenant architectures. This approach ensures that each tenant's workload runs in isolated containers with defined CPU, memory, and I/O limits, preventing 'noisy neighbor' issues where one tenant's resource consumption affects others.	{"summary": "Multi-tenant Isolation Strategies:", "breakdown": ["Container-based isolation provides process and resource separation", "Resource quotas prevent resource exhaustion by single tenants", "More cost-effective than full database instance separation", "Kubernetes namespaces and resource limits enforce boundaries"], "otherOptions": "Single database filtering offers weak isolation\\nSeparate instances are costly and inefficient\\nNo isolation creates security and performance risks"}	\N	\N	15	\N	\N
233	336	Cloud Architecture - Scaling Strategies	Intermediate	Cloud Architecture and Design	A global e-commerce company experiences seasonal traffic spikes during holiday shopping events. Their application currently uses fixed-capacity infrastructure. What combination of cloud features would BEST handle these predictable demand patterns while minimizing costs?	[{"text": "Over-provision resources year-round to handle peak capacity", "isCorrect": false}, {"text": "Scheduled auto-scaling with predictive scaling policies", "isCorrect": true}, {"text": "Manual scaling based on real-time monitoring alerts", "isCorrect": false}, {"text": "Reactive auto-scaling based only on CPU utilization thresholds", "isCorrect": false}]	Scheduled auto-scaling with predictive scaling policies	Scheduled auto-scaling with predictive scaling policies is ideal for handling known, recurring traffic patterns like holiday shopping events. This approach allows the system to proactively scale resources before demand increases, ensuring capacity is available when needed without performance degradation.	{"summary": "Predictive Scaling Benefits:", "breakdown": ["Scheduled scaling prepares for known peak periods (Black Friday, etc.)", "Predictive policies use ML to forecast demand from historical data", "Proactive scaling prevents lag from reactive-only approaches", "Significantly reduces costs compared to constant over-provisioning"], "otherOptions": "Over-provisioning wastes resources during off-peak times\\nManual scaling is error-prone and too slow\\nReactive-only scaling has lag time during rapid spikes"}	\N	\N	16	\N	\N
234	337	Cloud Architecture - Availability	Advanced	Cloud Architecture and Design	An enterprise is designing a cloud architecture that must maintain operations even if an entire availability zone fails. Their application consists of web servers, application servers, and a relational database. What is the MINIMUM requirement to achieve this level of fault tolerance?	[{"text": "Deploy all components in a single availability zone with backup instances", "isCorrect": false}, {"text": "Distribute application tiers across multiple availability zones with synchronous replication for the database", "isCorrect": true}, {"text": "Use a single region with automatic failover to another region", "isCorrect": false}, {"text": "Deploy read replicas in different regions for disaster recovery", "isCorrect": false}]	Distribute application tiers across multiple availability zones with synchronous replication for the database	To withstand an entire availability zone failure, all critical components must be distributed across multiple availability zones within the same region. This means deploying web servers, application servers, and database instances (or replicas) in at least two different availability zones.	{"summary": "Multi-AZ High Availability Architecture:", "breakdown": ["Deploy each tier (web, app, database) across 2+ availability zones", "Use synchronous replication for databases to prevent data loss", "Load balancers automatically route traffic to healthy zones", "Provides fault tolerance without cross-region complexity"], "otherOptions": "Single AZ deployment loses everything during zone failure\\nCross-region adds latency and complexity for HA\\nRead replicas alone don't provide write availability"}	\N	\N	17	\N	\N
235	338	Cloud Architecture - Storage	Intermediate	Cloud Architecture and Design	A media company needs to design storage architecture for their content delivery platform. They have three types of data: frequently accessed video thumbnails (hot data), completed videos accessed occasionally (warm data), and archived content rarely retrieved (cold data). What storage strategy would optimize costs while meeting performance requirements?	[{"text": "Store all data in high-performance SSD storage for consistent access speeds", "isCorrect": false}, {"text": "Implement lifecycle policies to automatically transition data between storage tiers based on access patterns", "isCorrect": true}, {"text": "Manually move data to cheaper storage after 30 days regardless of access frequency", "isCorrect": false}, {"text": "Use object storage exclusively with no tiering strategy", "isCorrect": false}]	Implement lifecycle policies to automatically transition data between storage tiers based on access patterns	Implementing automated lifecycle policies that transition data between storage tiers based on actual access patterns provides the optimal balance of cost and performance. Cloud providers offer multiple storage tiers with different cost and performance characteristics.	{"summary": "Storage Tiering Strategy:", "breakdown": ["Hot tier: High cost, low latency - for thumbnails and recent content", "Cool/Warm tier: Lower cost, moderate latency - for older videos", "Archive tier: Lowest cost, retrieval time in hours - for archived content", "Automated policies monitor access patterns and transition objects intelligently"], "otherOptions": "All SSD storage is unnecessarily expensive for infrequently accessed data\\nManual transitions can't respond to changing access patterns\\nNo tiering strategy misses significant cost optimization opportunities"}	\N	\N	18	\N	\N
236	339	Cloud Architecture - Compliance	Advanced	Cloud Architecture and Design	A financial services company must comply with regulations requiring data sovereignty, ensuring customer data never leaves specific geographic boundaries. They need a multi-region presence for disaster recovery. What architecture should they implement?	[{"text": "Single region deployment with backup tapes shipped to an off-site location", "isCorrect": false}, {"text": "Multi-region active-active deployment with data replication across all regions", "isCorrect": false}, {"text": "Regional data residency with disaster recovery infrastructure in the same geographic compliance zone", "isCorrect": true}, {"text": "Global CDN caching all customer data for performance", "isCorrect": false}]	Regional data residency with disaster recovery infrastructure in the same geographic compliance zone	Data sovereignty requirements mandate that data remains within specific geographic or legal boundaries. To meet these requirements while maintaining disaster recovery capabilities, the company should deploy infrastructure and disaster recovery resources within regions that comply with the same data sovereignty regulations.	{"summary": "Data Sovereignty Compliance Architecture:", "breakdown": ["Deploy primary and DR infrastructure within the same compliance zone", "Example: EU customer data stays within EU regions (GDPR compliance)", "Ensures data never crosses into non-compliant geographic zones", "Maintains disaster recovery without violating sovereignty requirements"], "otherOptions": "Global replication violates data sovereignty by crossing boundaries\\nBackup tapes provide inadequate RTO for business continuity\\nGlobal CDN caching would distribute sensitive data outside compliance zones"}	\N	\N	19	\N	\N
227	223	Container Technologies - Networking	Intermediate	Cloud Architecture and Design	An application requires network communication between containers across different hosts. Which networking solution is MOST appropriate?	[{"text": "Bridge network mode on each host", "isCorrect": false}, {"text": "Host network mode to eliminate network overhead", "isCorrect": false}, {"text": "Overlay network that spans multiple hosts", "isCorrect": true}, {"text": "None network mode with manual IP configuration", "isCorrect": false}]	Overlay network that spans multiple hosts	An overlay network is specifically designed for container communication across multiple hosts. It creates a virtual network that spans across the physical host infrastructure, allowing containers on different hosts to communicate as if they were on the same network. Container orchestration platforms like Docker Swarm and Kubernetes use overlay networks to enable seamless multi-host container networking with built-in service discovery and load balancing.	{"summary": "Container Networking Modes:", "breakdown": ["Overlay networks create a distributed network across multiple hosts", "Enables containers to communicate across hosts using virtual network", "Kubernetes uses CNI plugins (Calico, Flannel, Weave) for overlay networking", "Docker Swarm uses built-in overlay driver for multi-host communication", "Provides service discovery and load balancing across the cluster"], "otherOptions": "Bridge network is isolated to single host only\\nHost mode shares host network stack, no isolation between containers\\nNone mode disables networking entirely, requiring complex manual configuration"}	\N	\N	20	\N	\N
228	224	Cloud Architecture - Network Components	Advanced	Cloud Architecture and Design	An international organization with data centers spread across different continents aims to optimize data routing between these locations for maximum availability and the shortest path. What solution should they implement to manage this effectively?	[{"text": "Static routing", "isCorrect": false}, {"text": "OSPF (Open Shortest Path First)", "isCorrect": false}, {"text": "BGP (Border Gateway Protocol)", "isCorrect": true}, {"text": "RIP (Routing Information Protocol)", "isCorrect": false}]	BGP (Border Gateway Protocol)	BGP (Border Gateway Protocol) is the routing protocol used to exchange routing information between different autonomous systems (AS) on the internet. It selects the optimal path based on route metrics, policies, and network conditions to forward data. This protocol is crucial for managing and optimizing data traffic across global networks, ensuring that data is forwarded through the most efficient and reliable routes between geographically distributed data centers.	{"summary": "BGP for Global Routing:", "breakdown": ["Designed for inter-domain routing between autonomous systems", "Supports path selection based on multiple attributes (AS path, MED, local preference)", "Provides redundancy and failover for maximum availability", "Scales to handle internet-sized routing tables", "Enables traffic engineering and policy-based routing"], "otherOptions": "Static routing doesn't adapt to network changes or failures\\nOSPF is an interior gateway protocol, not designed for inter-AS routing\\nRIP has limited scalability and slow convergence for global networks"}	\N	\N	21	\N	\N
231	227	Cloud Architecture - Service Models	Intermediate	Cloud Architecture and Design	A manufacturing company needs to configure and deploy IoT sensor data collection. The deployment needs automatic scaling and minimal infrastructure management. Which service model is most appropriate?	[{"text": "IaaS with manual scaling", "isCorrect": false}, {"text": "PaaS with auto-scaling capabilities", "isCorrect": true}, {"text": "SaaS with limited customization", "isCorrect": false}, {"text": "Container as a Service (CaaS)", "isCorrect": false}]	PaaS with auto-scaling capabilities	Platform as a Service (PaaS) with auto-scaling is ideal for IoT sensor data collection because it abstracts infrastructure management while providing the flexibility to configure custom applications. PaaS platforms handle server provisioning, patching, scaling, and maintenance automatically, allowing the manufacturing company to focus on their IoT application logic rather than infrastructure. The built-in auto-scaling ensures the system can handle variable sensor data loads without manual intervention.	{"summary": "PaaS Benefits for IoT Applications:", "breakdown": ["Automatic infrastructure management and scaling", "Built-in services for data ingestion and processing", "Focus on application development rather than operations", "Elastic scaling handles variable IoT workloads", "Reduced operational overhead compared to IaaS"], "otherOptions": "IaaS requires manual infrastructure management and scaling\\nSaaS lacks customization needed for specific IoT data collection\\nCaaS requires more container orchestration knowledge and management"}	\N	\N	23	\N	\N
58	60	Security - Compliance	Knowledge	Cloud Security	Which compliance framework is specifically designed for organizations handling credit card data?	[{"text": "GDPR", "isCorrect": false}, {"text": "HIPAA", "isCorrect": false}, {"text": "SOC 2", "isCorrect": false}, {"text": "PCI DSS", "isCorrect": true}]	PCI DSS	PCI DSS (Payment Card Industry Data Security Standard) is specifically designed for organizations that handle credit card data.	{"summary": "PCI DSS requirements:", "breakdown": ["Secure network and system configuration", "Protect cardholder data", "Maintain vulnerability management program", "Implement access control measures"], "otherOptions": "SOC 2 is for service organizations\\nGDPR is for data privacy\\nHIPAA is for healthcare data"}	\N	\N	25	\N	\N
239	228	Cloud Architecture - Availability	Intermediate	Cloud Architecture and Design	An e-commerce application requires 99.9% uptime with automatic failover capabilities. Which availability zone configuration should you implement?	[{"text": "Single availability zone with redundant instances", "isCorrect": false}, {"text": "Multi-availability zone deployment with load balancing", "isCorrect": true}, {"text": "Single availability zone with manual failover procedures", "isCorrect": false}, {"text": "Cross-region deployment for all components", "isCorrect": false}]	Multi-availability zone deployment with load balancing	Multi-availability zone (Multi-AZ) deployment with load balancing provides the high availability needed to achieve 99.9% uptime. By distributing application instances across multiple physically separated availability zones within the same region, the system can automatically failover if one zone experiences an outage. Load balancers continuously monitor instance health and automatically route traffic away from failed zones to healthy ones, ensuring minimal disruption and meeting the uptime requirement.	{"summary": "Multi-AZ High Availability:", "breakdown": ["Distributes instances across physically separated data centers", "Load balancer provides automatic health checks and failover", "Achieves 99.9% uptime SLA (8.76 hours downtime per year)", "Protects against zone-level failures", "Lower latency than cross-region deployment"], "otherOptions": "Single AZ deployment fails entirely if the zone goes down\\nManual failover is too slow and violates uptime requirements\\nCross-region adds unnecessary complexity and latency for 99.9% target"}	\N	\N	24	\N	\N
242	231	Virtualization	Intermediate	Cloud Architecture and Design	An administrator needs to move a running virtual machine from one physical host to another without any service interruption or downtime. What is this process known as?	[{"text": "Cold migration", "isCorrect": false}, {"text": "Live migration (vMotion)", "isCorrect": true}, {"text": "Snapshot and restore", "isCorrect": false}, {"text": "Failover clustering", "isCorrect": false}]	Live migration (vMotion)	Live migration, also known as vMotion in VMware environments, allows a running virtual machine to be moved from one physical host to another without any downtime or service interruption. The process transfers the VM's active memory, execution state, and storage connections while the VM continues running, making the migration transparent to users and applications.	{"summary": "Live Migration Process:", "breakdown": ["Transfers VM memory and execution state while running", "Maintains active network connections during migration", "Enables zero-downtime maintenance and load balancing", "Requires shared storage or storage migration capability", "VMware vMotion, Hyper-V Live Migration, KVM live migration"], "otherOptions": "Cold migration requires VM shutdown and causes downtime\\nSnapshot and restore is not real-time and causes interruption\\nFailover clustering is for high availability, not seamless migration"}	\N	\N	29	\N	\N
237	233	Cloud Migration	Advanced	Cloud Deployment	A manufacturing company wants to migrate their IoT sensor data processing system. The current system processes data in real-time with sub-millisecond latency requirements. Which migration strategy is MOST appropriate?	[{"text": "Rehost (lift-and-shift)", "isCorrect": false}, {"text": "Re-platform with minor optimizations", "isCorrect": false}, {"text": "Refactor for cloud-native architecture", "isCorrect": true}, {"text": "Replace with SaaS solution", "isCorrect": false}]	Refactor for cloud-native architecture	Refactoring for cloud-native architecture is necessary to meet sub-millisecond latency requirements. This approach redesigns the application to leverage edge computing capabilities integrated with cloud services. The refactored architecture processes time-critical data locally at the edge (near sensors) while using cloud services for analytics, storage, and management. This hybrid edge-cloud design is the only migration strategy that can maintain the sub-millisecond latency requirement.	{"summary": "Cloud-Native Refactoring for IoT:", "breakdown": ["Implements edge computing nodes for local data processing", "Maintains sub-millisecond response times at the edge", "Integrates with cloud for analytics and orchestration", "Uses containerized microservices for flexibility", "Leverages cloud-native tools like Kubernetes for edge management"], "otherOptions": "Rehost moves to centralized cloud, adding 50-200ms+ latency\\nRe-platform still centralizes processing in cloud\\nSaaS solutions introduce even greater network latency"}	\N	\N	31	\N	\N
238	234	Cloud Migration	Intermediate	Cloud Deployment	A company has 700 virtual machines running legacy applications that need to be migrated to the cloud. The applications cannot be modified and must maintain exact configurations. Which migration approach is MOST suitable?	[{"text": "Physical-to-Virtual (P2V) migration", "isCorrect": false}, {"text": "Virtual-to-Virtual (V2V) migration", "isCorrect": true}, {"text": "Application refactoring", "isCorrect": false}, {"text": "Complete application rebuild", "isCorrect": false}]	Virtual-to-Virtual (V2V) migration	Virtual-to-Virtual (V2V) migration is the optimal approach for migrating existing virtual machines to the cloud without modification. V2V tools convert VMs from on-premises hypervisors to cloud-compatible formats while preserving exact configurations, operating systems, applications, and data. This allows legacy applications to run unchanged in the cloud environment, meeting the requirement that applications cannot be modified.	{"summary": "V2V Migration Benefits:", "breakdown": ["Preserves exact VM configurations and application state", "Minimal downtime using incremental replication", "Supports bulk migration of hundreds of VMs", "No application code changes required", "Tools like AWS SMS, Azure Migrate automate the process"], "otherOptions": "P2V is for physical servers, not existing VMs\\nRefactoring requires modifying applications\\nComplete rebuild violates the no-modification requirement"}	\N	\N	32	\N	\N
6	8	Cloud Architecture - Availability	Knowledge	Cloud Architecture and Design	What is the primary purpose of availability zones in cloud computing?	[{"text": "To separate different cloud services", "isCorrect": false}, {"text": "To provide different pricing tiers", "isCorrect": false}, {"text": "To comply with data sovereignty requirements", "isCorrect": false}, {"text": "To provide redundancy and fault tolerance", "isCorrect": true}]	To provide redundancy and fault tolerance	Availability zones provide redundancy and fault tolerance by isolating failures to specific geographic locations.	{"summary": "Availability zone characteristics:", "breakdown": ["Isolated data center locations within a region", "Independent power, cooling, and networking", "Designed to prevent cascading failures", "Enable high availability architecture design"], "otherOptions": "Pricing is not determined by AZ\\nServices can span multiple AZs\\nData sovereignty is handled at region level"}	\N	\N	33	\N	\N
37	39	Operations - Backup Strategies	Comprehension	Cloud Operations and Support	What is the primary advantage of incremental backups over full backups?	[{"text": "Better data compression", "isCorrect": false}, {"text": "Less storage space and faster backup time", "isCorrect": true}, {"text": "Higher reliability", "isCorrect": false}, {"text": "Faster restore times", "isCorrect": false}]	Less storage space and faster backup time	Incremental backups only capture changes since the last backup, requiring less storage and time.	{"summary": "Incremental backup advantages:", "breakdown": ["Only backs up changed data since last backup", "Significantly less storage space required", "Faster backup execution time", "Reduced network bandwidth usage"], "otherOptions": "Restore times are actually slower\\nCompression depends on backup software\\nReliability depends on backup chain integrity"}	\N	\N	34	\N	\N
7	9	Cloud Architecture - Availability	Comprehension	Cloud Architecture and Design	Which cloud strategy allows an organization to handle sudden traffic spikes by temporarily using public cloud resources while maintaining their private cloud for normal operations?	[{"text": "Multi-cloud deployment", "isCorrect": false}, {"text": "Edge computing", "isCorrect": false}, {"text": "Cloud bursting", "isCorrect": true}, {"text": "Hybrid cloud architecture", "isCorrect": false}]	Cloud bursting	Cloud bursting allows organizations to scale from private to public cloud during peak demand periods.	{"summary": "Cloud bursting characteristics:", "breakdown": ["Temporary use of public cloud resources", "Handles unexpected traffic spikes", "Cost-effective scaling approach", "Maintains private cloud for normal operations"], "otherOptions": "Multi-cloud uses multiple providers simultaneously\\nHybrid cloud is permanent architecture\\nEdge computing brings processing closer to users"}	\N	\N	35	\N	\N
2	4	Cloud Architecture - Service Models	Knowledge	Cloud Architecture and Design	Which service model is represented by applications like Salesforce, Office 35, and Gmail?	[{"text": "Software as a Service (SaaS)", "isCorrect": true}, {"text": "Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "Function as a Service (FaaS)", "isCorrect": false}, {"text": "Platform as a Service (PaaS)", "isCorrect": false}]	Software as a Service (SaaS)	SaaS delivers complete software applications over the internet that users access through web browsers.	{"summary": "SaaS characteristics:", "breakdown": ["Complete software applications delivered over internet", "No software installation or maintenance required", "Subscription-based pricing model", "Multi-tenant architecture"], "otherOptions": "IaaS provides infrastructure components\\nPaaS provides development platforms\\nFaaS provides serverless function execution"}	\N	\N	36	\N	\N
3	5	Cloud Architecture - Shared Responsibility	Comprehension	Cloud Architecture and Design	In the shared responsibility model, a customer using Amazon RDS (managed database service) is responsible for which of the following?	[{"text": "Physical security of the data center", "isCorrect": false}, {"text": "Hardware maintenance and replacement", "isCorrect": false}, {"text": "Data encryption and access control configuration", "isCorrect": true}, {"text": "Database engine patching and updates", "isCorrect": false}]	Data encryption and access control configuration	In managed services like RDS, customers are responsible for data security, access controls, and encryption configuration.	{"summary": "Customer responsibilities in managed database services:", "breakdown": ["Data encryption at rest and in transit", "User access management and IAM policies", "Network security groups and firewall rules", "Backup retention and recovery testing"], "otherOptions": "AWS manages engine patching\\nAWS handles physical security\\nAWS manages hardware infrastructure"}	\N	\N	37	\N	\N
4	6	Cloud Architecture - Shared Responsibility	Application	Cloud Architecture and Design	Your organization is using IaaS virtual machines. According to the shared responsibility model, which security aspect is the customer responsible for?	[{"text": "Hypervisor security and maintenance", "isCorrect": false}, {"text": "Physical security of the data center", "isCorrect": false}, {"text": "Network infrastructure hardware security", "isCorrect": false}, {"text": "Operating system patches and configuration", "isCorrect": true}]	Operating system patches and configuration	In IaaS, customers are responsible for securing the guest operating system, including patching and configuration.	{"summary": "Customer responsibilities in IaaS:", "breakdown": ["Guest operating system security and patching", "Application security and configuration", "Network traffic protection (encryption)", "Identity and access management"], "otherOptions": "Physical security is provider responsibility\\nHypervisor is managed by cloud provider\\nNetwork hardware is provider responsibility"}	\N	\N	38	\N	\N
5	7	Cloud Architecture - Availability	Application	Cloud Architecture and Design	Your application requires 99.99% uptime and must survive the failure of an entire data center. Which architecture approach best meets these requirements?	[{"text": "Use larger instance types for better reliability", "isCorrect": false}, {"text": "Deploy across multiple availability zones in the same region", "isCorrect": true}, {"text": "Deploy in a single availability zone with multiple instances", "isCorrect": false}, {"text": "Implement only vertical scaling", "isCorrect": false}]	Deploy across multiple availability zones in the same region	Multi-AZ deployment provides data center-level fault tolerance while maintaining low latency.	{"summary": "Multi-AZ deployment benefits:", "breakdown": ["Survives entire data center failures", "Maintains low latency within region", "Automatic failover capabilities", "Meets high availability requirements (99.99%)"], "otherOptions": "Single AZ cannot survive data center failure\\nInstance size doesn't address availability zones\\nVertical scaling doesn't provide fault tolerance"}	\N	\N	39	\N	\N
9	11	Cloud Architecture - Storage	Comprehension	Cloud Architecture and Design	An application requires high IOPS for database operations and low-level access to storage blocks. Which storage combination is most appropriate?	[{"text": "Object storage with HDD", "isCorrect": false}, {"text": "Block storage with SSD", "isCorrect": true}, {"text": "Object storage with SSD", "isCorrect": false}, {"text": "File storage with SSD", "isCorrect": false}]	Block storage with SSD	Block storage provides low-level access ideal for databases, while SSDs deliver the high IOPS required.	{"summary": "Block storage with SSD advantages:", "breakdown": ["Block-level access optimal for database workloads", "SSD provides high IOPS and low latency", "Direct attachment to compute instances", "Suitable for transactional applications"], "otherOptions": "Object storage lacks low-level access; HDD has lower IOPS\\nFile storage not optimal for databases\\nObject storage doesn't provide block-level access"}	\N	\N	40	\N	\N
10	12	Cloud Architecture - Storage	Application	Cloud Architecture and Design	Your organization needs to store 100TB of archived data that is accessed once per year for compliance purposes. Which storage solution offers the best cost optimization?	[{"text": "Hot storage tier", "isCorrect": false}, {"text": "Archive storage tier", "isCorrect": true}, {"text": "Cold storage tier", "isCorrect": false}, {"text": "Warm storage tier", "isCorrect": false}]	Archive storage tier	Archive storage tier is designed for long-term retention of rarely accessed data with lowest cost.	{"summary": "Archive storage characteristics:", "breakdown": ["Lowest cost storage option", "Designed for long-term retention", "Higher retrieval times and costs", "Ideal for compliance and backup data"], "otherOptions": "Hot storage too expensive for rarely accessed data\\nWarm storage still higher cost than needed\\nCold storage more expensive than archive"}	\N	\N	41	\N	\N
11	13	Cloud Architecture - Network Components	Knowledge	Cloud Architecture and Design	Which network component helps reduce latency for global users by caching content at edge locations closest to them?	[{"text": "Application gateway", "isCorrect": false}, {"text": "Network load balancer", "isCorrect": false}, {"text": "Application load balancer", "isCorrect": false}, {"text": "Content Delivery Network (CDN)", "isCorrect": true}]	Content Delivery Network (CDN)	CDNs distribute content across geographically dispersed servers to minimize latency for end users.	{"summary": "CDN functionality:", "breakdown": ["Caches static content at edge locations globally", "Routes requests to nearest geographic server", "Reduces bandwidth usage and server load", "Improves website performance and user experience"], "otherOptions": "ALB distributes traffic to backend servers\\nNLB handles network layer traffic\\nApplication gateway provides secure access"}	\N	\N	42	\N	\N
12	14	Cloud Architecture - Network Components	Comprehension	Cloud Architecture and Design	What is the primary difference between an application load balancer and a network load balancer?	[{"text": "Application load balancer works at Layer 7, network load balancer at Layer 4", "isCorrect": true}, {"text": "Application load balancer only works with HTTP, network load balancer with HTTPS", "isCorrect": false}, {"text": "Network load balancer is more expensive than application load balancer", "isCorrect": false}, {"text": "Network load balancer provides SSL termination, application load balancer does not", "isCorrect": false}]	Application load balancer works at Layer 7, network load balancer at Layer 4	Application load balancers operate at Layer 7 (application layer) while network load balancers operate at Layer 4 (transport layer).	{"summary": "Load balancer layer differences:", "breakdown": ["ALB: Layer 7 - can route based on HTTP headers, URLs, cookies", "NLB: Layer 4 - routes based on IP protocol data", "ALB: Content-based routing capabilities", "NLB: Higher performance, lower latency"], "otherOptions": "Pricing varies by provider and usage\\nBoth can handle HTTP and HTTPS\\nBoth can provide SSL termination"}	\N	\N	43	\N	\N
13	15	Cloud Architecture - Network Components	Application	Cloud Architecture and Design	Your web application needs to route traffic based on URL paths (/api/* to API servers, /images/* to image servers). Which load balancer type should you use?	[{"text": "Classic load balancer", "isCorrect": false}, {"text": "Network load balancer", "isCorrect": false}, {"text": "Application load balancer", "isCorrect": true}, {"text": "Gateway load balancer", "isCorrect": false}]	Application load balancer	Application load balancers can route traffic based on URL paths, headers, and other application-layer information.	{"summary": "Application load balancer routing capabilities:", "breakdown": ["Path-based routing (/api, /images, etc.)", "Header-based routing", "Query parameter-based routing", "Cookie-based routing"], "otherOptions": "Network load balancer routes at Layer 4, cannot inspect URLs\\nClassic load balancer has limited routing capabilities\\nGateway load balancer is for traffic inspection"}	\N	\N	44	\N	\N
14	16	Cloud Architecture - Disaster Recovery	Knowledge	Cloud Architecture and Design	What does RTO (Recovery Time Objective) represent in disaster recovery planning?	[{"text": "The frequency of disaster recovery testing", "isCorrect": false}, {"text": "The maximum acceptable downtime after a disaster", "isCorrect": true}, {"text": "The cost of implementing disaster recovery", "isCorrect": false}, {"text": "The amount of data that can be lost during a disaster", "isCorrect": false}]	The maximum acceptable downtime after a disaster	RTO defines the maximum acceptable duration within which a system must be restored after a disruption.	{"summary": "RTO characteristics:", "breakdown": ["Maximum acceptable downtime duration", "Measured in hours, minutes, or seconds", "Drives disaster recovery strategy selection", "Affects cost and complexity of DR solutions"], "otherOptions": "That describes RPO (Recovery Point Objective)\\nCost is a factor but not what RTO measures\\nTesting frequency is separate from RTO"}	\N	\N	45	\N	\N
15	17	Cloud Architecture - Disaster Recovery	Comprehension	Cloud Architecture and Design	What is the difference between RPO and RTO in disaster recovery?	[{"text": "RPO is about data loss, RTO is about downtime", "isCorrect": true}, {"text": "RPO is for applications, RTO is for infrastructure", "isCorrect": false}, {"text": "RPO is about downtime, RTO is about data loss", "isCorrect": false}, {"text": "RPO and RTO both measure downtime but in different units", "isCorrect": false}]	RPO is about data loss, RTO is about downtime	RPO (Recovery Point Objective) measures acceptable data loss, while RTO (Recovery Time Objective) measures acceptable downtime.	{"summary": "RPO vs RTO:", "breakdown": ["RPO: Maximum tolerable data loss (measured in time)", "RTO: Maximum tolerable downtime", "RPO drives backup frequency requirements", "RTO drives disaster recovery strategy selection"], "otherOptions": "This reverses the definitions\\nThey measure different aspects of recovery\\nBoth apply to applications and infrastructure"}	\N	\N	46	\N	\N
16	18	Cloud Architecture - Disaster Recovery	Application	Cloud Architecture and Design	Your organization has an RTO of 2 hours and RPO of 30 minutes for a critical application. Which disaster recovery strategy best meets these requirements?	[{"text": "Backup and restore only", "isCorrect": false}, {"text": "Cold site with daily backups", "isCorrect": false}, {"text": "Hot site with real-time replication", "isCorrect": false}, {"text": "Warm site with automated failover", "isCorrect": true}]	Warm site with automated failover	Warm site provides the right balance of cost and recovery time to meet 2-hour RTO requirements.	{"summary": "Warm site characteristics for this scenario:", "breakdown": ["Can achieve 2-hour RTO with quick startup", "30-minute RPO achievable with frequent backups", "More cost-effective than hot site", "Automated failover reduces manual intervention"], "otherOptions": "Cold site takes too long for 2-hour RTO\\nHot site exceeds requirements and increases cost\\nBackup/restore cannot meet 2-hour RTO"}	\N	\N	47	\N	\N
17	19	Cloud Architecture - Multicloud	Application	Cloud Architecture and Design	Your organization wants to avoid vendor lock-in while leveraging best-of-breed services from multiple cloud providers. What strategy should you implement?	[{"text": "Single cloud deployment with multiple regions", "isCorrect": false}, {"text": "Private cloud deployment only", "isCorrect": false}, {"text": "Multicloud tenancy strategy", "isCorrect": true}, {"text": "Hybrid cloud with on-premises integration", "isCorrect": false}]	Multicloud tenancy strategy	Multicloud tenancy allows using services from multiple providers, avoiding vendor lock-in while accessing optimal services.	{"summary": "Multicloud tenancy benefits:", "breakdown": ["Avoids dependency on a single cloud provider", "Enables selection of best services from each provider", "Provides redundancy and improved reliability", "Allows cost optimization through competitive pricing"], "otherOptions": "Single cloud still creates vendor dependency\\nHybrid focuses on on-premises integration\\nPrivate cloud limits service options"}	\N	\N	48	\N	\N
18	20	Cloud Architecture - Multicloud	Comprehension	Cloud Architecture and Design	Which scenario best represents a valid use case for multicloud strategy?	[{"text": "Using private cloud for all applications", "isCorrect": false}, {"text": "Using the same cloud provider in multiple regions", "isCorrect": false}, {"text": "Using on-premises servers with cloud storage", "isCorrect": false}, {"text": "Using AWS for compute and Azure for AI/ML services", "isCorrect": true}]	Using AWS for compute and Azure for AI/ML services	Multicloud involves using different cloud providers for different services based on their strengths.	{"summary": "Multicloud strategy benefits:", "breakdown": ["Best-of-breed service selection", "Avoid vendor lock-in", "Improved negotiating position", "Reduced single point of failure risk"], "otherOptions": "Multiple regions with same provider is not multicloud\\nPrivate cloud only is not multicloud\\nHybrid cloud, not multicloud"}	\N	\N	49	\N	\N
19	21	Deployments - Migration Types	Comprehension	Cloud Deployment	Which migration strategy involves moving applications to the cloud with minimal changes, often called ""lift and shift""?	[{"text": "Rebuilding", "isCorrect": false}, {"text": "Refactoring", "isCorrect": false}, {"text": "Rehosting", "isCorrect": true}, {"text": "Rearchitecting", "isCorrect": false}]	Rehosting	Rehosting (lift and shift) moves applications to cloud with minimal modification.	{"summary": "Rehosting characteristics:", "breakdown": ["Minimal application changes required", "Fastest migration approach", "Lower initial cost and complexity", "May not fully utilize cloud benefits"], "otherOptions": "Refactoring modifies applications for cloud\\nRearchitecting redesigns for cloud-native\\nRebuilding creates new applications"}	\N	\N	50	\N	\N
21	23	Deployments - Migration Types	Knowledge	Cloud Deployment	Which migration strategy involves completely redesigning an application to be cloud-native from the ground up?	[{"text": "Refactoring", "isCorrect": false}, {"text": "Rehosting", "isCorrect": false}, {"text": "Rearchitecting", "isCorrect": true}, {"text": "Replacing", "isCorrect": false}]	Rearchitecting	Rearchitecting involves completely redesigning applications to be cloud-native and take full advantage of cloud capabilities.	{"summary": "Rearchitecting characteristics:", "breakdown": ["Complete application redesign", "Full utilization of cloud-native features", "Highest development effort and cost", "Maximum long-term benefits and flexibility"], "otherOptions": "Rehosting moves with minimal changes\\nRefactoring makes modifications but not complete redesign\\nReplacing uses different software"}	\N	\N	51	\N	\N
23	25	Deployments - Infrastructure as Code	Comprehension	Cloud Deployment	What is the primary benefit of using declarative Infrastructure as Code templates?	[{"text": "Requires less storage space than imperative code", "isCorrect": false}, {"text": "Describes desired end state rather than step-by-step instructions", "isCorrect": true}, {"text": "Works only with specific cloud providers", "isCorrect": false}, {"text": "Faster execution than imperative scripts", "isCorrect": false}]	Describes desired end state rather than step-by-step instructions	Declarative IaC describes the desired end state, allowing the system to determine how to achieve it.	{"summary": "Declarative IaC benefits:", "breakdown": ["Describes desired infrastructure state", "System determines implementation steps", "Idempotent operations (safe to run multiple times)", "Easier to understand and maintain"], "otherOptions": "Execution speed depends on implementation\\nStorage size not a primary consideration\\nMany tools work across multiple providers"}	\N	\N	52	\N	\N
24	26	Deployments - Infrastructure as Code	Knowledge	Cloud Deployment	Which of the following is a key characteristic of Infrastructure as Code (IaC)?	[{"text": "Infrastructure changes require physical hardware installation", "isCorrect": false}, {"text": "Infrastructure is managed by third-party vendors only", "isCorrect": false}, {"text": "Infrastructure is managed manually through web consoles", "isCorrect": false}, {"text": "Infrastructure is defined in code and version controlled", "isCorrect": true}]	Infrastructure is defined in code and version controlled	IaC treats infrastructure as code that can be version controlled, tested, and deployed programmatically.	{"summary": "IaC key characteristics:", "breakdown": ["Infrastructure defined in code files", "Version control for infrastructure changes", "Automated deployment and provisioning", "Repeatable and consistent deployments"], "otherOptions": "IaC eliminates manual console management\\nIaC works with virtual/cloud infrastructure\\nIaC can be managed by internal teams"}	\N	\N	53	\N	\N
25	27	Deployments - Deployment Strategies	Application	Cloud Deployment	Your application needs zero-downtime deployment with the ability to quickly rollback if issues occur. Which deployment strategy is most appropriate?	[{"text": "Blue-green deployment", "isCorrect": true}, {"text": "Rolling deployment", "isCorrect": false}, {"text": "In-place deployment", "isCorrect": false}, {"text": "Recreate deployment", "isCorrect": false}]	Blue-green deployment	Blue-green deployment provides zero downtime and instant rollback capabilities by maintaining two identical environments.	{"summary": "Blue-green deployment characteristics:", "breakdown": ["Two identical production environments", "Instant traffic switching between environments", "Zero downtime during deployment", "Quick rollback by switching traffic back"], "otherOptions": "In-place deployment causes downtime\\nRolling deployment has slower rollback\\nRecreate deployment causes downtime"}	\N	\N	54	\N	\N
26	28	Deployments - Deployment Strategies	Comprehension	Cloud Deployment	What is the primary advantage of canary deployment strategy?	[{"text": "Lowest resource requirements", "isCorrect": false}, {"text": "Fastest deployment method", "isCorrect": false}, {"text": "Simplest to implement", "isCorrect": false}, {"text": "Limits impact of issues to small user subset", "isCorrect": true}]	Limits impact of issues to small user subset	Canary deployment gradually releases changes to small user groups, limiting the impact of potential issues.	{"summary": "Canary deployment benefits:", "breakdown": ["Gradual rollout to subset of users", "Early detection of issues with limited impact", "Ability to monitor and validate changes", "Reduced risk of widespread problems"], "otherOptions": "Not the fastest method\\nRequires additional monitoring infrastructure\\nMore complex than simple deployments"}	\N	\N	55	\N	\N
27	29	Deployments - Deployment Strategies	Knowledge	Cloud Deployment	In a rolling deployment strategy, how are application instances updated?	[{"text": "All instances are shut down before updates", "isCorrect": false}, {"text": "All instances are updated simultaneously", "isCorrect": false}, {"text": "Instances are updated one at a time or in small batches", "isCorrect": true}, {"text": "New instances are created while old ones remain", "isCorrect": false}]	Instances are updated one at a time or in small batches	Rolling deployment updates instances gradually, one at a time or in small batches, maintaining service availability.	{"summary": "Rolling deployment characteristics:", "breakdown": ["Gradual instance updates", "Maintains service availability", "Lower resource requirements than blue-green", "Slower rollback process"], "otherOptions": "That describes in-place deployment\\nThat describes blue-green deployment\\nThat would cause service interruption"}	\N	\N	56	\N	\N
28	30	Deployments - CI/CD	Application	Cloud Deployment	Your development team wants to automatically deploy code changes to production after passing all tests in the staging environment. Which CI/CD practice should be implemented?	[{"text": "Continuous Delivery only", "isCorrect": false}, {"text": "Continuous Deployment", "isCorrect": true}, {"text": "Manual deployment with CI", "isCorrect": false}, {"text": "Continuous Integration only", "isCorrect": false}]	Continuous Deployment	Continuous Deployment automatically releases code changes to production after passing all pipeline stages.	{"summary": "Continuous Deployment characteristics:", "breakdown": ["Fully automated pipeline from code to production", "No manual intervention required for deployment", "Requires robust testing and monitoring", "Enables rapid feature delivery to users"], "otherOptions": "CI only handles code integration\\nCD deploys to staging but requires manual production release\\nManual deployment contradicts automation goals"}	\N	\N	57	\N	\N
29	31	Operations - Scaling	Application	Cloud Operations and Support	Your web application experiences predictable traffic spikes every weekday from 9 AM to 5 PM. Which scaling approach would be most cost-effective?	[{"text": "Manual scaling during business hours", "isCorrect": false}, {"text": "Keeping maximum capacity running at all times", "isCorrect": false}, {"text": "Horizontal auto-scaling based on schedule and metrics", "isCorrect": true}, {"text": "Vertical scaling with larger instances", "isCorrect": false}]	Horizontal auto-scaling based on schedule and metrics	Scheduled auto-scaling with metric triggers optimizes both cost and performance for predictable patterns.	{"summary": "Auto-scaling advantages for predictable traffic:", "breakdown": ["Proactive scaling before traffic spikes", "Automatic scale-down during off-hours", "Combines scheduled and reactive scaling", "Optimizes cost while maintaining performance"], "otherOptions": "Vertical scaling requires downtime\\nManual scaling is reactive and error-prone\\nMaximum capacity wastes money during off-hours"}	\N	\N	58	\N	\N
30	32	Operations - Scaling	Comprehension	Cloud Operations and Support	What is the primary difference between horizontal and vertical scaling?	[{"text": "Horizontal works with databases, vertical works with web servers", "isCorrect": false}, {"text": "Horizontal adds more instances, vertical increases instance size", "isCorrect": true}, {"text": "Horizontal is automatic, vertical is manual", "isCorrect": false}, {"text": "Horizontal is cheaper, vertical is more expensive", "isCorrect": false}]	Horizontal adds more instances, vertical increases instance size	Horizontal scaling adds more instances (scale out), while vertical scaling increases the capacity of existing instances (scale up).	{"summary": "Scaling approach differences:", "breakdown": ["Horizontal: Scale out - add more instances", "Vertical: Scale up - increase instance capacity", "Horizontal: Better for distributed applications", "Vertical: Simpler but has hardware limits"], "otherOptions": "Cost depends on specific implementation\\nBoth can be automated\\nBoth work with various application types"}	\N	\N	59	\N	\N
31	33	Operations - Scaling	Knowledge	Cloud Operations and Support	Which scaling trigger would be most appropriate for a CPU-intensive application?	[{"text": "Network bandwidth", "isCorrect": false}, {"text": "Memory utilization", "isCorrect": false}, {"text": "Storage capacity", "isCorrect": false}, {"text": "CPU utilization", "isCorrect": true}]	CPU utilization	CPU-intensive applications should scale based on CPU utilization metrics to ensure adequate processing power.	{"summary": "Scaling trigger selection:", "breakdown": ["CPU utilization for compute-intensive workloads", "Memory utilization for memory-intensive applications", "Network bandwidth for high-throughput applications", "Custom metrics for application-specific needs"], "otherOptions": "Memory not primary bottleneck for CPU-intensive apps\\nNetwork may not be bottleneck\\nStorage not relevant for CPU-intensive scaling"}	\N	\N	60	\N	\N
32	34	Operations - Monitoring	Knowledge	Cloud Operations and Support	Which type of monitoring provides insights into application performance and user experience?	[{"text": "Security monitoring", "isCorrect": false}, {"text": "Network monitoring", "isCorrect": false}, {"text": "Application Performance Monitoring (APM)", "isCorrect": true}, {"text": "Infrastructure monitoring", "isCorrect": false}]	Application Performance Monitoring (APM)	APM focuses on application behavior, response times, and user experience metrics.	{"summary": "APM monitoring includes:", "breakdown": ["Application response times and throughput", "Error rates and exception tracking", "User experience and transaction traces", "Code-level performance insights"], "otherOptions": "Infrastructure monitors servers and resources\\nNetwork monitoring focuses on connectivity\\nSecurity monitoring tracks threats and vulnerabilities"}	\N	\N	61	\N	\N
33	35	Operations - Monitoring	Comprehension	Cloud Operations and Support	What is the primary purpose of distributed tracing in microservices architecture?	[{"text": "Measure network latency", "isCorrect": false}, {"text": "Monitor individual service performance", "isCorrect": false}, {"text": "Collect application logs", "isCorrect": false}, {"text": "Track requests across multiple services", "isCorrect": true}]	Track requests across multiple services	Distributed tracing tracks request paths through multiple services, identifying bottlenecks and failures.	{"summary": "Distributed tracing benefits:", "breakdown": ["Visualizes request journey across microservices", "Identifies latency bottlenecks in service chain", "Correlates spans across distributed components", "Enables root cause analysis for performance issues"], "otherOptions": "That's service-level monitoring\\nThat's log aggregation\\nThat's network monitoring"}	\N	\N	62	\N	\N
34	36	Operations - Monitoring	Application	Cloud Operations and Support	Your application is experiencing intermittent performance issues. Which observability practice would best help trace the request flow through your microservices architecture?	[{"text": "Log aggregation", "isCorrect": false}, {"text": "Alert configuration", "isCorrect": false}, {"text": "Distributed tracing", "isCorrect": true}, {"text": "Metrics monitoring", "isCorrect": false}]	Distributed tracing	Distributed tracing tracks request paths through multiple services, identifying bottlenecks and failures.	{"summary": "Distributed tracing advantages:", "breakdown": ["Visualizes request journey across microservices", "Identifies latency bottlenecks in service chain", "Correlates spans across distributed components", "Enables root cause analysis for performance issues"], "otherOptions": "Logs provide events but not request flow\\nMetrics show performance but not trace paths\\nAlerts notify of issues but don't trace flow"}	\N	\N	63	\N	\N
35	37	Operations - Backup Strategies	Knowledge	Cloud Operations and Support	Which backup type only captures changes made since the last full backup?	[{"text": "Snapshot backup", "isCorrect": false}, {"text": "Differential backup", "isCorrect": true}, {"text": "Incremental backup", "isCorrect": false}, {"text": "Full backup", "isCorrect": false}]	Differential backup	Differential backups capture all changes since the last full backup, not since the last backup of any type.	{"summary": "Differential backup characteristics:", "breakdown": ["Captures changes since last full backup", "Faster than full backup, slower than incremental", "Requires only full backup + latest differential for restore", "Size grows until next full backup"], "otherOptions": "Full backup captures everything\\nIncremental captures changes since last backup\\nSnapshot is point-in-time image"}	\N	\N	64	\N	\N
36	38	Operations - Backup Strategies	Application	Cloud Operations and Support	Your organization has an RPO of 4 hours for a critical database. The database receives constant updates throughout business hours. Which backup strategy best meets this requirement?	[{"text": "Full backup weekly with 4-hour incremental backups", "isCorrect": true}, {"text": "Weekly full backups with daily differentials", "isCorrect": false}, {"text": "Monthly full backups with weekly incrementals", "isCorrect": false}, {"text": "Daily full backups at midnight", "isCorrect": false}]	Full backup weekly with 4-hour incremental backups	Incremental backups every 4 hours ensure data loss is limited to the RPO requirement of 4 hours.	{"summary": "Backup strategy for 4-hour RPO:", "breakdown": ["Incremental backups every 4 hours meet RPO exactly", "Weekly full backups provide baseline restore point", "Captures all changes within acceptable data loss window", "Balances storage efficiency with recovery requirements"], "otherOptions": "Daily backups allow up to 24 hours data loss\\nDaily differentials still allow 24 hours data loss\\nWeekly incrementals allow up to 7 days data loss"}	\N	\N	65	\N	\N
38	40	Security - IAM	Application	Cloud Security	A developer needs temporary access to debug a production issue in a specific S3 bucket. What is the most secure approach following the principle of least privilege?	[{"text": "Share root account credentials", "isCorrect": false}, {"text": "Create time-limited IAM role with bucket-specific permissions", "isCorrect": true}, {"text": "Add developer to administrators group", "isCorrect": false}, {"text": "Create IAM user with permanent S3 full access", "isCorrect": false}]	Create time-limited IAM role with bucket-specific permissions	Time-limited IAM roles with specific permissions minimize security exposure while providing necessary access.	{"summary": "Secure temporary access principles:", "breakdown": ["Time-bound access that automatically expires", "Scope limited to specific resources needed", "No permanent credentials to manage", "Audit trail of role assumption"], "otherOptions": "Administrative access violates least privilege\\nPermanent access creates long-term security risk\\nRoot credentials should never be shared"}	\N	\N	66	\N	\N
39	41	Security - IAM	Comprehension	Cloud Security	What is the primary difference between Role-Based Access Control (RBAand Attribute-Based Access Control (ABAC)?	[{"text": "RBAC works with cloud, ABAC works with on-premises", "isCorrect": false}, {"text": "RBAC is newer technology than ABAC", "isCorrect": false}, {"text": "RBAC is more secure than ABAC", "isCorrect": false}, {"text": "RBAC uses job functions, ABAC uses multiple attributes", "isCorrect": true}]	RBAC uses job functions, ABAC uses multiple attributes	RBAC assigns permissions based on job roles, while ABAC uses multiple attributes like location, time, and resource sensitivity.	{"summary": "RBAC vs ABAC comparison:", "breakdown": ["RBAC: Access based on predefined roles", "ABAC: Access based on multiple dynamic attributes", "RBAC: Simpler to implement and manage", "ABAC: More flexible and granular control"], "otherOptions": "Security depends on implementation\\nBoth work in cloud and on-premises\\nABAC is actually newer than RBAC"}	\N	\N	67	\N	\N
40	42	Security - IAM	Knowledge	Cloud Security	Which authentication method provides the highest level of security for cloud access?	[{"text": "Single sign-on (SSO)", "isCorrect": false}, {"text": "Multi-factor authentication (MFA)", "isCorrect": true}, {"text": "API keys", "isCorrect": false}, {"text": "Username and password only", "isCorrect": false}]	Multi-factor authentication (MFA)	MFA requires multiple authentication factors, significantly increasing security by requiring something you know, have, or are.	{"summary": "MFA security benefits:", "breakdown": ["Requires multiple authentication factors", "Protects against credential theft", "Combines knowledge, possession, and inherence factors", "Significantly reduces unauthorized access risk"], "otherOptions": "Single factor is easily compromised\\nSSO is about convenience, not necessarily security\\nAPI keys are single factor"}	\N	\N	68	\N	\N
41	43	Security - Encryption	Comprehension	Cloud Security	Which encryption approach protects data while it is being transmitted between your application and cloud storage?	[{"text": "Encryption at rest", "isCorrect": false}, {"text": "File system encryption", "isCorrect": false}, {"text": "Encryption in transit", "isCorrect": true}, {"text": "Database encryption", "isCorrect": false}]	Encryption in transit	Encryption in transit protects data during transmission using protocols like TLS/SSL.	{"summary": "Encryption in transit protects:", "breakdown": ["Data moving between client and server", "API calls and responses", "File uploads and downloads", "Database connections and queries"], "otherOptions": "At rest protects stored data\\nDatabase encryption protects stored database data\\nFile system encryption protects local storage"}	\N	\N	69	\N	\N
42	44	Security - Encryption	Application	Cloud Security	Your organization requires that sensitive data be encrypted both when stored and when transmitted. Which encryption strategy should be implemented?	[{"text": "Both encryption at rest and in transit", "isCorrect": true}, {"text": "Encryption in transit only", "isCorrect": false}, {"text": "Application-level encryption only", "isCorrect": false}, {"text": "Encryption at rest only", "isCorrect": false}]	Both encryption at rest and in transit	Comprehensive data protection requires encrypting data both when stored (at rest) and when transmitted (in transit).	{"summary": "Complete encryption strategy includes:", "breakdown": ["Encryption at rest protects stored data", "Encryption in transit secures data movement", "Protects against both storage and network attacks", "Meets compliance requirements for data protection"], "otherOptions": "Transit-only leaves stored data vulnerable\\nRest-only leaves network traffic vulnerable\\nApplication-level alone insufficient for comprehensive protection"}	\N	\N	70	\N	\N
43	45	Security - Encryption	Knowledge	Cloud Security	What is the primary purpose of key management in cloud encryption?	[{"text": "To eliminate the need for encryption", "isCorrect": false}, {"text": "To reduce encryption costs", "isCorrect": false}, {"text": "To securely store, rotate, and control access to encryption keys", "isCorrect": true}, {"text": "To improve encryption performance", "isCorrect": false}]	To securely store, rotate, and control access to encryption keys	Key management ensures encryption keys are securely stored, regularly rotated, and access is properly controlled.	{"summary": "Key management responsibilities:", "breakdown": ["Secure key storage and protection", "Regular key rotation and lifecycle management", "Access control and audit logging", "Key recovery and backup procedures"], "otherOptions": "Cost reduction is not primary purpose\\nPerformance optimization is secondary\\nKey management supports encryption, not eliminates it"}	\N	\N	71	\N	\N
44	46	DevOps - CI/CD	Knowledge	DevOps Fundamentals	What is the primary purpose of Continuous Integration (CI) in a DevOps pipeline?	[{"text": "Integrate code changes frequently and run automated tests", "isCorrect": true}, {"text": "Manage infrastructure as code", "isCorrect": false}, {"text": "Monitor application performance", "isCorrect": false}, {"text": "Automatically deploy to production", "isCorrect": false}]	Integrate code changes frequently and run automated tests	CI focuses on frequently integrating code changes and running automated builds and tests.	{"summary": "Continuous Integration benefits:", "breakdown": ["Early detection of integration issues", "Automated build and test execution", "Frequent code integration reduces conflicts", "Faster feedback to development teams"], "otherOptions": "Production deployment is Continuous Deployment\\nPerformance monitoring is separate from CI\\nIaC management is infrastructure automation"}	\N	\N	72	\N	\N
45	47	DevOps - CI/CD	Comprehension	DevOps Fundamentals	What is the difference between Continuous Delivery and Continuous Deployment?	[{"text": "Continuous Delivery is for testing, Continuous Deployment is for production", "isCorrect": false}, {"text": "Continuous Delivery deploys automatically, Continuous Deployment requires manual approval", "isCorrect": false}, {"text": "They are the same thing with different names", "isCorrect": false}, {"text": "Continuous Delivery requires manual approval for production, Continuous Deployment is fully automated", "isCorrect": true}]	Continuous Delivery requires manual approval for production, Continuous Deployment is fully automated	Continuous Delivery prepares code for production deployment but requires manual approval, while Continuous Deployment automatically deploys to production.	{"summary": "CD vs CD comparison:", "breakdown": ["Continuous Delivery: Automated pipeline with manual production approval", "Continuous Deployment: Fully automated pipeline to production", "Both require robust testing and quality gates", "Continuous Deployment requires higher confidence in automation"], "otherOptions": "This reverses the definitions\\nThey have different automation levels\\nBoth involve production deployment"}	\N	\N	73	\N	\N
46	48	DevOps - CI/CD	Application	DevOps Fundamentals	Your development team wants to catch integration issues early and run automated tests on every code commit. Which DevOps practice should be implemented first?	[{"text": "Configuration Management", "isCorrect": false}, {"text": "Infrastructure as Code", "isCorrect": false}, {"text": "Continuous Integration", "isCorrect": true}, {"text": "Continuous Deployment", "isCorrect": false}]	Continuous Integration	Continuous Integration should be implemented first to establish automated builds and testing on every code commit.	{"summary": "CI as foundation practice:", "breakdown": ["Establishes automated build processes", "Runs tests on every code commit", "Provides immediate feedback to developers", "Foundation for more advanced DevOps practices"], "otherOptions": "CD builds on CI foundation\\nIaC is infrastructure focused\\nConfiguration management is separate concern"}	\N	\N	74	\N	\N
47	49	DevOps - Containers	Comprehension	DevOps Fundamentals	Which container orchestration approach is best for production environments requiring automated scaling and service discovery?	[{"text": "Containers running on a single host", "isCorrect": false}, {"text": "Standalone containers managed manually", "isCorrect": false}, {"text": "Virtual machines with containers installed", "isCorrect": false}, {"text": "Container orchestration platform like Kubernetes", "isCorrect": true}]	Container orchestration platform like Kubernetes	Container orchestration platforms provide automated management, scaling, and service discovery for production workloads.	{"summary": "Orchestration platform benefits:", "breakdown": ["Automated container lifecycle management", "Built-in scaling and load balancing", "Service discovery and networking", "Rolling updates and rollback capabilities"], "otherOptions": "Manual management doesn't scale for production\\nVMs add unnecessary overhead\\nSingle host creates single point of failure"}	\N	\N	75	\N	\N
48	50	DevOps - Containers	Application	DevOps Fundamentals	Your team needs to deploy a microservices application that can automatically scale, handle failures, and manage service discovery. Which approach is most suitable?	[{"text": "Serverless functions only", "isCorrect": false}, {"text": "Virtual machines with manual deployment", "isCorrect": false}, {"text": "Standalone containers on virtual machines", "isCorrect": false}, {"text": "Container orchestration with Kubernetes", "isCorrect": true}]	Container orchestration with Kubernetes	Kubernetes provides comprehensive container orchestration with auto-scaling, self-healing, and service discovery capabilities.	{"summary": "Kubernetes orchestration features:", "breakdown": ["Automatic scaling based on resource utilization", "Self-healing with pod restart and rescheduling", "Built-in service discovery and load balancing", "Rolling updates and rollback capabilities"], "otherOptions": "Standalone containers lack orchestration features\\nVMs with manual deployment don't provide automation\\nServerless alone insufficient for complex microservices"}	\N	\N	76	\N	\N
49	51	Troubleshooting - Performance	Application	Troubleshooting	Users report slow application response times. Your monitoring shows high CPU utilization but normal memory and disk usage. What should be your first troubleshooting step?	[{"text": "Increase memory allocation", "isCorrect": false}, {"text": "Increase disk storage capacity", "isCorrect": false}, {"text": "Restart all application servers", "isCorrect": false}, {"text": "Analyze CPU-intensive processes and optimize or scale CPU resources", "isCorrect": true}]	Analyze CPU-intensive processes and optimize or scale CPU resources	High CPU utilization directly correlates with the performance issue, making CPU analysis the logical first step.	{"summary": "CPU troubleshooting approach:", "breakdown": ["Identify CPU-intensive processes or queries", "Analyze application code for optimization opportunities", "Consider vertical scaling for more CPU power", "Implement horizontal scaling to distribute load"], "otherOptions": "Memory is not the bottleneck here\\nRestart is temporary and doesn't address root cause\\nDisk capacity is not related to CPU issues"}	\N	\N	77	\N	\N
50	52	Troubleshooting - Performance	Comprehension	Troubleshooting	What is the most likely cause of high IOPS (Input/Output Operations Per Second) in a cloud environment?	[{"text": "Memory leaks", "isCorrect": false}, {"text": "Network latency issues", "isCorrect": false}, {"text": "Database queries or file system operations", "isCorrect": true}, {"text": "CPU overutilization", "isCorrect": false}]	Database queries or file system operations	High IOPS typically indicates intensive database operations or file system read/write activities.	{"summary": "Common causes of high IOPS:", "breakdown": ["Database queries and transactions", "File system read/write operations", "Application logging activities", "Backup and data replication processes"], "otherOptions": "Network latency affects throughput, not IOPS\\nCPU issues don't directly cause IOPS\\nMemory leaks affect memory usage, not IOPS"}	\N	\N	78	\N	\N
51	53	Troubleshooting - Network	Comprehension	Troubleshooting	An application cannot connect to a database in another subnet. The database is running and accessible from other sources. What is the most likely cause?	[{"text": "Network security group or firewall blocking the connection", "isCorrect": true}, {"text": "Application code error", "isCorrect": false}, {"text": "Database server is down", "isCorrect": false}, {"text": "DNS resolution failure", "isCorrect": false}]	Network security group or firewall blocking the connection	Network connectivity issues between subnets typically involve security group or firewall configuration problems.	{"summary": "Network troubleshooting for subnet connectivity:", "breakdown": ["Check security group rules for required ports", "Verify network ACL configurations", "Ensure route table entries for subnet communication", "Confirm firewall rules on both source and destination"], "otherOptions": "Database is confirmed running and accessible\\nDNS would affect name resolution, not subnet connectivity\\nCode error wouldn't be subnet-specific"}	\N	\N	79	\N	\N
52	54	Troubleshooting - Network	Application	Troubleshooting	A multi-tier application can communicate between web and application tiers, but the application tier cannot reach the database tier. What should you check first?	[{"text": "Database server hardware status", "isCorrect": false}, {"text": "Application server logs", "isCorrect": false}, {"text": "Web server configuration", "isCorrect": false}, {"text": "Network security groups and routing between application and database tiers", "isCorrect": true}]	Network security groups and routing between application and database tiers	Network connectivity issues between specific tiers typically involve security group rules or routing configuration.	{"summary": "Network troubleshooting for tier connectivity:", "breakdown": ["Security groups may block database port access", "Subnet routing tables might be misconfigured", "Network ACLs could prevent tier communication", "VPC peering or transit gateway issues possible"], "otherOptions": "Web tier communication works, not a web server issue\\nApp logs won't show network configuration problems\\nHardware status wouldn't be tier-specific"}	\N	\N	80	\N	\N
53	55	Troubleshooting - Security	Comprehension	Troubleshooting	An application suddenly cannot access a cloud storage bucket that worked fine yesterday. No code changes were made. What is the most likely cause?	[{"text": "Storage bucket has been deleted", "isCorrect": false}, {"text": "Network connectivity issues", "isCorrect": false}, {"text": "Application server hardware failure", "isCorrect": false}, {"text": "IAM permissions or security policies changed", "isCorrect": true}]	IAM permissions or security policies changed	Sudden access failures without code changes typically indicate permission or security policy modifications.	{"summary": "Common causes of sudden access loss:", "breakdown": ["IAM role permissions modified or revoked", "Security group rules changed", "Access keys expired or rotated", "Bucket policies or ACLs updated"], "otherOptions": "Deletion would affect all access, not just this app\\nNetwork issues would show connectivity errors\\nHardware failure would cause broader application issues"}	\N	\N	81	\N	\N
54	56	Cloud Architecture - Microservices	Comprehension	Cloud Architecture and Design	What is the primary benefit of using microservices architecture in cloud environments?	[{"text": "Lower infrastructure costs", "isCorrect": false}, {"text": "Reduced development complexity", "isCorrect": false}, {"text": "Simplified monitoring and logging", "isCorrect": false}, {"text": "Independent scaling and deployment of services", "isCorrect": true}]	Independent scaling and deployment of services	Microservices allow each service to be developed, deployed, and scaled independently, providing flexibility and resilience.	{"summary": "Microservices benefits:", "breakdown": ["Independent service deployment and scaling", "Technology diversity across services", "Fault isolation and resilience", "Team autonomy and faster development cycles"], "otherOptions": "Actually increases complexity\\nMay increase infrastructure costs\\nMonitoring becomes more complex"}	\N	\N	82	\N	\N
55	57	Cloud Architecture - Managed Services	Application	Cloud Architecture and Design	Your organization wants to reduce operational overhead for database management while maintaining high availability. Which approach is most suitable?	[{"text": "Self-managed database on virtual machines", "isCorrect": false}, {"text": "Managed database service with multi-AZ deployment", "isCorrect": true}, {"text": "On-premises database with cloud backup", "isCorrect": false}, {"text": "Containerized database with manual orchestration", "isCorrect": false}]	Managed database service with multi-AZ deployment	Managed database services with multi-AZ deployment provide high availability while reducing operational overhead.	{"summary": "Managed database benefits:", "breakdown": ["Automated patching and maintenance", "Built-in high availability with multi-AZ", "Automated backups and point-in-time recovery", "Reduced operational overhead"], "otherOptions": "Self-managed increases operational overhead\\nManual orchestration increases complexity\\nOn-premises doesn't reduce overhead"}	\N	\N	83	\N	\N
56	58	Deployments - Version Control	Knowledge	Cloud Deployment	Which version control operation allows developers to propose changes for review before merging into the main branch?	[{"text": "Git merge", "isCorrect": false}, {"text": "Git commit", "isCorrect": false}, {"text": "Git push", "isCorrect": false}, {"text": "Pull request", "isCorrect": true}]	Pull request	Pull requests enable code review and discussion before changes are merged into the main codebase.	{"summary": "Pull request benefits:", "breakdown": ["Facilitates peer code review process", "Enables discussion and feedback on changes", "Maintains code quality through review gates", "Provides audit trail of changes and approvals"], "otherOptions": "Push uploads code to repository\\nCommit saves changes locally\\nMerge combines branches without review"}	\N	\N	84	\N	\N
57	59	Operations - Lifecycle Management	Comprehension	Cloud Operations and Support	What is the primary purpose of patch management in cloud environments?	[{"text": "Reduce infrastructure costs", "isCorrect": false}, {"text": "Increase storage capacity", "isCorrect": false}, {"text": "Address security vulnerabilities and bugs", "isCorrect": true}, {"text": "Improve application performance", "isCorrect": false}]	Address security vulnerabilities and bugs	Patch management primarily addresses security vulnerabilities and software bugs to maintain system security and stability.	{"summary": "Patch management objectives:", "breakdown": ["Address security vulnerabilities", "Fix software bugs and issues", "Maintain system stability", "Ensure compliance with security standards"], "otherOptions": "Performance improvements are secondary\\nCost reduction is not primary purpose\\nStorage capacity is unrelated to patching"}	\N	\N	85	\N	\N
59	61	Cloud Migration and Capacity Planning	Application	Cloud Architecture and Design	A retail company with 200 stores operates a legacy inventory system requiring 48 CPU cores, 256GB RAM, and 10TB storage with 15,000 IOPS. The system experiences 300% load increase during Black Friday sales. They want to migrate to the cloud with the ability to handle peak loads cost-effectively. Which migration strategy best meets these requirements?	[{"text": "Containerize the application as-is and deploy to managed Kubernetes", "isCorrect": false}, {"text": "Lift-and-shift to cloud with 3x capacity provisioned year-round", "isCorrect": false}, {"text": "Re-architect as microservices with auto-scaling and cloud-native database", "isCorrect": true}, {"text": "Hybrid approach keeping database on-premises with cloud compute", "isCorrect": false}]	Re-architect as microservices with auto-scaling and cloud-native database	Re-architecting as microservices enables auto-scaling for the 300% peak load without over-provisioning year-round, while cloud-native databases provide elastic IOPS scaling.	{"summary": "Microservices migration benefits for variable loads:", "breakdown": ["Auto-scaling handles 300% peak without year-round costs", "Cloud-native database scales IOPS on demand", "Service isolation allows scaling only needed components", "Pay-per-use model optimizes costs during normal operations"], "otherOptions": "3x capacity year-round wastes resources and budget\\nHybrid approach doesn't solve IOPS scaling challenge\\nContainerizing monolith doesn't enable granular scaling"}	\N	\N	86	\N	\N
60	96	Security - Identity and Access Management	Advanced	Cloud Security	A multinational corporation uses multiple cloud providers and requires centralized identity management with single sign-on capability. Users need access to resources across AWS, Azure, and on-premises systems. Which solution provides the MOST comprehensive approach?	[{"text": "Multi-factor authentication on each system independently", "isCorrect": false}, {"text": "Shared service accounts across all platforms", "isCorrect": false}, {"text": "Separate identity systems for each cloud provider", "isCorrect": false}, {"text": "Federated identity management with SAML 2.0", "isCorrect": true}]	Federated identity management with SAML 2.0	Federated identity allows single sign-on across multiple systems and cloud providers using standard protocols like SAML.	{"summary": "Federated identity benefits:", "breakdown": ["Single sign-on across multiple systems", "Centralized user management", "Works with multiple cloud providers", "Reduces password fatigue and improves security"], "otherOptions": "Creates management overhead and security gaps\\nViolates security best practices\\nDoesn't provide centralized management or SSO"}	\N	\N	87	\N	\N
61	62	Cloud Security and Compliance	Analysis	Cloud Security	A healthcare provider must implement cloud storage for patient records with these requirements: data encrypted at rest and in transit, 7-year retention for compliance, access logs retained for 1 year, and automatic deletion after retention period. They need to prove compliance during audits. Which security controls combination ensures all requirements are met?	[{"text": "Third-party encryption tools, backup to tape, and annual compliance audits", "isCorrect": false}, {"text": "Client-side encryption, manual lifecycle policies, and quarterly access reviews", "isCorrect": false}, {"text": "Cloud provider encryption, automated lifecycle rules, immutable audit logs, and compliance certificates", "isCorrect": true}, {"text": "Database encryption, daily backups, and manual log reviews", "isCorrect": false}]	Cloud provider encryption, automated lifecycle rules, immutable audit logs, and compliance certificates	Automated lifecycle rules ensure compliant retention and deletion, immutable audit logs provide tamper-proof evidence, and cloud provider compliance certificates demonstrate adherence to healthcare standards.	{"summary": "Healthcare compliance in cloud storage requires:", "breakdown": ["Automated lifecycle policies prevent human error in retention", "Immutable audit logs ensure tamper-proof compliance evidence", "Provider encryption meets regulatory requirements efficiently", "Compliance certificates (HIPAA, SOC2) simplify audit process"], "otherOptions": "Manual processes risk non-compliance through human error\\nTape backups don't provide automated deletion\\nManual reviews insufficient for audit requirements"}	\N	\N	88	\N	\N
62	63	Cloud Automation and Orchestration	Application	Cloud Deployment	A development team deploys applications across dev, test, and prod environments in multiple cloud regions. They currently spend 15 hours weekly on manual deployments with a 5% error rate causing rollbacks. Which automation approach would best reduce deployment time and errors while maintaining environment-specific configurations?	[{"text": "Infrastructure as Code with parameterized templates and CI/CD pipelines", "isCorrect": true}, {"text": "Container images with hardcoded environment settings", "isCorrect": false}, {"text": "Shell scripts with environment variables for each deployment", "isCorrect": false}, {"text": "Configuration management tools with manual approval gates", "isCorrect": false}]	Infrastructure as Code with parameterized templates and CI/CD pipelines	Infrastructure as Code with parameterized templates enables consistent deployments across environments while CI/CD pipelines automate the process, reducing both time and error rates.	{"summary": "IaC and CI/CD benefits for multi-environment deployments:", "breakdown": ["Parameterized templates handle environment-specific configs", "Version control tracks all infrastructure changes", "Automated testing catches errors before production", "Consistent deployments reduce error rate from 5% to <1%"], "otherOptions": "Shell scripts lack version control and testing capabilities\\nManual approvals don't reduce deployment time\\nHardcoded settings prevent environment flexibility"}	\N	\N	89	\N	\N
63	64	Cloud Performance Optimization	Analysis	Cloud Operations and Support	A SaaS application experiences intermittent performance issues reported by 15% of users. Monitoring shows normal CPU (40%), memory (60%), and network (30%) utilization. However, user session recordings reveal 3-second delays during specific database queries. What combination of tools and techniques would best identify and resolve the root cause?	[{"text": "Enable database query profiling, analyze execution plans, and implement query optimization with caching", "isCorrect": true}, {"text": "Migrate to faster storage and increase network bandwidth", "isCorrect": false}, {"text": "Increase server resources and implement load balancing", "isCorrect": false}, {"text": "Add more monitoring agents and create utilization alerts", "isCorrect": false}]	Enable database query profiling, analyze execution plans, and implement query optimization with caching	Database query profiling identifies specific slow queries, execution plan analysis reveals inefficiencies, and strategic caching prevents repeated expensive operations.	{"summary": "Database performance troubleshooting approach:", "breakdown": ["Query profiling pinpoints exact problematic queries", "Execution plans reveal missing indexes or inefficient joins", "Query optimization reduces 3-second delays to milliseconds", "Caching frequently accessed data prevents repeated slow queries"], "otherOptions": "Resources aren't the issue (40% CPU, 60% memory)\\nMore monitoring won't fix identified query delays\\nHardware upgrades don't address query inefficiency"}	\N	\N	90	\N	\N
64	65	Cloud Business Continuity	Application	Troubleshooting	During a cloud provider outage, a company's primary region becomes unavailable. Their disaster recovery plan activates, but the failover process takes 6 hours instead of the planned 2 hours. Post-incident analysis reveals DNS propagation delays and cold database replicas. Which improvements would most effectively achieve the 2-hour RTO target?	[{"text": "Create detailed runbooks and conduct monthly drills", "isCorrect": false}, {"text": "Increase backup frequency and add more regions", "isCorrect": false}, {"text": "Implement DNS pre-staging with low TTL and maintain warm database replicas", "isCorrect": true}, {"text": "Purchase dedicated network connections between regions", "isCorrect": false}]	Implement DNS pre-staging with low TTL and maintain warm database replicas	DNS pre-staging with low TTL ensures rapid traffic redirection while warm database replicas eliminate lengthy data loading and cache warming during failover.	{"summary": "Achieving 2-hour RTO requires addressing specific bottlenecks:", "breakdown": ["Low TTL DNS (5 minutes) enables quick traffic switching", "DNS pre-staging eliminates record creation time during disaster", "Warm replicas maintain recent data and cache, ready for traffic", "Combined approach reduces failover from 6 hours to under 2 hours"], "otherOptions": "More backups don't address DNS or cold replica issues\\nRunbooks help execution but don't fix technical delays\\nNetwork connections don't solve DNS propagation delays"}	\N	\N	91	\N	\N
65	97	Security - Data Protection	Intermediate	Cloud Security	A healthcare organization stores patient data in the cloud and must comply with HIPAA requirements. Which combination of security controls is MOST important for protecting PHI (Protected Health Information)?	[{"text": "Physical security controls and backup systems", "isCorrect": false}, {"text": "Network firewalls and antivirus software only", "isCorrect": false}, {"text": "Encryption at rest and role-based access controls", "isCorrect": true}, {"text": "Strong passwords and security awareness training", "isCorrect": false}]	Encryption at rest and role-based access controls	HIPAA requires encryption of PHI and strict access controls to ensure only authorized personnel can access patient data.	{"summary": "HIPAA compliance requirements:", "breakdown": ["Encryption protects data if storage is compromised", "Role-based access ensures only authorized access", "Audit trails for compliance reporting", "These are fundamental HIPAA safeguards"], "otherOptions": "Important but insufficient for HIPAA compliance\\nPhysical controls are important but not primary for cloud\\nGood practices but don't directly protect PHI"}	\N	\N	92	\N	\N
66	66	Cloud Cost Management	Analysis	Cloud Architecture and Design	A company's cloud bill increased 150% over 6 months despite stable user numbers. Investigation reveals: 500 unused elastic IPs, 200TB of orphaned snapshots, 50 stopped but not terminated instances, and development databases running 24/7 on production-grade hardware. Which cost optimization strategy would yield the greatest immediate savings?	[{"text": "Reduce application features to decrease resource usage", "isCorrect": false}, {"text": "Migrate to a different cloud provider with lower rates", "isCorrect": false}, {"text": "Implement resource tagging and automated cleanup policies for unused resources", "isCorrect": true}, {"text": "Negotiate enterprise discounts and purchase reserved capacity", "isCorrect": false}]	Implement resource tagging and automated cleanup policies for unused resources	Automated cleanup policies immediately eliminate costs from unused resources (IPs, snapshots, stopped instances) while resource tagging enables ongoing cost visibility and management.	{"summary": "Immediate cost reduction through resource hygiene:", "breakdown": ["Unused elastic IPs: $0.005/hour each = $1,800/month savings", "Orphaned snapshots: $0.05/GB/month = $10,000/month savings", "Stopped instances: Still incur storage costs, termination saves 100%", "Automated policies prevent future resource accumulation"], "otherOptions": "Reserved capacity doesn't address unused resources\\nMigration costs outweigh potential savings\\nFeature reduction impacts business unnecessarily"}	\N	\N	93	\N	\N
67	67	Cloud Network Architecture	Application	Cloud Architecture and Design	A global company needs to connect 15 branch offices to their cloud infrastructure. Each office has different bandwidth requirements (10Mbps to 1Gbps) and varying security policies. Current MPLS costs are $50,000/month. Which cloud networking solution provides the most flexible and cost-effective approach?	[{"text": "Hub-and-spoke topology with central data center", "isCorrect": false}, {"text": "Individual site-to-site VPNs for each branch office", "isCorrect": false}, {"text": "Direct dedicated connections from each office to cloud", "isCorrect": false}, {"text": "SD-WAN overlay with cloud backbone and local internet breakout", "isCorrect": true}]	SD-WAN overlay with cloud backbone and local internet breakout	SD-WAN provides flexible bandwidth allocation, policy-based routing, and leverages cost-effective internet connections while maintaining security and performance.	{"summary": "SD-WAN advantages for multi-site cloud connectivity:", "breakdown": ["Dynamic bandwidth allocation based on real-time needs", "Local internet breakout reduces backhaul costs", "Policy-based routing enforces security requirements per site", "Typical 40-60% cost reduction versus MPLS"], "otherOptions": "15 individual VPNs create management complexity\\nDedicated connections too expensive for small sites\\nHub-and-spoke creates bottlenecks and latency"}	\N	\N	94	\N	\N
68	68	Cloud Monitoring and Logging	Analysis	Cloud Operations and Support	An e-commerce platform processes 1 million transactions daily across 50 microservices. The ops team struggles to troubleshoot issues due to distributed logs, missing correlation IDs, and 10TB daily log volume. Which observability strategy best addresses these challenges?	[{"text": "Centralize all logs to a single database with full-text search", "isCorrect": false}, {"text": "Increase log retention and add more verbose logging", "isCorrect": false}, {"text": "Create service-specific dashboards with custom metrics", "isCorrect": false}, {"text": "Implement distributed tracing, structured logging with correlation IDs, and intelligent log sampling", "isCorrect": true}]	Implement distributed tracing, structured logging with correlation IDs, and intelligent log sampling	Distributed tracing provides end-to-end visibility, correlation IDs link related events across services, and intelligent sampling reduces volume while preserving important events.	{"summary": "Modern observability for microservices requires:", "breakdown": ["Distributed tracing shows complete request flow across services", "Correlation IDs enable tracking single transactions through 50 services", "Structured logging improves queryability and reduces storage", "Intelligent sampling keeps important events while reducing volume 80%"], "otherOptions": "Single database can't handle 10TB daily efficiently\\nMore logs worsen the volume problem\\nDashboards don't solve log correlation issues"}	\N	\N	95	\N	\N
70	70	Cloud Service Models	Comprehension	Cloud Architecture and Design	A software startup needs to choose between IaaS, PaaS, and SaaS solutions for their new mobile app backend. They have 3 developers, limited DevOps experience, need to reach market in 3 months, and have $10,000 monthly budget. Which approach best balances their constraints?	[{"text": "Hybrid approach with IaaS compute and SaaS databases", "isCorrect": false}, {"text": "IaaS with full control over infrastructure and custom configuration", "isCorrect": false}, {"text": "PaaS for backend services with managed databases and authentication", "isCorrect": true}, {"text": "SaaS solutions only with no custom development", "isCorrect": false}]	PaaS for backend services with managed databases and authentication	PaaS provides the right abstraction level for a small team, offering managed services that accelerate development while staying within budget and timeline constraints.	{"summary": "PaaS advantages for startups:", "breakdown": ["Managed infrastructure reduces DevOps burden on 3-person team", "Built-in services (auth, databases) accelerate 3-month timeline", "Pay-per-use model fits $10,000 budget with room to scale", "Focus remains on app development, not infrastructure"], "otherOptions": "IaaS requires DevOps expertise they lack\\nPure SaaS too limiting for custom mobile backend\\nHybrid approach adds unnecessary complexity"}	\N	\N	96	\N	\N
71	76	Cloud Storage Concepts	Analysis	Cloud Architecture and Design	A media company needs storage for their video editing workflow with the following requirements: high IOPS for database operations, large capacity for raw video files, and long-term archival with cost optimization. Which storage architecture provides the BEST solution?	[{"text": "Network-attached storage (NAS) for all data types", "isCorrect": false}, {"text": "Object storage for all data with automated lifecycle policies", "isCorrect": false}, {"text": "All data on high-performance SSD storage", "isCorrect": false}, {"text": "Tiered storage with SSD for databases, HDD for active files, and cold storage for archives", "isCorrect": true}]	Tiered storage with SSD for databases, HDD for active files, and cold storage for archives	Tiered storage matches storage types to specific use cases: SSD for high IOPS databases, HDD for large file capacity, cold storage for cost-effective archival.	{"summary": "Optimal tiered storage strategy:", "breakdown": ["SSD tier: High IOPS and low latency for database operations", "HDD tier: Large capacity and moderate performance for active video files", "Cold storage: Cost-effective for long-term archival with slower retrieval", "Lifecycle automation: Automatic data movement based on access patterns"], "otherOptions": "All-SSD expensive for large video files and archives\\nNAS doesn't optimize for different performance requirements\\nObject storage alone may not provide required IOPS for databases"}	\N	\N	97	\N	\N
73	72	DevOps - CI/CD	Analysis	DevOps Fundamentals	A software company is experiencing frequent integration issues and broken builds after developers merge their code. They also have a slow release cycle. Which two (2) DevOps practices should they prioritize to address these problems?	[{"text": "Implement Continuous Deployment (Cand roll back frequently.", "isCorrect": false}, {"text": "Implement Continuous Integration (CI) and establish automated testing.", "isCorrect": true}, {"text": "Focus on manual code reviews and increase documentation efforts.", "isCorrect": false}, {"text": "Adopt a Microservices architecture and use serverless functions.", "isCorrect": false}]	Implement Continuous Integration (CI) and establish automated testing.	Continuous Integration (CI) focuses on frequent code integration and automated testing to catch issues early, while establishing automated testing verifies code quality and functionality, addressing broken builds and integration problems. These are foundational to speeding up the release cycle.	{"summary": "Addressing integration issues and slow releases with CI and automated testing:", "breakdown": ["**Continuous Integration (CI):** Integrates code changes frequently (multiple times a day), reducing integration hell and catching conflicts early.", "**Automated Testing:** Runs tests on every code commit or integration, immediately identifying broken builds and ensuring code quality and functionality before merging.", "These two practices are fundamental for ensuring a stable codebase and enabling faster, more reliable releases."], "otherOptions": "Microservices and serverless functions are architectural choices that may *support* better CI/CD, but do not directly solve existing integration issues or broken builds. \\nManual code reviews are important but often too slow and cannot reliably catch all integration issues in a fast-paced environment. Increasing documentation does not solve technical problems. \\nImplementing Continuous Deployment *before* fixing CI issues (broken builds, frequent integration problems) would lead to deploying broken software to production more rapidly, exacerbating the problem. Frequent rollbacks indicate a problematic CI/CD pipeline, not a solution."}	\N	\N	98	\N	\N
74	73	Cloud Service Models	Knowledge	Cloud Architecture and Design	A company wants to migrate their email system to the cloud but maintain full control over the operating system and middleware while letting the cloud provider manage the underlying infrastructure. Which service model BEST meets their requirements?	[{"text": "Function as a Service (FaaS)", "isCorrect": false}, {"text": "Software as a Service (SaaS)", "isCorrect": false}, {"text": "Infrastructure as a Service (IaaS)", "isCorrect": true}, {"text": "Platform as a Service (PaaS)", "isCorrect": false}]	Infrastructure as a Service (IaaS)	IaaS provides virtual infrastructure while allowing customers to maintain control over the operating system, middleware, and applications.	{"summary": "IaaS characteristics for email migration:", "breakdown": ["Customer controls: OS, middleware, applications, and data", "Provider manages: Physical hardware, hypervisor, networking", "Email flexibility: Can install any email server software", "Full administrative access to customize configurations"], "otherOptions": "SaaS provides ready-to-use applications with no OS control\\nPaaS abstracts OS layer, limiting administrative control\\nFaaS is for serverless functions, not email systems"}	\N	\N	99	\N	\N
75	74	Cloud Service Models	Application	Cloud Architecture and Design	A development team needs a cloud environment where they can deploy applications without managing servers, operating systems, or runtime environments. They want to focus solely on code development and automatic scaling based on demand. Which service model is MOST appropriate?	[{"text": "Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "Desktop as a Service (DaaS)", "isCorrect": false}, {"text": "Software as a Service (SaaS)", "isCorrect": false}, {"text": "Platform as a Service (PaaS)", "isCorrect": true}]	Platform as a Service (PaaS)	PaaS provides a development platform with automated scaling, allowing developers to focus on code while the platform handles infrastructure management.	{"summary": "PaaS benefits for development teams:", "breakdown": ["Abstracted infrastructure: No server or OS management needed", "Built-in scaling: Automatic resource allocation based on demand", "Development tools: Integrated IDEs, databases, and services", "Focus on code: Developers concentrate on application logic"], "otherOptions": "IaaS requires managing servers and OS\\nSaaS provides ready-made applications, not development platforms\\nDaaS provides virtual desktops, not development platforms"}	\N	\N	100	\N	\N
131	148	Operations	Knowledge	Operations and Support	What is the term for a predefined, documented set of procedures to be followed in response to a specific event or incident?	[{"text": "A service level agreement (SLA)", "isCorrect": false}, {"text": "A baseline", "isCorrect": false}, {"text": "A runbook", "isCorrect": true}, {"text": "A metric", "isCorrect": false}]	A runbook	A runbook is a compilation of routine procedures and operations that a system administrator or operator carries out. They are designed to be followed step-by-step to ensure consistency and speed when responding to a known scenario or alert.	{"summary": "This document is known as a runbook.", "breakdown": ["It contains step-by-step instructions for routine or emergency tasks.", "It is a key part of operational readiness and incident response.", "Runbooks can be manual (documents) or automated (scripts)."], "otherOptions": "An SLA is a contract about service performance.\\nA baseline is a measurement of normal performance.\\nA metric is a specific measurement (e.g., CPU %)."}	0	\N	101	\N	\N
76	75	Shared Responsibility Model	Application	Cloud Architecture and Design	A healthcare organization is concerned about data security compliance in their PaaS deployment. According to the shared responsibility model, which security aspects remain the customer's responsibility in a PaaS environment?	[{"text": "Physical security of data centers and network controls", "isCorrect": false}, {"text": "Hardware maintenance and power/cooling systems", "isCorrect": false}, {"text": "Application code security, data encryption, and user access management", "isCorrect": true}, {"text": "Hypervisor patching and host operating system security", "isCorrect": false}]	Application code security, data encryption, and user access management	In PaaS, customers are responsible for application-layer security including code security, data encryption, and identity/access management.	{"summary": "PaaS customer responsibilities:", "breakdown": ["Application security: Secure coding practices and vulnerability management", "Data protection: Encryption at rest and in transit", "Identity management: User authentication and authorization", "Compliance: Meeting regulatory requirements for data handling"], "otherOptions": "Physical security is provider responsibility\\nPlatform infrastructure managed by provider\\nHardware and facilities managed by provider"}	\N	\N	102	\N	\N
77	77	Container Technologies	Application	Cloud Architecture and Design	A company runs microservices using standalone containers but experiences challenges with scaling, service discovery, and load balancing during peak traffic. Which approach would BEST address these operational challenges?	[{"text": "Implement container orchestration with Kubernetes or Docker Swarm", "isCorrect": true}, {"text": "Use container registries for better image management", "isCorrect": false}, {"text": "Migrate all containers to virtual machines", "isCorrect": false}, {"text": "Increase container resource limits and add more standalone containers", "isCorrect": false}]	Implement container orchestration with Kubernetes or Docker Swarm	Container orchestration platforms provide automated scaling, service discovery, load balancing, and health management for containerized applications.	{"summary": "Container orchestration benefits:", "breakdown": ["Auto-scaling: Automatic container scaling based on demand", "Service discovery: Automatic service registration and routing", "Load balancing: Built-in traffic distribution across containers", "Health management: Automatic restart of failed containers"], "otherOptions": "Manual scaling doesn't solve automation challenges\\nVMs don't provide the orchestration features needed\\nRegistries help with image management but not runtime orchestration"}	\N	\N	103	\N	\N
78	78	Cloud Deployment Models	Application	Cloud Deployment	A financial institution requires strict data sovereignty, complete infrastructure control, and the ability to meet regulatory compliance requirements while gaining cloud benefits like scalability and self-service provisioning. Which deployment model is MOST suitable?	[{"text": "Public cloud with dedicated instances", "isCorrect": false}, {"text": "Hybrid cloud with data replication", "isCorrect": false}, {"text": "Private cloud hosted on-premises", "isCorrect": true}, {"text": "Community cloud shared with other financial institutions", "isCorrect": false}]	Private cloud hosted on-premises	Private cloud provides complete control, data sovereignty, and regulatory compliance while offering cloud capabilities like automation and scalability.	{"summary": "Private cloud benefits for financial institutions:", "breakdown": ["Complete control: Full authority over infrastructure and security", "Data sovereignty: Data remains within institutional boundaries", "Regulatory compliance: Easier to meet strict financial regulations", "Cloud benefits: Self-service, automation, and scalability features"], "otherOptions": "Public cloud may not meet data sovereignty requirements\\nHybrid cloud introduces complexity for strict compliance needs\\nCommunity cloud shares resources with other organizations"}	\N	\N	104	\N	\N
79	79	Cloud Migration	Analysis	Cloud Deployment	A company has a legacy monolithic application that works well but has outdated dependencies and architecture. They want to move to the cloud quickly while minimizing risk, then modernize later. Which migration strategy is MOST appropriate for the initial move?	[{"text": "Replace with a SaaS solution", "isCorrect": false}, {"text": "Rebuild the entire application using cloud-native services", "isCorrect": false}, {"text": "Refactor to microservices architecture immediately", "isCorrect": false}, {"text": "Rehost (lift and shift) to move quickly with minimal changes", "isCorrect": true}]	Rehost (lift and shift) to move quickly with minimal changes	Rehosting allows quick migration with minimal risk and changes, providing immediate cloud benefits while enabling future modernization phases.	{"summary": "Rehost strategy advantages:", "breakdown": ["Speed: Fastest migration approach with minimal changes", "Low risk: Preserves existing functionality and stability", "Immediate benefits: Cost savings and basic cloud features", "Future flexibility: Provides foundation for later modernization"], "otherOptions": "Refactoring increases complexity and migration risk\\nRebuilding takes significant time and resources\\nSaaS replacement may not maintain existing functionality"}	\N	\N	105	\N	\N
81	81	Cloud Observability	Analysis	Cloud Operations and Support	A microservices application experiences intermittent performance issues that are difficult to trace across multiple services. Standard monitoring shows healthy individual services, but users report slow response times. Which observability approach would BEST identify the root cause?	[{"text": "Set up alerting based on response time thresholds", "isCorrect": false}, {"text": "Implement distributed tracing across microservices", "isCorrect": true}, {"text": "Add more performance counters and metrics", "isCorrect": false}, {"text": "Increase log verbosity on all services", "isCorrect": false}]	Implement distributed tracing across microservices	Distributed tracing follows requests across multiple microservices, providing visibility into the complete request path and identifying bottlenecks.	{"summary": "Distributed tracing benefits:", "breakdown": ["End-to-end visibility: Tracks requests across all microservices", "Bottleneck identification: Shows where delays occur in the request path", "Service dependencies: Maps interactions between services", "Performance analysis: Measures latency at each service hop"], "otherOptions": "More logs don't provide cross-service correlation\\nAdditional metrics don't show service interactions\\nAlerting identifies problems but doesn't show root cause"}	\N	\N	106	\N	\N
82	82	Cloud Scaling	Application	Cloud Operations and Support	An e-commerce application experiences predictable traffic patterns with gradual increases during business hours and sudden spikes during flash sales. Which auto-scaling strategy would provide the BEST performance and cost optimization?	[{"text": "Predictive scaling with scheduled scaling for business hours and reactive scaling for spikes", "isCorrect": true}, {"text": "Manual scaling based on sales calendar events", "isCorrect": false}, {"text": "Fixed scaling with maximum capacity provisioned at all times", "isCorrect": false}, {"text": "Reactive scaling based only on CPU utilization", "isCorrect": false}]	Predictive scaling with scheduled scaling for business hours and reactive scaling for spikes	Combined predictive and reactive scaling handles both predictable patterns efficiently and responds to unexpected spikes automatically.	{"summary": "Hybrid scaling strategy benefits:", "breakdown": ["Predictive scaling: Anticipates business hour traffic increases", "Reactive scaling: Responds automatically to unexpected spikes", "Cost optimization: Scales down during low-traffic periods", "Performance assurance: Maintains responsiveness during all scenarios"], "otherOptions": "CPU-only reactive scaling too slow for sudden spikes\\nFixed capacity wastes resources during low traffic\\nManual scaling can't respond quickly to unexpected events"}	\N	\N	107	\N	\N
83	83	Cloud Backup Strategies	Analysis	Cloud Operations and Support	A company needs to design a backup strategy for critical business data with a Recovery Point Objective (RPO) of 1 hour and Recovery Time Objective (RTO) of 2 hours. The solution must be cost-effective while meeting compliance requirements for 7-year retention. Which backup approach is MOST suitable?	[{"text": "Weekly full backups with manual restore processes", "isCorrect": false}, {"text": "Daily full backups with 7-year retention in hot storage", "isCorrect": false}, {"text": "Real-time replication to a secondary site with immediate failover", "isCorrect": false}, {"text": "Hourly incremental backups with tiered storage and lifecycle policies", "isCorrect": true}]	Hourly incremental backups with tiered storage and lifecycle policies	Hourly incremental backups meet the RPO requirement, while tiered storage and lifecycle policies optimize costs for long-term retention.	{"summary": "Optimal backup strategy components:", "breakdown": ["Hourly incrementals: Meet 1-hour RPO requirement efficiently", "Tiered storage: Hot storage for recent backups, cold for long-term", "Lifecycle policies: Automatic movement to cheaper storage over time", "Fast recovery: 2-hour RTO achievable from recent incremental backups"], "otherOptions": "Daily backups exceed 1-hour RPO requirement\\nReal-time replication expensive for 7-year retention\\nWeekly backups far exceed RPO requirement"}	\N	\N	108	\N	\N
84	84	Cloud Security - Access Management	Application	Cloud Security	A global organization needs to manage user access to cloud resources across multiple locations with different security requirements. They want to implement Zero Trust principles while maintaining user productivity. Which approach BEST achieves these goals?	[{"text": "Role-based access control with periodic access reviews", "isCorrect": false}, {"text": "Single sign-on with basic username/password authentication", "isCorrect": false}, {"text": "VPN access with network-based security controls", "isCorrect": false}, {"text": "Multi-factor authentication with conditional access policies based on context", "isCorrect": true}]	Multi-factor authentication with conditional access policies based on context	Conditional access with MFA implements Zero Trust by continuously verifying users based on context like location, device, and behavior patterns.	{"summary": "Zero Trust conditional access benefits:", "breakdown": ["Context awareness: Considers location, device, time, and behavior", "Continuous verification: Doesn't trust based solely on network location", "Risk-based decisions: Adjusts requirements based on calculated risk", "User productivity: Seamless access for low-risk scenarios"], "otherOptions": "VPN assumes trust based on network location\\nBasic authentication insufficient for Zero Trust\\nRBAC alone doesn't provide continuous verification"}	\N	\N	109	\N	\N
85	85	Cloud Compliance	Analysis	Cloud Security	A healthcare organization moving to the cloud must demonstrate HIPAA compliance for patient data. They need automated compliance monitoring, evidence collection, and remediation capabilities. Which combination of cloud security controls provides the MOST comprehensive compliance framework?	[{"text": "Network segmentation with firewall rules and access logs", "isCorrect": false}, {"text": "Manual security audits with document-based evidence collection", "isCorrect": false}, {"text": "Encryption of all data with annual compliance reviews", "isCorrect": false}, {"text": "Cloud security posture management (CSPM) with automated policy enforcement and audit trails", "isCorrect": true}]	Cloud security posture management (CSPM) with automated policy enforcement and audit trails	CSPM provides continuous compliance monitoring, automated policy enforcement, and detailed audit trails required for HIPAA compliance demonstration.	{"summary": "CSPM benefits for HIPAA compliance:", "breakdown": ["Continuous monitoring: Real-time compliance posture assessment", "Automated enforcement: Immediate remediation of policy violations", "Audit trails: Comprehensive logging for compliance evidence", "Risk assessment: Identifies and prioritizes compliance gaps"], "otherOptions": "Manual audits don't provide continuous compliance monitoring\\nEncryption alone doesn't address all HIPAA requirements\\nNetwork controls are part of compliance but not comprehensive"}	\N	\N	110	\N	\N
86	86	Cloud Security - Vulnerability Management	Application	Cloud Security	A development team deploys applications using container images from various sources. Security scans reveal vulnerabilities in base images and third-party components. Which approach provides the BEST security posture for the container supply chain?	[{"text": "Implement security scanning throughout the CI/CD pipeline with policy enforcement", "isCorrect": true}, {"text": "Perform monthly vulnerability assessments on deployed containers", "isCorrect": false}, {"text": "Scan containers only in production environments", "isCorrect": false}, {"text": "Use only official base images from operating system vendors", "isCorrect": false}]	Implement security scanning throughout the CI/CD pipeline with policy enforcement	Pipeline security scanning catches vulnerabilities early, enforces security policies, and prevents vulnerable images from reaching production.	{"summary": "CI/CD security scanning benefits:", "breakdown": ["Shift-left security: Identifies vulnerabilities early in development", "Policy enforcement: Blocks deployment of vulnerable images", "Continuous scanning: Monitors throughout the software lifecycle", "Supply chain security: Validates all components and dependencies"], "otherOptions": "Production-only scanning allows vulnerabilities to reach live systems\\nOfficial images can still contain vulnerabilities\\nMonthly scans too infrequent for active development"}	\N	\N	111	\N	\N
87	87	DevOps CI/CD	Application	DevOps Fundamentals	A development team wants to implement automated deployments while ensuring code quality and minimizing deployment risks. They currently perform manual testing and deployment processes. Which CI/CD pipeline design BEST balances automation with quality assurance?	[{"text": "Automated deployment only to development environments", "isCorrect": false}, {"text": "Manual build with automated deployment to all environments", "isCorrect": false}, {"text": "Automated build and deployment without testing stages", "isCorrect": false}, {"text": "Automated build, test, and staged deployment with approval gates", "isCorrect": true}]	Automated build, test, and staged deployment with approval gates	Staged deployment with automated testing and approval gates provides comprehensive automation while maintaining quality controls and risk mitigation.	{"summary": "Comprehensive CI/CD pipeline benefits:", "breakdown": ["Automated testing: Catches issues early in the pipeline", "Staged deployment: Progressive rollout reduces risk", "Approval gates: Human oversight for critical stages", "Quality assurance: Multiple validation points ensure code quality"], "otherOptions": "No testing increases deployment risk\\nManual build defeats automation benefits\\nLimited to dev environments doesn't provide full deployment automation"}	\N	\N	112	\N	\N
88	88	DevOps Version Control	Knowledge	DevOps Fundamentals	A DevOps team manages both application code and infrastructure configurations. They need to implement version control strategies that support collaboration, change tracking, and rollback capabilities. Which approach provides the MOST comprehensive version management?	[{"text": "Version control for application code only, with manual infrastructure management", "isCorrect": false}, {"text": "Single repository with unified branching strategy for both code and infrastructure", "isCorrect": true}, {"text": "Multiple repositories per microservice with independent versioning", "isCorrect": false}, {"text": "Separate repositories for code and infrastructure with different branching strategies", "isCorrect": false}]	Single repository with unified branching strategy for both code and infrastructure	Unified repository and branching strategy ensures synchronized changes between application code and infrastructure, simplifying deployment and rollback procedures.	{"summary": "Unified version control benefits:", "breakdown": ["Synchronized changes: Code and infrastructure changes tracked together", "Simplified rollbacks: Single point to revert both code and infrastructure", "Consistent branching: Same workflow for all team members", "Atomic deployments: Code and infrastructure deployed as single unit"], "otherOptions": "Separate repositories can lead to version mismatches\\nManual infrastructure management introduces inconsistency\\nMultiple repositories increase complexity and coordination overhead"}	\N	\N	113	\N	\N
89	89	Cloud Troubleshooting - Network	Analysis	Troubleshooting	Users report intermittent connectivity issues to a cloud-hosted web application. The application works fine from the office but fails sporadically from remote locations. Network monitoring shows no infrastructure issues. Which troubleshooting approach would MOST effectively identify the root cause?	[{"text": "Analyze network paths and implement distributed monitoring from multiple locations", "isCorrect": true}, {"text": "Review application performance metrics and database queries", "isCorrect": false}, {"text": "Check firewall logs and security group configurations", "isCorrect": false}, {"text": "Increase server resources and add more instances", "isCorrect": false}]	Analyze network paths and implement distributed monitoring from multiple locations	Distributed monitoring from multiple geographic locations helps identify network path issues, ISP problems, or regional connectivity challenges.	{"summary": "Distributed network troubleshooting approach:", "breakdown": ["Geographic perspective: Monitoring from affected user locations", "Network path analysis: Traces routes to identify bottlenecks", "ISP correlation: Identifies provider-specific issues", "Performance baselines: Compares connectivity quality across locations"], "otherOptions": "Resource scaling doesn't address location-specific connectivity\\nSecurity configurations affect access, not intermittent connectivity\\nApplication metrics don't reveal network path issues"}	\N	\N	114	\N	\N
90	90	Cloud Troubleshooting - Performance	Analysis	Troubleshooting	A cloud application experiences performance degradation only during specific hours despite consistent user load. CPU and memory utilization remain within normal ranges. Database queries show normal execution times. Which factor is MOST likely causing the performance issues?	[{"text": "Database connection pool exhaustion", "isCorrect": false}, {"text": "Application memory leaks accumulating over time", "isCorrect": false}, {"text": "Network bandwidth limitations during peak traffic", "isCorrect": false}, {"text": "Resource contention with other workloads sharing the same physical infrastructure", "isCorrect": true}]	Resource contention with other workloads sharing the same physical infrastructure	Time-specific performance issues with normal resource utilization often indicate "noisy neighbor" problems where other workloads compete for underlying physical resources.	{"summary": "Noisy neighbor characteristics:", "breakdown": ["Time correlation: Performance degrades at specific, recurring times", "Normal metrics: Application-level resources appear adequate", "Shared infrastructure: Multiple workloads compete for physical resources", "External dependency: Performance affected by factors outside direct control"], "otherOptions": "Memory leaks would show gradually increasing memory usage\\nConnection pool issues would show in database connection metrics\\nNetwork bandwidth problems would show in network utilization metrics"}	\N	\N	115	\N	\N
91	91	Cloud Troubleshooting - Security	Application	Troubleshooting	A cloud application suddenly starts receiving "Access Denied" errors for API calls that were previously working. No code changes were deployed recently. Security logs show successful authentication but failed authorization. Which troubleshooting approach would MOST quickly identify the issue?	[{"text": "Examine application logs for authentication token issues", "isCorrect": false}, {"text": "Verify SSL certificates and encryption configurations", "isCorrect": false}, {"text": "Review recent changes to IAM roles, policies, and resource permissions", "isCorrect": true}, {"text": "Check network security group rules and firewall configurations", "isCorrect": false}]	Review recent changes to IAM roles, policies, and resource permissions	Access Denied with successful authentication indicates an authorization problem, typically caused by recent changes to IAM policies or role permissions.	{"summary": "Authorization troubleshooting focus areas:", "breakdown": ["IAM policy changes: Recent modifications to access permissions", "Role updates: Changes to role assignments or capabilities", "Resource permissions: Updates to resource-specific access controls", "Time-based policies: Scheduled changes or policy expirations"], "otherOptions": "Network rules affect connectivity, not authorization after authentication\\nSSL issues would prevent successful authentication\\nAuthentication is working; the issue is with authorization"}	\N	\N	116	\N	\N
92	92	Cloud Troubleshooting - Integration	Expert	Troubleshooting	After migrating a legacy application to the cloud, users report that batch processing jobs that completed in 2 hours on-premises now take 6 hours in the cloud. The application code was not modified during migration. Which factors should be investigated FIRST to identify the performance degradation?	[{"text": "Network latency between cloud services and data storage locations", "isCorrect": false}, {"text": "Application timeout settings and connection pool configurations", "isCorrect": false}, {"text": "Cloud instance sizing and compute resources compared to on-premises hardware", "isCorrect": false}, {"text": "Storage I/O performance and disk configuration differences", "isCorrect": true}]	Storage I/O performance and disk configuration differences	Batch processing performance is often heavily dependent on storage I/O patterns, which can be significantly different between on-premises and cloud storage configurations.	{"summary": "Storage I/O impact on batch processing:", "breakdown": ["I/O patterns: Batch jobs typically involve intensive read/write operations", "Storage types: Cloud storage may have different performance characteristics", "Configuration differences: RAID, caching, and optimization settings", "Sequential vs random: Batch workloads often require high sequential throughput"], "otherOptions": "Compute resources would show in CPU/memory utilization\\nNetwork latency affects real-time applications more than batch processing\\nConfiguration issues would likely cause failures, not just slower performance"}	\N	\N	117	\N	\N
93	93	Cloud Architecture - Deployment Models	Intermediate	Cloud Architecture and Design	A financial services company requires complete control over their cloud infrastructure while meeting strict compliance requirements. They want to leverage cloud benefits but cannot share physical hardware with other organizations. Which deployment model BEST meets these requirements?	[{"text": "Hybrid cloud with encrypted connections", "isCorrect": false}, {"text": "Public cloud with dedicated tenancy", "isCorrect": false}, {"text": "Community cloud with financial sector partners", "isCorrect": false}, {"text": "Private cloud with on-premises infrastructure", "isCorrect": true}]	Private cloud with on-premises infrastructure	Private cloud provides dedicated infrastructure, complete control, and meets compliance requirements without sharing hardware.	{"summary": "Private cloud characteristics:", "breakdown": ["Complete infrastructure control", "No shared hardware with other organizations", "Meets strict compliance requirements", "Maintains cloud benefits like scalability"], "otherOptions": "Public cloud still shares hardware\\nHybrid involves public cloud components\\nCommunity cloud shares with other organizations"}	\N	\N	118	\N	\N
94	94	Cloud Architecture - Service Models	Beginner	Cloud Architecture and Design	Your development team wants to deploy applications without managing operating systems, runtime environments, or middleware. Which cloud service model provides this capability?	[{"text": "Software as a Service (SaaS)", "isCorrect": false}, {"text": "Function as a Service (FaaS)", "isCorrect": false}, {"text": "Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "Platform as a Service (PaaS)", "isCorrect": true}]	Platform as a Service (PaaS)	PaaS provides a platform for developing and deploying applications without managing underlying infrastructure components.	{"summary": "PaaS eliminates infrastructure management:", "breakdown": ["Provides development platforms and runtime environments", "Manages OS, middleware, and runtime automatically", "Developers focus on application code only", "Examples: Azure App Service, Google App Engine"], "otherOptions": "IaaS requires OS and middleware management\\nSaaS provides complete applications, not development platforms\\nFaaS is for serverless functions, not full applications"}	\N	\N	119	\N	\N
95	95	Cloud Architecture - Scaling Strategies	Intermediate	Cloud Architecture and Design	An e-commerce application experiences predictable traffic spikes during holiday seasons. The current infrastructure manually scales servers, causing delays and potential revenue loss. Which scaling approach provides the MOST efficient solution?	[{"text": "Load balancing across existing servers only", "isCorrect": false}, {"text": "Vertical scaling with larger instances during peak periods", "isCorrect": false}, {"text": "Horizontal scaling with auto-scaling groups", "isCorrect": true}, {"text": "Manual scaling with pre-provisioned servers", "isCorrect": false}]	Horizontal scaling with auto-scaling groups	Auto-scaling automatically adds/removes instances based on demand, providing cost efficiency and handling unpredictable traffic patterns.	{"summary": "Auto-scaling benefits:", "breakdown": ["Automatically responds to demand changes", "Cost-effective: pay only for needed resources", "Handles unpredictable traffic patterns", "Reduces manual intervention and delays"], "otherOptions": "Vertical scaling has limits and potential downtime\\nManual scaling causes delays and inefficiency\\nLoad balancing without scaling doesn't add capacity"}	\N	\N	120	\N	\N
96	98	Operations - Monitoring and Logging	Advanced	Cloud Operations and Support	A cloud application experiences intermittent performance issues that are difficult to reproduce. The operations team needs comprehensive visibility into application performance, user experience, and infrastructure metrics. Which monitoring approach provides the BEST observability?	[{"text": "Network monitoring with bandwidth analysis", "isCorrect": false}, {"text": "Application Performance Monitoring (APM) with distributed tracing", "isCorrect": true}, {"text": "Log aggregation with keyword searching", "isCorrect": false}, {"text": "Infrastructure monitoring with basic alerting", "isCorrect": false}]	Application Performance Monitoring (APM) with distributed tracing	APM with distributed tracing provides comprehensive visibility into application performance across all components and services.	{"summary": "APM with distributed tracing provides:", "breakdown": ["End-to-end transaction visibility", "Performance bottleneck identification", "Service dependency mapping", "Real user experience monitoring"], "otherOptions": "Infrastructure monitoring lacks application-level insights\\nLog aggregation is reactive, not proactive\\nNetwork monitoring only covers network layer issues"}	\N	\N	121	\N	\N
97	99	Operations - Backup and Recovery	Intermediate	Cloud Operations and Support	A company's critical database requires a Recovery Point Objective (RPO) of 15 minutes and Recovery Time Objective (RTO) of 1 hour. Which backup and recovery strategy BEST meets these requirements?	[{"text": "Hourly incremental backups with manual restoration", "isCorrect": false}, {"text": "Real-time snapshots with 4-hour restoration window", "isCorrect": false}, {"text": "Daily full backups with weekly testing", "isCorrect": false}, {"text": "Continuous data replication with automated failover", "isCorrect": true}]	Continuous data replication with automated failover	Continuous replication ensures minimal data loss (15-minute RPO) and automated failover meets the 1-hour RTO requirement.	{"summary": "Meeting RPO/RTO requirements:", "breakdown": ["Continuous replication: minimal data loss", "Automated failover: fast recovery time", "15-minute RPO: very recent data recovery", "1-hour RTO: quick service restoration"], "otherOptions": "Daily backups exceed RPO requirements\\nManual restoration may exceed RTO\\n4-hour restoration exceeds RTO requirement"}	\N	\N	122	\N	\N
98	100	Operations - Automation and Orchestration	Advanced	Cloud Operations and Support	A company needs to deploy identical applications across multiple cloud environments (AWS, Azure, GCP) with consistent configuration and automated updates. Which approach provides the BEST multi-cloud orchestration?	[{"text": "Container orchestration with Kubernetes only", "isCorrect": false}, {"text": "Native cloud provider tools for each environment", "isCorrect": false}, {"text": "Manual deployment procedures with documentation", "isCorrect": false}, {"text": "Infrastructure as Code with cloud-agnostic tools like Terraform", "isCorrect": true}]	Infrastructure as Code with cloud-agnostic tools like Terraform	Terraform provides cloud-agnostic infrastructure provisioning with consistent syntax and state management across multiple cloud providers.	{"summary": "Multi-cloud IaC benefits:", "breakdown": ["Cloud-agnostic: Single tool for multiple providers", "Consistent configuration: Same syntax across environments", "Version control: Infrastructure changes tracked", "Automated deployment: Reduces manual errors"], "otherOptions": "Native tools create vendor lock-in and inconsistency\\nManual procedures are error-prone and don't scale\\nKubernetes handles container orchestration, not infrastructure provisioning"}	\N	\N	123	\N	\N
99	101	Troubleshooting - Performance Issues	Advanced	Troubleshooting	Users report slow application response times during peak hours. Monitoring shows high CPU utilization on application servers but normal database performance. Network latency is within acceptable ranges. Which troubleshooting approach should be the FIRST priority?	[{"text": "Implement application server horizontal scaling", "isCorrect": true}, {"text": "Optimize database query performance", "isCorrect": false}, {"text": "Upgrade network bandwidth capacity", "isCorrect": false}, {"text": "Increase database connection pool size", "isCorrect": false}]	Implement application server horizontal scaling	High CPU utilization on application servers indicates the bottleneck is at the compute layer, requiring additional server capacity.	{"summary": "Performance troubleshooting methodology:", "breakdown": ["Identify the bottleneck component (application servers)", "Address the root cause (CPU utilization)", "Scale horizontally for load distribution", "Monitor results and adjust as needed"], "otherOptions": "Database performance is normal\\nNetwork latency is acceptable\\nDatabase isn't the performance bottleneck"}	\N	\N	124	\N	\N
100	102	Troubleshooting - Network Connectivity	Intermediate	Troubleshooting	A cloud application deployed across multiple availability zones experiences intermittent connectivity issues between services. Some requests succeed while others timeout. Which troubleshooting steps should be performed FIRST?	[{"text": "Increase instance sizes across all zones", "isCorrect": false}, {"text": "Contact cloud provider support immediately", "isCorrect": false}, {"text": "Restart all application services", "isCorrect": false}, {"text": "Check security group rules and network ACLs", "isCorrect": true}]	Check security group rules and network ACLs	Intermittent connectivity issues often indicate network-level blocking. Security groups and NACLs are the most common cause of partial connectivity problems.	{"summary": "Network troubleshooting approach:", "breakdown": ["Security groups: Instance-level firewall rules", "Network ACLs: Subnet-level traffic control", "Intermittent issues: Often indicate partial blocking", "Rule verification: Check allowed ports and protocols"], "otherOptions": "Service restart doesn't address network-level issues\\nInstance size doesn't affect connectivity\\nShould troubleshoot systematically before escalating"}	\N	\N	125	\N	\N
101	238	Cloud Operations - Log Management	Application	Cloud Operations and Support	A healthcare organization operates 200 cloud-based application servers across multiple regions. They need centralized log collection for compliance auditing and must ensure accurate timestamps for all log entries. Which TWO solutions should be implemented? (Choose TWO)	[{"text": "Deploy log aggregation agents on each server", "isCorrect": false}, {"text": "Configure centralized syslog forwarding", "isCorrect": true}, {"text": "Enable NTP synchronization across all servers", "isCorrect": true}, {"text": "Implement local log rotation policies", "isCorrect": false}, {"text": "Use cloud provider managed logging service", "isCorrect": false}]	Configure centralized syslog forwarding, Enable NTP synchronization across all servers	Centralized syslog forwarding collects logs from all servers, while NTP synchronization ensures accurate timestamps essential for compliance auditing and log correlation.	{"summary": "Centralized logging requirements:", "breakdown": ["Syslog forwarding: Collects logs from distributed servers", "NTP synchronization: Ensures accurate, correlated timestamps", "Compliance needs: Audit trails require precise timing", "Cross-region consistency: All servers must use synchronized time"], "otherOptions": "Agents add complexity without solving time sync\\nLocal rotation doesn't provide centralization \\nManaged services still need time synchronization"}	1	{"Configure centralized syslog forwarding","Enable NTP synchronization across all servers"}	126	\N	\N
102	239	Cloud Architecture - API Design	Comprehension	Cloud Architecture and Design	A mobile application needs to minimize bandwidth usage when retrieving user profile data from a cloud API. The app only needs specific fields like name, email, and avatar, but the current REST API returns all profile fields. Which API design pattern would BEST optimize data transfer?	[{"text": "Implement API caching with Redis", "isCorrect": false}, {"text": "Use GraphQL with field selection", "isCorrect": true}, {"text": "Compress API responses with gzip", "isCorrect": false}, {"text": "Implement API pagination", "isCorrect": false}]	Use GraphQL with field selection	GraphQL allows clients to request exactly the fields they need, reducing bandwidth by eliminating unnecessary data transfer compared to REST APIs that return fixed response structures.	{"summary": "GraphQL bandwidth optimization:", "breakdown": ["Field selection: Request only needed data fields", "Single request: Eliminate multiple API calls", "Bandwidth reduction: Transfer only necessary information", "Mobile optimization: Critical for limited data plans"], "otherOptions": "Caching improves response time but does not reduce initial data transfer\\nCompression helps but does not eliminate unnecessary fields\\nPagination limits data volume but does not select specific fields"}	\N	\N	127	\N	\N
103	240	Cloud Troubleshooting - VDI	Application	Troubleshooting	Users can successfully connect to their cloud-hosted virtual desktops but receive authentication failures when trying to access domain resources. Other users on the same VDI infrastructure can access domain resources normally. What is the MOST likely cause?	[{"text": "VDI licensing has expired", "isCorrect": false}, {"text": "Network connectivity issues to domain controller", "isCorrect": false}, {"text": "Computer account trust relationship broken", "isCorrect": true}, {"text": "User profile corruption", "isCorrect": false}]	Computer account trust relationship broken	When users can connect to VDI but cannot authenticate to domain resources, it typically indicates the computer account trust relationship with the domain controller has been compromised.	{"summary": "VDI domain authentication issues:", "breakdown": ["Trust relationship: Computer accounts must be trusted by domain", "Selective failure: Only affects specific virtual desktops", "Authentication chain: VDI connects  Domain auth fails", "Resolution: Rejoin affected machines to domain"], "otherOptions": "Licensing would prevent VDI connection entirely\\nNetwork issues would affect all users consistently\\nProfile corruption affects user settings, not domain authentication"}	\N	\N	128	\N	\N
104	241	Cloud Security - Data Governance	Analysis	Cloud Security	A financial services company must comply with multiple regulations requiring different data retention periods: PCI DSS (1 year), SOX (7 years), and GDPR (right to be forgotten). How should they implement their cloud data retention strategy?	[{"text": "Apply the longest retention period (7 years) to all data", "isCorrect": false}, {"text": "Implement data classification with automated lifecycle policies", "isCorrect": true}, {"text": "Store all data indefinitely to ensure compliance", "isCorrect": false}, {"text": "Apply the shortest retention period (1 year) to minimize risk", "isCorrect": false}]	Implement data classification with automated lifecycle policies	Data classification enables different retention periods for different data types, with automated policies ensuring compliance with multiple regulations while supporting GDPR deletion rights.	{"summary": "Multi-regulation data retention:", "breakdown": ["Data classification: Categorize by regulatory requirements", "Automated policies: Enforce retention rules consistently", "Compliance matrix: Different data types = different rules", "GDPR balance: Enable deletion while maintaining required records"], "otherOptions": "Over-retention violates GDPR right to be forgotten\\nIndefinite storage violates multiple privacy regulations\\nUnder-retention violates SOX and other compliance requirements"}	\N	\N	129	\N	\N
105	242	Cloud Operations - Disaster Recovery	Application	Cloud Operations and Support	Following a complete cloud region outage, an e-commerce platform needs to restore service as quickly as possible. The company has a warm standby environment in another region with data replicated every 15 minutes. What should be the FIRST action?	[{"text": "Verify the Recovery Point Objective (RPO)", "isCorrect": false}, {"text": "Update DNS records to point to backup region", "isCorrect": true}, {"text": "Restore from the most recent backup", "isCorrect": false}, {"text": "Contact the cloud provider for status updates", "isCorrect": false}]	Update DNS records to point to backup region	With a warm standby already running, the fastest way to restore service is updating DNS records to redirect traffic to the backup region, minimizing downtime.	{"summary": "Warm standby failover process:", "breakdown": ["Warm standby: System already running and updated", "DNS failover: Fastest method to redirect traffic", "Minimize RTO: Immediate service restoration", "15-minute RPO: Acceptable data loss window"], "otherOptions": "RPO verification comes after service restoration\\nWarm standby eliminates need for backup restoration\\nProvider contact does not restore immediate service"}	\N	\N	130	\N	\N
106	243	Cloud Security - Authentication	Comprehension	Cloud Security	A cloud-based trading application experiences intermittent authentication failures with time-sensitive security tokens. The failures occur randomly across different user sessions and geographic locations. What is the MOST likely root cause?	[{"text": "Insufficient server processing power", "isCorrect": false}, {"text": "Network latency between regions", "isCorrect": false}, {"text": "Inconsistent time synchronization across servers", "isCorrect": true}, {"text": "Database connection timeouts", "isCorrect": false}]	Inconsistent time synchronization across servers	Time-sensitive security tokens depend on synchronized clocks. When servers have different times, valid tokens may appear expired or not yet valid, causing random authentication failures.	{"summary": "Time-sensitive token requirements:", "breakdown": ["Token timestamps: Include issued-at and expiration times", "Clock synchronization: All servers must have accurate time", "Random failures: Indicates time skew between servers", "Geographic distribution: NTP essential across regions"], "otherOptions": "Processing power affects response time, not token validation\\nNetwork latency affects communication speed, not time validation\\nDatabase timeouts would show consistent patterns, not random failures"}	\N	\N	131	\N	\N
107	244	Cloud Security - API Management	Analysis	Cloud Security	A cloud API serves sensitive financial data and experiences varying loads throughout the trading day. Peak trading hours require 10x normal capacity, but current REST endpoints over-fetch data, causing bandwidth and latency issues. Which combination addresses BOTH performance and security concerns?	[{"text": "Implement rate limiting and API caching", "isCorrect": false}, {"text": "Deploy GraphQL with OAuth 2.0 token validation", "isCorrect": true}, {"text": "Use WebSockets with TLS encryption", "isCorrect": false}, {"text": "Implement gRPC with mutual TLS authentication", "isCorrect": false}]	Deploy GraphQL with OAuth 2.0 token validation	GraphQL reduces over-fetching by allowing precise data selection, while OAuth 2.0 provides secure token-based authentication suitable for financial APIs with varying access patterns.	{"summary": "Financial API optimization:", "breakdown": ["GraphQL: Eliminates over-fetching for better performance", "OAuth 2.0: Industry standard for secure API access", "Token validation: Suitable for high-frequency trading systems", "Bandwidth reduction: Critical during peak trading periods"], "otherOptions": "Does not solve over-fetching problem\\nWebSockets for real-time but does not address over-fetching\\ngRPC efficient but more complex than needed"}	\N	\N	132	\N	\N
108	245	Cloud Security - Compliance Automation	Expert	Cloud Security	A multinational corporation processes personal data under GDPR, PCI DSS, and HIPAA regulations across different cloud regions. They need automated compliance monitoring and data subject rights management. Which approach provides the MOST comprehensive solution?	[{"text": "Manual quarterly compliance audits with documentation", "isCorrect": false}, {"text": "Implement cloud security posture management (CSPM) with automated remediation", "isCorrect": true}, {"text": "Deploy separate compliance tools for each regulation", "isCorrect": false}, {"text": "Use cloud provider native compliance dashboards only", "isCorrect": false}]	Implement cloud security posture management (CSPM) with automated remediation	CSPM provides continuous compliance monitoring across multiple regulations and cloud environments, with automated remediation capabilities and centralized data subject rights management.	{"summary": "Multi-regulation compliance automation:", "breakdown": ["CSPM: Continuous monitoring across all cloud resources", "Automated remediation: Fixes compliance violations immediately", "Multi-regulation support: Handles GDPR, PCI DSS, HIPAA simultaneously", "Data subject rights: Automated GDPR deletion and reporting"], "otherOptions": "Manual audits do not provide continuous monitoring or automation\\nSeparate tools create management complexity and gaps\\nNative dashboards lack cross-regulation correlation and automation"}	\N	\N	133	\N	\N
109	246	Cloud Architecture - VDI Scaling	Application	Cloud Architecture and Design	A company's VDI environment supports 500 remote workers. During peak hours, users experience slow login times and desktop responsiveness issues. The VDI infrastructure shows adequate CPU and memory resources. What should be addressed FIRST?	[{"text": "Increase the number of VDI host servers", "isCorrect": false}, {"text": "Optimize storage IOPS and implement caching", "isCorrect": true}, {"text": "Upgrade network bandwidth to the data center", "isCorrect": false}, {"text": "Reduce the number of applications per desktop", "isCorrect": false}]	Optimize storage IOPS and implement caching	VDI performance issues with adequate CPU/memory typically indicate storage bottlenecks. Multiple users accessing virtual desktops simultaneously creates high IOPS demand that requires optimization.	{"summary": "VDI storage performance optimization:", "breakdown": ["IOPS bottleneck: Multiple desktops competing for storage", "Boot storms: Users logging in simultaneously", "Profile loading: User data accessed from shared storage", "Caching: Reduces storage load for common desktop images"], "otherOptions": "CPU/memory adequate, more servers won't help\\nNetwork issues would affect all operations, not just peak times\\nApplication reduction does not address underlying storage bottleneck"}	\N	\N	134	\N	\N
110	127	Cloud Concepts	Knowledge	Cloud Architecture and Design	Bob is accessing a self-service portal in the cloud to instantly create additional servers, storage, and database instances for his firms DevOps group. Which of the following options best describes this operation?	[{"text": "On-demand", "isCorrect": true}, {"text": "Bursting", "isCorrect": false}, {"text": "Pay-as-you-grow", "isCorrect": false}, {"text": "Multitenancy", "isCorrect": false}]	On-demand	On-demand self-service is a key characteristic of cloud computing, allowing users to provision resources automatically without requiring human interaction from the service provider.	{"summary": "This scenario describes on-demand self-service.", "breakdown": ["Users can provision resources as needed.", "The process is automated and instantaneous.", "It is one of the five essential characteristics of cloud computing defined by NIST."], "otherOptions": "Bursting refers to scaling from a private to a public cloud for peak demand.\\nPay-as-you-grow is a pricing model, not the act of provisioning.\\nMultitenancy is the architecture where a single software instance serves multiple customers."}	0	\N	135	\N	\N
111	128	Deployment Models	Comprehension	Cloud Architecture and Design	Jillian is working on a project to interconnect her companys private data center to a cloud company that offers email services and another that can provide burstable compute capacity. What type of cloud delivery model is she creating?	[{"text": "Public", "isCorrect": false}, {"text": "Hybrid", "isCorrect": true}, {"text": "Community", "isCorrect": false}, {"text": "Private", "isCorrect": false}]	Hybrid	A hybrid cloud model is composed of two or more distinct cloud infrastructures (private, community, or public) that remain unique entities but are bound together by standardized or proprietary technology that enables data and application portability.	{"summary": "This describes a Hybrid cloud model.", "breakdown": ["It combines a private data center (private cloud) with public cloud services.", "This model allows organizations to leverage public cloud benefits while keeping sensitive data on-premises.", "The key is the interconnection between the different environments."], "otherOptions": "A public cloud is entirely hosted by a third-party provider.\\nA community cloud is shared by several organizations with common concerns.\\nA private cloud is operated solely for a single organization."}	0	\N	136	\N	\N
112	129	Cloud Concepts	Knowledge	Cloud Architecture and Design	Carl is learning how cloud service providers allocate physical resources into a group. These resources are then dynamically associated with cloud services as demand requires. What best describes this?	[{"text": "On-demand virtualization", "isCorrect": false}, {"text": "Dynamic scaling", "isCorrect": false}, {"text": "Resource pooling", "isCorrect": true}, {"text": "Elasticity", "isCorrect": false}]	Resource pooling	Resource pooling is the concept where a cloud providers computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand.	{"summary": "This is the definition of resource pooling.", "breakdown": ["Providers serve multiple customers from a shared pool of physical hardware.", "Resources are dynamically assigned based on demand.", "This is what enables the efficiency and scale of public cloud services."], "otherOptions": "Dynamic scaling is a result of resource pooling, not the concept itself.\\nElasticity is the ability to scale resources up and down, which is enabled by resource pooling."}	0	\N	137	\N	\N
113	130	Service Models	Application	Cloud Architecture and Design	Liza is a new Cloud+ architect for BigCo Inc. She is investigating cloud services that provide server hardware, but not applications. What cloud service is she using?	[{"text": "IaaS", "isCorrect": true}, {"text": "PaaS", "isCorrect": false}, {"text": "SaaS", "isCorrect": false}, {"text": "CaaS", "isCorrect": false}]	IaaS	Infrastructure as a Service (IaaS) is the cloud service model that provides fundamental computing resources such as virtual servers, storage, and networking. The customer is responsible for the operating system and applications.	{"summary": "IaaS provides the foundational infrastructure.", "breakdown": ["The customer rents IT infrastructureservers and virtual machines (VMs), storage, networks, operating systems.", "It offers the most control over the hardware and OS.", "Examples include AWS EC2, Azure VMs, and Google Compute Engine."], "otherOptions": "PaaS provides a platform and abstracts the OS.\\nSaaS provides the entire application.\\nCaaS (Containers as a Service) is a subset of IaaS focused on containers."}	0	\N	138	\N	\N
114	131	Service Models	Application	Cloud Architecture and Design	Harold is investigating his options to migrate his companys time and attendance application to the cloud. He wants to be responsible only for maintaining the application and would prefer that the public cloud company manage all underlying infrastructure and servers. What would you suggest that he implement?	[{"text": "IaaS", "isCorrect": false}, {"text": "PaaS", "isCorrect": true}, {"text": "SaaS", "isCorrect": false}, {"text": "CaaS", "isCorrect": false}]	PaaS	Platform as a Service (PaaS) provides a platform allowing customers to develop, run, and manage applications without the complexity of building and maintaining the infrastructure typically associated with developing and launching an app.	{"summary": "PaaS is the best fit for this requirement.", "breakdown": ["The cloud provider manages the OS, middleware, and runtime.", "The customer is only responsible for their application and data.", "This model accelerates development and reduces operational overhead."], "otherOptions": "IaaS would require Harold to manage the OS and servers.\\nSaaS would involve using a pre-built application, not migrating his own.\\nCaaS focuses on container management, but PaaS is a broader and better fit."}	0	\N	139	\N	\N
115	132	Shared Responsibility	Comprehension	Security	Jane is a Cloud+ architect who is working to educate her staff on the shared responsibility security model. In an IaaS deployment, which of the following is Janes company responsible for securing?	[{"text": "The virtualization software", "isCorrect": false}, {"text": "The physical servers", "isCorrect": false}, {"text": "The guest operating system", "isCorrect": true}, {"text": "The storage arrays", "isCorrect": false}]	The guest operating system	In the IaaS shared responsibility model, the customer is responsible for everything from the guest operating system upwards. This includes patching the OS, configuring it securely, and managing all applications and data running on it.	{"summary": "In IaaS, the customer manages the Guest OS and above.", "breakdown": ["Cloud Provider Responsibility: Physical data center, network infrastructure, virtualization hypervisor.", "Customer Responsibility: Guest OS, middleware, runtime, applications, and data security."], "otherOptions": "A, B, The virtualization software, physical servers, and storage arrays are all part of the underlying infrastructure managed by the cloud provider."}	0	\N	140	\N	\N
116	133	Cloud Concepts	Comprehension	Cloud Architecture and Design	A cloud provider has three data centers in close proximity, all interconnected with low-latency, high-bandwidth links. They are designed so that a failure in one does not affect the others. What does this grouping of data centers represent?	[{"text": "A region", "isCorrect": false}, {"text": "An availability zone", "isCorrect": true}, {"text": "A storage array", "isCorrect": false}, {"text": "A server rack", "isCorrect": false}]	An availability zone	While the term can sometimes refer to a single data center, an Availability Zone (AZ) is a fault-tolerant construct typically made of one or more data centers with redundant power, networking, and cooling. They are designed to be isolated from failures in other AZs.	{"summary": "This describes an Availability Zone (AZ).", "breakdown": ["An AZ is a location with one or more data centers.", "They are designed for high availability and fault tolerance.", "Multiple AZs are grouped together to form a Region."], "otherOptions": "A region is a larger geographic area that contains multiple AZs.\\nC, Storage arrays and server racks are components within a data center."}	0	\N	141	\N	\N
117	134	Security	Comprehension	Security	You are migrating a sensitive application to the cloud and need to ensure that the virtual servers are not running on the same physical hardware as any other customer. Which tenancy model should you select?	[{"text": "Multitenant", "isCorrect": false}, {"text": "Dedicated host", "isCorrect": true}, {"text": "Containerized", "isCorrect": false}, {"text": "Serverless", "isCorrect": false}]	Dedicated host	A dedicated host provides a physical server that is fully dedicated for your use. This ensures complete isolation from other customers at the hardware level, which is often a requirement for compliance or licensing reasons.	{"summary": "Dedicated host tenancy provides physical isolation.", "breakdown": ["The customer gets an entire physical server.", "It helps meet compliance requirements for physical isolation.", "It can be more expensive than standard multi-tenant instances."], "otherOptions": "Multitenant is the standard model where you share hardware.\\nContainerization provides OS-level isolation but not hardware isolation.\\nServerless is an execution model and does not provide hardware isolation."}	0	\N	142	\N	\N
118	135	Networking	Knowledge	Cloud Architecture and Design	Your company has a hybrid cloud deployment and requires a consistent, high-bandwidth, low-latency private connection between your on-premises data center and the cloud provider. Which of the following services should be used?	[{"text": "Site-to-Site VPN", "isCorrect": false}, {"text": "Direct Connect / ExpressRoute", "isCorrect": true}, {"text": "Client VPN", "isCorrect": false}, {"text": "NAT Gateway", "isCorrect": false}]	Direct Connect / ExpressRoute	Direct Connect (AWS) and ExpressRoute (Azure) are dedicated private network connection services. They bypass the public internet to provide a more reliable, faster, and lower-latency connection between an on-premises environment and the cloud.	{"summary": "A dedicated, private connection is the best solution.", "breakdown": ["Provides a private, dedicated link, not over the public internet.", "Offers higher bandwidth and more consistent network performance than VPN.", "It is the preferred method for enterprise-grade hybrid cloud connectivity."], "otherOptions": "A site-to-site VPN runs over the public internet and has variable performance.\\nA client VPN is for individual users, not for connecting data centers.\\nA NAT Gateway is for outbound internet access from private subnets."}	0	\N	143	\N	\N
119	136	Cloud Concepts	Comprehension	Cloud Architecture and Design	What is the term for a cloud architecture that uses services from more than one cloud provider to leverage the best features of each?	[{"text": "Hybrid cloud", "isCorrect": false}, {"text": "Multicloud", "isCorrect": true}, {"text": "Community cloud", "isCorrect": false}, {"text": "Private cloud", "isCorrect": false}]	Multicloud	A multicloud strategy involves using two or more cloud computing services from different cloud providers. This can be done to avoid vendor lock-in, for cost savings, or to use the best-of-breed services from each provider.	{"summary": "Using multiple providers is known as multicloud.", "breakdown": ["It avoids dependency on a single vendor.", "Allows for leveraging unique services from different providers (e.g., Google for AI, AWS for serverless).", "Can improve resilience and availability."], "otherOptions": "Hybrid cloud specifically refers to a mix of on-premises and public cloud.\\nA community cloud is shared by organizations with a common goal.\\nA private cloud is a single-tenant environment."}	0	\N	144	\N	\N
120	137	Migration	Knowledge	Deployment	A company wants to move a legacy application to the cloud as quickly as possible with the fewest changes to the application itself. What is this migration strategy commonly called?	[{"text": "Re-architecting", "isCorrect": false}, {"text": "Replatforming", "isCorrect": false}, {"text": "Lift and shift", "isCorrect": true}, {"text": "Repurchasing", "isCorrect": false}]	Lift and shift	Lift and shift, also known as rehosting, is a migration strategy where you move an application from on-premises to the cloud with minimal or no changes. It is the fastest way to start taking advantage of cloud infrastructure.	{"summary": "This strategy is called Lift and Shift (Rehosting).", "breakdown": ["Involves moving the application with minimal modifications.", "It is the quickest migration path.", "The application may not be optimized for the cloud, but this can be done later in a phased approach."], "otherOptions": "Re-architecting involves significant changes to the application to make it cloud-native.\\nReplatforming involves minor changes to take advantage of cloud services like managed databases.\\nRepurchasing means switching to a different product, often a SaaS solution."}	0	\N	145	\N	\N
121	138	Service Models	Comprehension	Cloud Architecture and Design	Frank is looking for a cloud service that will allow him to deploy his custom application but does not want to manage the underlying operating system. Which of the following cloud service models would you recommend to him?	[{"text": "IaaS", "isCorrect": false}, {"text": "PaaS", "isCorrect": true}, {"text": "SaaS", "isCorrect": false}, {"text": "DaaS", "isCorrect": false}]	PaaS	Platform as a Service (PaaS) provides the platformincluding the operating system, middleware, and runtimefor developers to build and deploy applications without managing the underlying infrastructure.	{"summary": "PaaS is the ideal model for this scenario.", "breakdown": ["The provider manages the OS, patching, and server maintenance.", "The developer focuses only on the application code and data.", "This accelerates the development lifecycle."], "otherOptions": "IaaS would require Frank to manage the OS.\\nSaaS would mean using a pre-existing application, not deploying his own.\\nDaaS (Desktop as a Service) provides virtual desktops."}	0	\N	146	\N	\N
122	139	Storage	Knowledge	Cloud Architecture and Design	What type of cloud storage is best suited for storing and serving large, unstructured data such as videos, images, and backups?	[{"text": "Block storage", "isCorrect": false}, {"text": "File storage", "isCorrect": false}, {"text": "Object storage", "isCorrect": true}, {"text": "Ephemeral storage", "isCorrect": false}]	Object storage	Object storage is designed to store massive quantities of unstructured data. Data is stored as objects, each with its own unique identifier, metadata, and the data itself. It is highly scalable and durable.	{"summary": "Object storage is best for unstructured data.", "breakdown": ["Stores data in a flat structure, not a file hierarchy.", "Accessed via APIs (typically REST).", "Extremely scalable and cost-effective for large datasets.", "Common use cases include backups, archives, data lakes, and static website assets."], "otherOptions": "Block storage provides raw volumes for servers, like a hard drive.\\nFile storage provides a hierarchical file system (like NFS or SMB).\\nEphemeral storage is temporary and is lost when an instance stops."}	0	\N	147	\N	\N
123	140	Business Continuity	Comprehension	Operations and Support	A company has defined that in the event of a disaster, they can tolerate losing up to 4 hours of data. What does this metric define?	[{"text": "Recovery Time Objective (RTO)", "isCorrect": false}, {"text": "Recovery Point Objective (RPO)", "isCorrect": true}, {"text": "Mean Time Between Failures (MTBF)", "isCorrect": false}, {"text": "Service Level Agreement (SLA)", "isCorrect": false}]	Recovery Point Objective (RPO)	The Recovery Point Objective (RPO) is a disaster recovery metric that defines the maximum acceptable amount of data loss, measured in time. An RPO of 4 hours means backups must be performed at least every 4 hours.	{"summary": "This metric is the Recovery Point Objective (RPO).", "breakdown": ["RPO is about data loss tolerance.", "It dictates the minimum frequency of backups or replication.", "A lower RPO generally means a more expensive disaster recovery solution."], "otherOptions": "RTO is the target time to restore the service, i.e., the acceptable downtime.\\nMTBF is a measure of reliability, not a disaster recovery target.\\nSLA is a formal agreement on service uptime and performance."}	0	\N	148	\N	\N
124	141	Business Continuity	Comprehension	Operations and Support	What is the key difference between a hot site and a cold site for disaster recovery?	[{"text": "A hot site is fully operational and ready for immediate failover; a cold site has only the basic infrastructure and requires significant setup.", "isCorrect": true}, {"text": "A hot site is located in a warm climate; a cold site is in a cold climate.", "isCorrect": false}, {"text": "A hot site uses physical servers; a cold site uses virtual servers.", "isCorrect": false}, {"text": "A hot site is for short-term outages; a cold site is for long-term outages.", "isCorrect": false}]	A hot site is fully operational and ready for immediate failover; a cold site has only the basic infrastructure and requires significant setup.	A hot site is a fully redundant data center with real-time data synchronization, allowing for near-instantaneous failover. A cold site is just a space with power and cooling, requiring equipment and data to be brought in, leading to a long recovery time.	{"summary": "Hot sites are ready immediately; cold sites are not.", "breakdown": ["Hot Site: Fully equipped, data is replicated, allows for very low RTO.", "Warm Site: Has hardware but requires data restoration.", "Cold Site: Basically an empty data center, has the longest RTO."], "otherOptions": "The terms are unrelated to climate.\\nBoth can use either physical or virtual servers.\\nThe choice depends on the required RTO, not the duration of the outage."}	0	\N	149	\N	\N
125	142	Security	Application	Security	You are setting up a security group for a web server. Which of the following inbound rules is the most appropriate and secure configuration?	[{"text": "Allow ALL traffic from source 0.0.0.0/0", "isCorrect": false}, {"text": "Allow TCP port 22 (SSH) from source 0.0.0.0/0", "isCorrect": false}, {"text": "Allow TCP ports 80 (HTTP) and 443 (HTTPS) from source 0.0.0.0/0", "isCorrect": true}, {"text": "Allow TCP port 3389 (RDP) from source 0.0.0.0/0", "isCorrect": false}]	Allow TCP ports 80 (HTTP) and 443 (HTTPS) from source 0.0.0.0/0	A web server needs to accept incoming traffic from any IP address (0.0.0.0/0) on the standard web ports, which are TCP 80 for HTTP and TCP 443 for HTTPS. All other ports should be restricted.	{"summary": "A secure web server only exposes necessary web ports.", "breakdown": ["Port 80 is for HTTP traffic.", "Port 443 is for HTTPS (secure) traffic.", "The source 0.0.0.0/0 means 'any IP address on the internet'.", "This follows the principle of least privilege by only opening the ports required for its function."], "otherOptions": "Allowing all traffic is extremely insecure.\\nB, Opening management ports like SSH or RDP to the entire internet is a major security risk and should be restricted to specific admin IPs."}	0	\N	150	\N	\N
126	143	Cloud Concepts	Comprehension	Cloud Architecture and Design	What does the term "elasticity" refer to in the context of cloud computing?	[{"text": "The ability of a system to remain operational despite component failures.", "isCorrect": false}, {"text": "The ability to automatically scale computing resources up and down to match demand.", "isCorrect": true}, {"text": "The ability to access services from anywhere over the network.", "isCorrect": false}, {"text": "The pooling of provider resources to serve multiple customers.", "isCorrect": false}]	The ability to automatically scale computing resources up and down to match demand.	Elasticity is the ability of the cloud to automatically and dynamically add or remove resources (like VMs or containers) to meet the current workload demand. This is a key benefit that prevents over-provisioning and reduces cost.	{"summary": "Elasticity is the automatic scaling of resources.", "breakdown": ["Scaling up (or out) to handle increases in load.", "Scaling down (or in) to save money when demand decreases.", "This process is typically automated based on metrics like CPU utilization or request count."], "otherOptions": "This describes fault tolerance or high availability.\\nThis describes broad network access.\\nThis describes resource pooling."}	0	\N	151	\N	\N
127	144	Networking	Knowledge	Troubleshooting	A network administrator is troubleshooting an issue where a user cannot resolve a website's domain name, such as www.example.com, to its IP address. Which service is most likely experiencing a problem?	[{"text": "DHCP", "isCorrect": false}, {"text": "DNS", "isCorrect": true}, {"text": "NAT", "isCorrect": false}, {"text": "BGP", "isCorrect": false}]	DNS	The Domain Name System (DNS) is responsible for translating human-readable domain names into the IP addresses that computers use to connect to each other. If this translation fails, the user cannot connect to the website.	{"summary": "DNS handles domain name to IP address translation.", "breakdown": ["DNS acts like the phonebook of the internet.", "When you type a domain name, your computer queries a DNS server to get the corresponding IP address.", "Failure in this process is a common cause of connectivity issues."], "otherOptions": "DHCP assigns IP addresses to devices on a local network.\\nNAT translates private IP addresses to public ones for internet access.\\nBGP is a routing protocol used by internet service providers."}	0	\N	152	\N	\N
128	145	DevOps	Knowledge	Deployment	What is the primary goal of Continuous Integration (CI) in a DevOps workflow?	[{"text": "To automatically deploy every change directly to production.", "isCorrect": false}, {"text": "To frequently merge developer code changes into a central repository and run automated builds and tests.", "isCorrect": true}, {"text": "To manage and provision infrastructure using code.", "isCorrect": false}, {"text": "To monitor the application and infrastructure for performance issues.", "isCorrect": false}]	To frequently merge developer code changes into a central repository and run automated builds and tests.	Continuous Integration (CI) is a DevOps practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run. The key goals are to find and address bugs quicker, improve software quality, and reduce the time it takes to validate and release new software updates.	{"summary": "CI is about frequent integration and automated testing.", "breakdown": ["Developers commit code to a shared repository multiple times a day.", "An automated system builds the application and runs a suite of tests.", "This provides rapid feedback, allowing teams to find and fix bugs early."], "otherOptions": "This describes Continuous Deployment, which is a subsequent step.\\nThis is Infrastructure as Code (IaC).\\nThis is Continuous Monitoring."}	0	\N	153	\N	\N
129	146	Deployment	Application	Deployment	A company wants to deploy a new version of their web application with zero downtime. The strategy involves setting up a completely new, identical environment with the new version, testing it, and then switching all user traffic from the old environment to the new one instantly. What is this deployment strategy called?	[{"text": "Canary deployment", "isCorrect": false}, {"text": "Rolling deployment", "isCorrect": false}, {"text": "Blue-green deployment", "isCorrect": true}, {"text": "In-place deployment", "isCorrect": false}]	Blue-green deployment	In a blue-green deployment, you create two separate, but identical, environments. One environment (blue) is running the current application version and one environment (green) is running the new application version. You can then switch traffic instantly from blue to green. This provides zero downtime and a rapid way to roll back if issues are found.	{"summary": "This is a blue-green deployment strategy.", "breakdown": ["Two identical production environments are maintained.", "Traffic is routed to only one environment at a time.", "This allows for safe testing of the new version before release and provides instant rollback capability."], "otherOptions": "A canary deployment releases the new version to a small subset of users first.\\nA rolling deployment gradually replaces old instances with new ones.\\nAn in-place deployment updates the code on the existing instances, causing downtime."}	0	\N	154	\N	\N
132	149	Containers	Comprehension	Cloud Architecture and Design	What is a primary benefit of using containers over traditional virtual machines?	[{"text": "Containers provide better hardware-level isolation.", "isCorrect": false}, {"text": "Containers are more lightweight and have faster startup times because they share the host OS kernel.", "isCorrect": true}, {"text": "Each container runs a full copy of the guest operating system.", "isCorrect": false}, {"text": "Containers are more difficult to manage and orchestrate.", "isCorrect": false}]	Containers are more lightweight and have faster startup times because they share the host OS kernel.	Containers virtualize the operating system, allowing multiple applications to run in isolated user spaces while sharing the same OS kernel. This makes them much more lightweight and faster to start than VMs, which must each boot a full guest OS.	{"summary": "Containers are lightweight due to sharing the host OS kernel.", "breakdown": ["VMs virtualize the hardware; containers virtualize the OS.", "This results in smaller image sizes and faster startup times (seconds vs. minutes).", "Higher density allows more containers than VMs to run on the same host."], "otherOptions": "VMs provide stronger, hardware-level isolation. Containers provide OS-level isolation.\\nThis describes a virtual machine, not a container.\\nWhile orchestration adds complexity, individual containers are generally easier to manage."}	0	\N	155	\N	\N
133	150	Security	Knowledge	Security	What is the purpose of a Web Application Firewall (WAF)?	[{"text": "To filter traffic at the network layer based on IP addresses and ports.", "isCorrect": false}, {"text": "To protect against application-layer attacks such as SQL injection and cross-site scripting (XSS).", "isCorrect": true}, {"text": "To provide secure remote access for employees to the corporate network.", "isCorrect": false}, {"text": "To scan virtual machine images for known vulnerabilities.", "isCorrect": false}]	To protect against application-layer attacks such as SQL injection and cross-site scripting (XSS).	A Web Application Firewall (WAoperates at Layer 7 (the application layer) to inspect HTTP traffic and protect web applications from common exploits that network firewalls cannot detect.	{"summary": "A WAF protects against Layer 7 application attacks.", "breakdown": ["It sits in front of web servers to filter malicious traffic.", "It can detect and block common attacks like SQL injection, cross-site scripting, and file inclusion.", "It helps organizations comply with regulations like PCI DSS."], "otherOptions": "This describes a network firewall or Network ACL.\\nThis describes a VPN.\\nThis describes a vulnerability scanner."}	0	\N	156	\N	\N
134	151	Operations	Application	Operations and Support	An administrator is trying to determine the normal performance of an application over a one-week period to set effective alerting thresholds. What is the administrator establishing?	[{"text": "A disaster recovery plan", "isCorrect": false}, {"text": "A performance baseline", "isCorrect": true}, {"text": "A security audit", "isCorrect": false}, {"text": "A capacity plan", "isCorrect": false}]	A performance baseline	A performance baseline is a standardized level of performance for a given system. It is established by collecting metrics (like CPU, memory, and latency) over a period of normal operation. This baseline is then used to identify deviations that may indicate a problem.	{"summary": "The administrator is establishing a performance baseline.", "breakdown": ["It defines what 'normal' looks like for the system.", "It is essential for effective monitoring and alerting.", "Alerts are configured to trigger when metrics deviate significantly from the established baseline."], "otherOptions": "A DR plan is for responding to disasters.\\nA security audit assesses security controls.\\nA capacity plan forecasts future resource needs."}	0	\N	157	\N	\N
135	152	Networking	Comprehension	Cloud Architecture and Design	What is the purpose of a load balancer in a cloud architecture?	[{"text": "To provide a private, dedicated connection to the cloud.", "isCorrect": false}, {"text": "To distribute incoming network traffic across multiple backend servers.", "isCorrect": true}, {"text": "To translate domain names into IP addresses.", "isCorrect": false}, {"text": "To cache content closer to end-users for faster delivery.", "isCorrect": false}]	To distribute incoming network traffic across multiple backend servers.	A load balancer acts as a reverse proxy and distributes network or application traffic across a number of servers. Load balancers are used to increase capacity (concurrent users) and reliability of applications.	{"summary": "Load balancers distribute traffic for scalability and availability.", "breakdown": ["Improves application scalability by spreading requests.", "Increases availability by routing traffic away from failed or unhealthy servers.", "Can perform health checks to monitor the status of backend servers."], "otherOptions": "This describes a Direct Connect or ExpressRoute.\\nThis describes DNS.\\nThis describes a Content Delivery Network (CDN)."}	0	\N	158	\N	\N
136	153	Storage	Knowledge	Cloud Architecture and Design	Which of the following is a characteristic of block storage?	[{"text": "It is accessed via API calls using HTTP methods.", "isCorrect": false}, {"text": "It stores data in a hierarchical structure of files and folders.", "isCorrect": false}, {"text": "It presents a raw storage volume to an operating system, which formats it with a file system.", "isCorrect": true}, {"text": "It is primarily used for long-term archival of unstructured data.", "isCorrect": false}]	It presents a raw storage volume to an operating system, which formats it with a file system.	Block storage provides raw storage volumes (blocks) that are attached to a server. The server's operating system can partition, format, and mount the volume just like a local physical hard drive (e.g., an SSD or HDD).	{"summary": "Block storage acts like a virtual hard drive for a server.", "breakdown": ["The cloud provider manages the physical hardware.", "The customer manages the volume and the file system on it (e.g., NTFS, ext4).", "It is used for performance-sensitive workloads like databases and virtual machine disks."], "otherOptions": "This describes object storage.\\nThis describes file storage.\\nThis is a use case for object storage (archive tier)."}	0	\N	159	\N	\N
137	154	Security	Comprehension	Security	Which security principle involves giving a user account or service only the permissions essential to perform its intended function?	[{"text": "Defense in depth", "isCorrect": false}, {"text": "Principle of least privilege", "isCorrect": true}, {"text": "Security through obscurity", "isCorrect": false}, {"text": "Separation of duties", "isCorrect": false}]	Principle of least privilege	The principle of least privilege requires that in a particular abstraction layer of a computing environment, every module (such as a process, a user, or a program) must be able to access only the information and resources that are necessary for its legitimate purpose.	{"summary": "This is the principle of least privilege.", "breakdown": ["It minimizes the potential damage from a security breach.", "If an account is compromised, the attacker only gains access to a limited set of resources.", "It is a fundamental concept in information security."], "otherOptions": "Defense in depth is about layering multiple security controls.\\nSecurity through obscurity is the ineffective practice of trying to hide vulnerabilities.\\nSeparation of duties involves splitting a critical task between multiple people."}	0	\N	160	\N	\N
138	155	Deployment	Knowledge	Deployment	A developer has written a script using a tool like Ansible to configure 100 web servers to have the exact same state (installed software, file permissions, etc.). What is this practice called?	[{"text": "Infrastructure as Code", "isCorrect": false}, {"text": "Configuration Management", "isCorrect": true}, {"text": "Continuous Integration", "isCorrect": false}, {"text": "Container Orchestration", "isCorrect": false}]	Configuration Management	Configuration management is the process of maintaining computer systems, servers, and software in a desired, consistent state. Tools like Ansible, Puppet, and Chef are used to automate the process of configuring and maintaining the state of servers.	{"summary": "This practice is known as configuration management.", "breakdown": ["It ensures consistency across multiple servers.", "It automates the setup and maintenance of server state.", "It helps prevent configuration drift, where servers become inconsistent over time."], "otherOptions": "Infrastructure as Code is broader and includes provisioning the servers themselves, not just configuring them.\\nContinuous Integration is about integrating and testing code.\\nContainer Orchestration is about managing the lifecycle of containers."}	0	\N	161	\N	\N
139	332	Deployment - Serverless	Knowledge	Cloud Deployment	A development team wants to focus all its efforts on creating and maintaining code. The team does not have\nthe resources to provision and scale the infrastructure their applications require to run. Which solution\nshould the development team use?	[{"text": "Containerize the app and deploy a container cluster service.", "isCorrect": false}, {"text": "Deploy a serverless compute subscription and upload the code", "isCorrect": true}, {"text": "Configure instance VMs and deploy app updates using a playbook", "isCorrect": false}, {"text": "Provision systems using templates and configure auto-scaling", "isCorrect": false}]	Deploy a serverless compute subscription and upload the code	The development team should deploy a serverless compute subscription and upload the code. \n\nIn serverless computing, the customer simply submits their application code, and the cloud service provider (CSP)\nprovisions and maintains the servers and infrastructure required to run an application. This includes code\nbackups, high availabilty features, and auto-scaling to meet increased workloads. \n\nContainers typically contain only the binaries and libraries to run a single app or service. However, a container is an infrastructure component, and the containers must be created, deployed, and periodically updated	{"summary": "The development team should deploy a serverless compute subscription and upload the code.", "breakdown": ["In serverless computing, the customer simply submits their application code, and the cloud service provider (CSP) provisions and maintains the servers and infrastructure required to run an application. This includes code backups, high availabilty features, and auto-scaling to meet increased workloads. ", "Containers typically contain only the binaries and libraries to run a single app or service. However, a container is an infrastructure component, and the containers must be created, deployed, and periodically updated"], "otherOptions": "VM templates can be compared to a clone. Many organizations use templates to speed the deployment of\\nfrequently used operating system configurations. Like containers, VMs are an infrastructure component\\nAn Ansible playbook is a great method for deploying automated configuration changes. However, in this\\nscenario the underlying VM would still need to be created and deployed."}	\N	\N	162	\N	\N
140	156	Operations	Comprehension	Troubleshooting	A cloud-hosted application has become completely unresponsive. The monitoring system shows that the virtual machine it runs on has a CPU utilization of 100%. What is the MOST likely cause of the issue?	[{"text": "A network misconfiguration", "isCorrect": false}, {"text": "A storage I/O bottleneck", "isCorrect": false}, {"text": "A runaway process or infinite loop in the application", "isCorrect": true}, {"text": "Insufficient memory (RAM)", "isCorrect": false}]	A runaway process or infinite loop in the application	When a system is unresponsive and the CPU is at 100%, it typically indicates that one or more processes are consuming all available processing power. This is often caused by a software bug, such as an infinite loop, or a process that has become stuck in a resource-intensive state.	{"summary": "100% CPU utilization points to a process-level problem.", "breakdown": ["This is a classic symptom of a software fault.", "The application is likely stuck in a loop or a high-intensity computation.", "Troubleshooting would involve identifying and terminating or debugging the offending process."], "otherOptions": "Network issues would not cause 100% CPU.\\nA storage bottleneck would show high disk I/O, not high CPU.\\nInsufficient memory would typically cause high memory usage and swapping, but not necessarily 100% CPU."}	0	\N	163	\N	\N
141	157	Cloud Concepts	Comprehension	Cloud Architecture and Design	Which two of the following are considered Capital Expenditure (CapEx)? (Choose TWO)	[{"text": "Paying a monthly bill for a cloud-based virtual machine.", "isCorrect": false}, {"text": "Purchasing physical servers for an on-premises data center.", "isCorrect": true}, {"text": "Paying for electricity and cooling for a data center.", "isCorrect": false}, {"text": "Buying a 5-year license for a piece of software.", "isCorrect": true}, {"text": "Paying for data transfer out of a public cloud.", "isCorrect": false}]	B, D	Capital Expenditure (CapEx) involves acquiring assets whose benefits extend beyond the current year. This includes buying physical hardware like servers and long-term software licenses. Cloud computing primarily shifts these costs to Operational Expenditure (OpEx).	{"summary": "CapEx is the upfront spending on physical or fixed assets.", "breakdown": ["Purchasing servers is a classic example of CapEx.", "A multi-year software license is also treated as a capital asset.", "Cloud computing helps companies reduce their CapEx in favor of OpEx."], "otherOptions": "A, C, Monthly cloud bills, utility costs, and data transfer fees are all examples of ongoing Operational Expenditure (OpEx)."}	1	{B,D}	164	\N	\N
142	158	Security	Application	Security	A company needs to provide its employees with access to a set of cloud-based virtual desktops. The solution must be centrally managed and accessible from any location. Which cloud service model should they use?	[{"text": "IaaS", "isCorrect": false}, {"text": "PaaS", "isCorrect": false}, {"text": "SaaS", "isCorrect": false}, {"text": "DaaS", "isCorrect": true}]	DaaS	Desktop as a Service (DaaS) is a cloud computing offering where a service provider delivers virtual desktops to end users over the internet, licensed with a per-user subscription. The provider takes care of the backend management for this VDI (Virtual Desktop Infrastructure) deployment.	{"summary": "Desktop as a Service (DaaS) provides virtual desktops.", "breakdown": ["It provides a full virtualized desktop experience to users.", "The infrastructure is managed by the cloud provider.", "It is a form of SaaS, specifically for delivering desktops."], "otherOptions": "A, IaaS and PaaS are for building and deploying applications, not for providing end-user desktops.\\nWhile DaaS is a type of SaaS, DaaS is the more specific and correct term for this use case."}	0	\N	165	\N	\N
143	159	Business Continuity	Knowledge	Operations and Support	What is the purpose of performing regular disaster recovery tests?	[{"text": "To satisfy the curiosity of the management team.", "isCorrect": false}, {"text": "To validate the effectiveness of the DR plan and identify any gaps or issues.", "isCorrect": true}, {"text": "To intentionally cause downtime to see how customers react.", "isCorrect": false}, {"text": "To reduce the overall cost of the disaster recovery solution.", "isCorrect": false}]	To validate the effectiveness of the DR plan and identify any gaps or issues.	Disaster recovery testing is a critical process that simulates a disaster scenario to ensure that the recovery plan is effective, the technical solutions work as expected, and the teams involved are prepared. It helps uncover issues that would otherwise only be found during a real disaster.	{"summary": "DR testing validates the recovery plan and team readiness.", "breakdown": ["It verifies that RPO and RTO targets can be met.", "It identifies technical problems with failover scripts or replication.", "It ensures that the operations staff are familiar with the procedures.", "It is a key requirement for many compliance frameworks."], "otherOptions": "A, The purpose is technical validation, not satisfying curiosity or testing customer reaction.\\nTesting often adds to the cost of DR but is essential for ensuring it works."}	0	\N	166	\N	\N
144	160	Networking	Comprehension	Troubleshooting	A user is able to connect to a web server using its IP address (e.g., 52.95.122.208) but not by using its domain name (e.g., www.example.com). What is the MOST likely problem?	[{"text": "The user's computer does not have an IP address.", "isCorrect": false}, {"text": "There is a problem with DNS resolution.", "isCorrect": true}, {"text": "The web server is down.", "isCorrect": false}, {"text": "A network firewall is blocking HTTP traffic.", "isCorrect": false}]	There is a problem with DNS resolution.	Since the user can connect via the IP address, we know that the server is running and network connectivity exists. The failure to connect via the domain name points directly to an issue with the Domain Name System (DNS), which is responsible for translating the name to the IP address.	{"summary": "This is a classic DNS resolution issue.", "breakdown": ["The ability to connect via IP proves the server is online and reachable.", "The failure of the domain name to work means the translation step is failing.", "The issue could be with the user's local DNS resolver, a corporate DNS server, or the public DNS records for the domain."], "otherOptions": "The user must have an IP address to connect to anything.\\nThe server cannot be down if a connection via IP is successful.\\nA firewall block would prevent connection to the IP address as well."}	0	\N	167	\N	\N
145	161	Cloud Concepts	Application	Operations and Support	A company is expecting a massive, temporary surge in traffic for a 24-hour marketing event. They need to rapidly provision a large number of servers to handle the load and then remove them immediately after the event. Which cloud characteristic is MOST critical to meet this requirement?	[{"text": "Measured service", "isCorrect": false}, {"text": "Broad network access", "isCorrect": false}, {"text": "Rapid elasticity", "isCorrect": true}, {"text": "Resource pooling", "isCorrect": false}]	Rapid elasticity	Rapid elasticity allows for the quick and automatic scaling of resources to meet demand. This is essential for handling sudden, large traffic spikes for events like this, and then scaling back down to save costs.	{"summary": "Rapid elasticity is key for handling traffic surges.", "breakdown": ["It allows for provisioning and de-provisioning resources in minutes.", "It enables companies to handle peak loads without permanently owning the required hardware.", "It is a core value proposition of cloud computing for variable workloads."], "otherOptions": "Measured service is the pay-as-you-go billing model.\\nBroad network access allows users to connect from anywhere.\\nResource pooling is the underlying mechanism that enables elasticity."}	0	\N	168	\N	\N
146	162	Security	Knowledge	Security	Which of the following cloud security tools is specifically designed to detect and alert on anomalous or malicious activity in your cloud account, such as an instance communicating with a known cryptocurrency mining pool?	[{"text": "A vulnerability scanner", "isCorrect": false}, {"text": "A threat detection service (e.g., AWS GuardDuty)", "isCorrect": true}, {"text": "A configuration management database (CMDB)", "isCorrect": false}, {"text": "A secrets management tool", "isCorrect": false}]	A threat detection service (e.g., AWS GuardDuty)	Cloud threat detection services use machine learning, anomaly detection, and integrated threat intelligence to continuously monitor for malicious activity and unauthorized behavior within a cloud environment.	{"summary": "This describes a cloud threat detection service.", "breakdown": ["It analyzes data sources like VPC Flow Logs, DNS logs, and CloudTrail events.", "It can detect threats like reconnaissance, instance compromise, and account compromise.", "It provides intelligent, actionable alerts on potential security issues."], "otherOptions": "A vulnerability scanner looks for known software vulnerabilities, it does not monitor live traffic.\\nA CMDB is a database of configuration items.\\nA secrets management tool stores credentials securely."}	0	\N	169	\N	\N
147	163	Deployment	Application	Deployment	You are deploying a new version of an application and want to ensure that it meets all security and compliance requirements before it goes live. Which of the following should be integrated into your CI/CD pipeline? (Choose TWO)	[{"text": "Static Application Security Testing (SAST)", "isCorrect": true}, {"text": "Manual deployment to production by the lead developer.", "isCorrect": false}, {"text": "Dynamic Application Security Testing (DAST)", "isCorrect": true}, {"text": "A step that emails the security team for approval before every test run.", "isCorrect": false}, {"text": "Disabling all automated tests to speed up the pipeline.", "isCorrect": false}]	A, C	Integrating security testing directly into the CI/CD pipeline is a key practice of DevSecOps. SAST analyzes the source code for vulnerabilities before the application is compiled, while DAST tests the running application for vulnerabilities, often in a staging environment.	{"summary": "Integrate both SAST and DAST into the CI/CD pipeline.", "breakdown": ["SAST (Static Testing): Analyzes code without executing it. Finds issues like SQL injection flaws or improper input validation early.", "DAST (Dynamic Testing): Tests the application while it is running. Finds runtime vulnerabilities and configuration errors.", "Automating these scans ensures security is a continuous part of the development process."], "otherOptions": "Manual deployments are error-prone and slow down the pipeline.\\nManual approvals should be for critical gates (like production release), not for every test run.\\nDisabling tests is the opposite of ensuring quality."}	1	{A,C}	170	\N	\N
149	169	Operations	Knowledge	Operations and Support	An organization needs to track all API calls made within their cloud account for security analysis and compliance auditing. Which service should they enable?	[{"text": "A performance monitoring tool (APM)", "isCorrect": false}, {"text": "A logging and auditing service (e.g., AWS CloudTrail)", "isCorrect": true}, {"text": "A billing dashboard", "isCorrect": false}, {"text": "A service catalog", "isCorrect": false}]	A logging and auditing service (e.g., AWS CloudTrail)	Services like AWS CloudTrail or Azure Monitor Audit Logs are specifically designed to record every API call made in an account. This provides a detailed audit trail of who did what, from where, and when, which is essential for security investigations and compliance.	{"summary": "Cloud logging and auditing services track all API activity.", "breakdown": ["Records API calls, including the user, source IP, time, and parameters.", "Provides an event history for security analysis and troubleshooting.", "Is a critical component for meeting compliance standards like PCI DSS, HIPAA, and SOC."], "otherOptions": "An APM tool monitors application performance, not account-level API calls.\\nA billing dashboard shows cost data, not API activity.\\nA service catalog is for managing approved services for deployment."}	0	\N	171	\N	\N
150	165	Troubleshooting	Comprehension	Troubleshooting	A user reports they are unable to access a newly deployed application in the cloud. You have verified the application is running correctly on the virtual machine. Which TWO of the following are the most likely causes of the connectivity issue? (Choose TWO)	[{"text": "The virtual machine's CPU is too small.", "isCorrect": false}, {"text": "The security group or firewall is blocking the user's traffic.", "isCorrect": true}, {"text": "The application is not compatible with the user's web browser.", "isCorrect": false}, {"text": "There is no route from the internet to the application's subnet.", "isCorrect": true}, {"text": "The user has forgotten their password for the application.", "isCorrect": false}]	B, D	When an application is confirmed to be running but is inaccessible from the outside, the problem is almost always related to networking. The two most common issues are firewalls (security groups, NACLs) blocking the traffic, or a missing route (e.g., no Internet Gateway or incorrect route table entry) that prevents traffic from reaching the subnet where the application resides.	{"summary": "Inaccessibility of a running app is usually a network issue.", "breakdown": ["Security Groups / Firewalls: These are stateful firewalls that must explicitly allow inbound traffic on the application's port.", "Routing: The VPC's route table must have a path from the source (e.g., an Internet Gateway) to the destination subnet."], "otherOptions": "A small CPU would cause performance issues, not a complete lack of access.\\nBrowser compatibility would result in a rendering error, not a connection failure.\\nA forgotten password would occur after connecting to the application's login page."}	1	{B,D}	172	\N	\N
151	166	Automation	Comprehension	Deployment	Which of the following best describes the difference between automation and orchestration?	[{"text": "Automation is for servers, while orchestration is for networks.", "isCorrect": false}, {"text": "Automation refers to a single task being performed without human intervention, while orchestration is the coordination of multiple automated tasks into a complete workflow.", "isCorrect": true}, {"text": "Automation requires scripting, while orchestration uses graphical user interfaces.", "isCorrect": false}, {"text": "Automation is a term for on-premises, while orchestration is used for the cloud.", "isCorrect": false}]	Automation refers to a single task being performed without human intervention, while orchestration is the coordination of multiple automated tasks into a complete workflow.	Automation focuses on making individual tasks repeatable and efficient. Orchestration takes a higher-level view, arranging and managing a sequence of automated tasks to deliver a service or complete a process, such as deploying a multi-tier application.	{"summary": "Orchestration is the automation of automation.", "breakdown": ["Automation Example: A script that installs a web server on a VM.", "Orchestration Example: A workflow that provisions a VPC, launches a database server, launches multiple web servers, and configures a load balancer to point to them."], "otherOptions": "A, C, These are false distinctions. Both can apply to any resource, use various tools, and be used in any environment."}	0	\N	173	\N	\N
156	177	Business Continuity	Comprehension	Operations and Support	During a disaster recovery test, the team was able to recover the application in 2 hours, but they discovered that the recovered database was 12 hours old. Which of the following has occurred?	[{"text": "The RTO was met, but the RPO was not.", "isCorrect": true}, {"text": "The RPO was met, but the RTO was not.", "isCorrect": false}, {"text": "Both the RTO and RPO were met.", "isCorrect": false}, {"text": "Neither the RTO nor RPO were met.", "isCorrect": false}]	The RTO was met, but the RPO was not.	The Recovery Time Objective (RTO) is the time it takes to restore the service, which was 2 hours. The Recovery Point Objective (RPO) is the amount of acceptable data loss. Since the data was 12 hours old, the 12-hour data loss exceeded their likely RPO target, meaning the RPO was not met.	{"summary": "Distinguish between recovery time and data loss point.", "breakdown": ["RTO (Time): The time to recover the service. 2 hours is the time it took.", "RPO (Data Point): The age of the data upon recovery. 12 hours old means 12 hours of data was lost."], "otherOptions": "B, C, Based on the definitions, the RTO was achieved but the RPO was missed."}	0	\N	174	\N	\N
152	167	Storage	Application	Cloud Architecture and Design	A company is designing a system to store medical images. The images must be stored with the highest level of durability, be replicated across multiple data centers automatically, and be accessible via a web-based API. Which storage type is the best fit?	[{"text": "Block storage attached to a virtual machine.", "isCorrect": false}, {"text": "File storage mounted on multiple servers.", "isCorrect": false}, {"text": "Object storage.", "isCorrect": true}, {"text": "A relational database.", "isCorrect": false}]	Object storage.	Object storage is designed for high durability and massive scalability. It inherently replicates data across multiple facilities (availability zones) and is accessed via APIs, making it perfect for storing large, unstructured data like medical images for web applications.	{"summary": "Object storage meets all the requirements.", "breakdown": ["High Durability: Cloud object storage typically offers 11 nines (99.999999999%) of durability.", "Automatic Replication: Data is automatically replicated across multiple AZs within a region.", "API Access: It is designed to be accessed programmatically via REST APIs."], "otherOptions": "Block storage is not inherently replicated across data centers and is not accessed via web APIs.\\nFile storage can be complex to scale and manage for petabytes of data.\\nRelational databases are not suitable for storing large binary files like images."}	0	\N	175	\N	\N
153	168	Security	Comprehension	Security	What is the purpose of using federated identity management (e.g., with SAML or OIDfor accessing cloud services?	[{"text": "To create and manage user accounts directly within each cloud service.", "isCorrect": false}, {"text": "To allow users to authenticate with their existing corporate credentials without creating new accounts in the cloud service.", "isCorrect": true}, {"text": "To encrypt all data traffic between the user and the cloud service.", "isCorrect": false}, {"text": "To enforce a single, strong password policy across all services.", "isCorrect": false}]	To allow users to authenticate with their existing corporate credentials without creating new accounts in the cloud service.	Federation establishes trust between an organization's identity provider (IdP) and the cloud service (SP). This allows users to sign in once to their corporate network and gain access to multiple trusted applications without needing to manage a separate set of credentials for each one.	{"summary": "Federation enables Single Sign-On (SSO) with existing credentials.", "breakdown": ["Users authenticate with their primary IdP (e.g., Active Directory).", "The IdP sends a secure assertion to the cloud service, verifying the user's identity.", "This improves user experience and centralizes access control."], "otherOptions": "Federation avoids the need to create separate user accounts.\\nWhile the connection is encrypted, that is not the primary purpose of federation.\\nPassword policy is enforced by the corporate IdP, not the federation itself."}	0	\N	176	\N	\N
154	170	Networking	Application	Troubleshooting	A web server is deployed in a public subnet, and a database server is in a private subnet. The web server cannot connect to the database. You have verified that the database is running. Which two settings should you investigate first? (Choose TWO)	[{"text": "The security group attached to the web server.", "isCorrect": false}, {"text": "The security group attached to the database server.", "isCorrect": true}, {"text": "The route table for the public subnet.", "isCorrect": false}, {"text": "The Network ACL (NACL) for the private subnet.", "isCorrect": true}, {"text": "The Internet Gateway (IGW) configuration.", "isCorrect": false}]	B, D	Connectivity issues between subnets are almost always caused by network filtering rules. The two primary filters are security groups (stateful firewalls attached to instances) and NACLs (stateless firewalls attached to subnets). You must check the DB security group to ensure it allows inbound traffic from the web server, and the private subnet's NACL to ensure it allows both inbound and outbound traffic for the database connection.	{"summary": "Check security groups and NACLs for inter-subnet connectivity.", "breakdown": ["The database security group must have an inbound rule allowing traffic from the web server's security group on the database port.", "The NACL on the private subnet must allow inbound traffic on the database port and outbound traffic on the ephemeral ports for the return connection."], "otherOptions": "The web server is initiating the connection, so its outbound rules are less likely to be the issue (they are typically permissive).\\nThe route table for the public subnet controls outbound internet traffic, not inter-subnet traffic.\\nThe IGW is for internet connectivity, not communication between subnets in the same VPC."}	1	{B,D}	177	\N	\N
155	171	High Availability	Comprehension	Cloud Architecture and Design	What is the primary benefit of designing an application to run across multiple availability zones?	[{"text": "It protects against the failure of an entire geographic region.", "isCorrect": false}, {"text": "It improves application performance by caching data closer to users.", "isCorrect": false}, {"text": "It provides high availability by protecting the application from the failure of a single data center.", "isCorrect": true}, {"text": "It reduces data transfer costs between services.", "isCorrect": false}]	It provides high availability by protecting the application from the failure of a single data center.	Availability Zones (AZs) are physically separate data centers within a region. By deploying an application across multiple AZs, it can continue to operate even if one of those data centers fails due to a power outage, flood, or other disaster. This is a fundamental pattern for building highly available systems in the cloud.	{"summary": "Multi-AZ architectures provide data center-level fault tolerance.", "breakdown": ["If one AZ fails, traffic is automatically routed to the healthy AZs.", "This allows the application to remain online without interruption.", "It is a standard best practice for all production workloads in the cloud."], "otherOptions": "To protect against regional failure, you need a multi-region architecture.\\nThis describes a Content Delivery Network (CDN).\\nData transfer between AZs typically incurs costs."}	0	\N	178	\N	\N
157	172	Cost Management	Application	Operations and Support	A company notices that their cloud bill is consistently high, even during nights and weekends when traffic is very low. They are running a fleet of virtual machines for a web application. Which of the following is the MOST effective strategy to reduce costs?	[{"text": "Purchase Reserved Instances for all virtual machines.", "isCorrect": false}, {"text": "Implement auto-scaling to automatically reduce the number of running instances during off-peak hours.", "isCorrect": true}, {"text": "Switch to a different cloud provider with a lower hourly rate.", "isCorrect": false}, {"text": "Manually shut down all servers every evening and restart them in the morning.", "isCorrect": false}]	Implement auto-scaling to automatically reduce the number of running instances during off-peak hours.	Auto-scaling is the ideal solution for workloads with variable traffic patterns. It automatically scales out (adds instances) to handle high demand and, crucially, scales in (removes instances) when demand is low, ensuring you only pay for the capacity you actually need.	{"summary": "Auto-scaling aligns cost with demand.", "breakdown": ["It avoids paying for idle resources during nights and weekends.", "The scaling can be based on a schedule for predictable traffic or on metrics for unpredictable traffic.", "This is a primary method for cost optimization in the cloud."], "otherOptions": "Reserved Instances provide a discount but do not address the problem of over-provisioning during low traffic periods.\\nMigrating providers is a major undertaking and does not solve the underlying issue of static capacity.\\nManual intervention is error-prone, can cause downtime, and is not a scalable solution."}	0	\N	179	\N	\N
216	334	Cloud Architecture - Service Models	Comprehension	Cloud Architecture and Models	Your company wants to deploy a web application without managing the underlying servers, operating systems, or runtime environments. Which service model best fits this requirement?	[{"text": "Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "Software as a Service (SaaS)", "isCorrect": false}, {"text": "Platform as a Service (PaaS)", "isCorrect": true}, {"text": "Function as a Service (FaaS)", "isCorrect": false}]	Platform as a Service (PaaS)	PaaS abstracts away infrastructure management while providing a platform for application development and deployment.	{"summary": "PaaS characteristics for this scenario:", "breakdown": ["Eliminates server and OS management overhead", "Provides development tools and runtime environments", "Allows focus on application code and business logic", "Examples: Azure App Service, Google App Engine, Heroku"], "otherOptions": "IaaS requires managing servers and OS\\nSaaS provides complete applications\\nFaaS is for event-driven functions"}	\N	\N	180	\N	\N
158	173	Security	Comprehension	Security	What is the primary function of a key management service (KMS) in the cloud?	[{"text": "To store and manage user passwords for applications.", "isCorrect": false}, {"text": "To create, manage, and control the use of cryptographic keys for data encryption.", "isCorrect": true}, {"text": "To manage SSH keys for accessing virtual machines.", "isCorrect": false}, {"text": "To store API keys for third-party services.", "isCorrect": false}]	To create, manage, and control the use of cryptographic keys for data encryption.	A Key Management Service (KMS) is a centralized system for managing the entire lifecycle of cryptographic keys. This includes key generation, storage, rotation, and deletion, as well as controlling who and what services can use the keys to encrypt and decrypt data.	{"summary": "KMS provides centralized control over encryption keys.", "breakdown": ["It simplifies the process of managing cryptographic keys at scale.", "It integrates with other cloud services to provide transparent data encryption.", "It provides detailed audit logs of all key usage, which is critical for compliance."], "otherOptions": "A, While a KMS could be used for this, a dedicated secrets management service is often a better fit for application credentials.\\nSSH key management is typically handled by other mechanisms, not a KMS."}	0	\N	181	\N	\N
159	174	Virtualization	Knowledge	Cloud Architecture and Design	In virtualization, what is the software that creates and runs virtual machines called?	[{"text": "A guest operating system", "isCorrect": false}, {"text": "A hypervisor", "isCorrect": true}, {"text": "A container engine", "isCorrect": false}, {"text": "A host operating system", "isCorrect": false}]	A hypervisor	The hypervisor, also known as a virtual machine monitor (VMM), is the software that creates the virtualized environment and manages the allocation of physical hardware resources (CPU, memory, storage) to the virtual machines.	{"summary": "The software is called a hypervisor.", "breakdown": ["Type 1 (Bare-Metal) hypervisors run directly on the host's hardware.", "Type 2 (Hosted) hypervisors run on top of a conventional operating system.", "It is the core component that enables virtualization."], "otherOptions": "A guest OS is the operating system running inside a VM.\\nA container engine (like Docker) runs containers, not VMs.\\nA host OS is the operating system on which a Type 2 hypervisor runs."}	0	\N	182	\N	\N
160	175	Networking	Knowledge	Cloud Architecture and Design	What type of network load balancer makes routing decisions based on information at Layer 7 of the OSI model, such as HTTP headers or URL paths?	[{"text": "A network load balancer", "isCorrect": false}, {"text": "An application load balancer", "isCorrect": true}, {"text": "A gateway load balancer", "isCorrect": false}, {"text": "A classic load balancer", "isCorrect": false}]	An application load balancer	Application Load Balancers (ALBs) operate at the application layer (Layer 7). This allows them to inspect application-level content and perform advanced routing, such as sending requests to different backend servers based on the URL path (e.g., /images vs. /api).	{"summary": "Application Load Balancers are Layer 7 aware.", "breakdown": ["They can make intelligent, content-based routing decisions.", "They support features like path-based routing, host-based routing, and SSL termination.", "They are ideal for modern microservices-based architectures."], "otherOptions": "A network load balancer operates at Layer 4 (Transport) and routes based on IP and port, not application content.\\nA gateway load balancer is used to deploy and scale third-party virtual network appliances.\\nA classic load balancer is a legacy type that has been largely superseded by ALBs and NLBs."}	0	\N	183	\N	\N
161	176	Cloud Concepts	Comprehension	Deployment	Which of the following scenarios is the BEST use case for an edge computing strategy?	[{"text": "Running a large-scale data warehousing and analytics platform.", "isCorrect": false}, {"text": "A factory that needs to process sensor data in real-time for immediate machine shutdown to prevent accidents.", "isCorrect": true}, {"text": "Storing long-term backups for regulatory compliance.", "isCorrect": false}, {"text": "Hosting a corporate employee intranet portal.", "isCorrect": false}]	A factory that needs to process sensor data in real-time for immediate machine shutdown to prevent accidents.	Edge computing is designed for use cases that require extremely low latency, real-time data processing, or operation during periods of disconnected network access. Processing IoT sensor data on-site at a factory for immediate safety responses is a classic example.	{"summary": "Edge computing is ideal for low-latency, real-time applications.", "breakdown": ["It brings computation and data storage closer to the sources of data.", "This reduces latency by avoiding a round-trip to a centralized cloud.", "It can improve security and operate even when the primary internet connection is down."], "otherOptions": "A, C, Data warehousing, long-term backups, and intranet portals are all well-suited for a centralized cloud architecture and do not have the same extreme low-latency requirements."}	0	\N	184	\N	\N
162	178	Security	Application	Security	An administrator needs to ensure that all data written to a cloud storage bucket is encrypted, without requiring any action from the applications or users who upload the data. Which feature should be enabled on the storage bucket?	[{"text": "Client-side encryption", "isCorrect": false}, {"text": "Server-side encryption", "isCorrect": true}, {"text": "Multi-factor authentication", "isCorrect": false}, {"text": "Access control lists (ACLs)", "isCorrect": false}]	Server-side encryption	Server-side encryption is a feature where the cloud provider automatically encrypts data as it is written to the storage service and decrypts it when it is accessed. This is transparent to the end-user or application and enforces encryption for all objects in the bucket.	{"summary": "Server-side encryption enforces encryption at rest.", "breakdown": ["The encryption and decryption are handled automatically by the service.", "It protects data from unauthorized access if the physical storage media is compromised.", "It is a standard security best practice and a requirement for many compliance frameworks."], "otherOptions": "Client-side encryption requires the application or user to encrypt the data *before* uploading it.\\nMFA is an access control measure for users, it does not encrypt data.\\nACLs are a form of access control, they do not encrypt data."}	0	\N	185	\N	\N
163	179	Automation	Comprehension	Deployment	Which of the following is an example of orchestration?	[{"text": "A script that automatically reboots a server every night.", "isCorrect": false}, {"text": "A workflow that provisions a network, deploys a database, deploys a set of web servers, and then configures a load balancer.", "isCorrect": true}, {"text": "A tool that ensures a specific software package is installed on all servers.", "isCorrect": false}, {"text": "A manual checklist that an operator follows to deploy an application.", "isCorrect": false}]	A workflow that provisions a network, deploys a database, deploys a set of web servers, and then configures a load balancer.	Orchestration is the coordination of multiple automated tasks to execute a larger workflow. Deploying a complete multi-tier application involves a sequence of dependent tasks, which is a perfect example of orchestration.	{"summary": "Orchestration coordinates multiple automated tasks.", "breakdown": ["It manages the entire lifecycle of a complex service.", "It handles dependencies and sequencing between different tasks.", "It is a higher level of automation than simply scripting a single action."], "otherOptions": "A, These are examples of automation (a single automated task), not orchestration.\\nThis is a manual process, not automation or orchestration."}	0	\N	186	\N	\N
164	180	Networking	Knowledge	Cloud Architecture and Design	What is the purpose of a VPC Peering connection?	[{"text": "To connect a VPC to the public internet.", "isCorrect": false}, {"text": "To connect a VPC to an on-premises data center.", "isCorrect": false}, {"text": "To connect two VPCs together privately, allowing them to communicate as if they were on the same network.", "isCorrect": true}, {"text": "To provide a dedicated endpoint for accessing a specific cloud service without traversing the internet.", "isCorrect": false}]	To connect two VPCs together privately, allowing them to communicate as if they were on the same network.	VPC Peering is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network.	{"summary": "VPC Peering connects two VPCs privately.", "breakdown": ["Traffic uses the cloud provider's private backbone, not the public internet.", "The connection is not a gateway or a VPN connection and does not rely on a separate piece of physical hardware.", "It has limitations, such as not being transitive (if A is peered with B, and B with C, A cannot talk to C)."], "otherOptions": "This is done with an Internet Gateway.\\nThis is done with a VPN or a Direct Connect/ExpressRoute.\\nThis describes a VPC Endpoint."}	0	\N	187	\N	\N
165	181	Cloud Concepts	Comprehension	Operations and Support	A company is considering moving from an on-premises data center to the cloud. Which of the following is a primary financial benefit of this move?	[{"text": "Converting Capital Expenditure (CapEx) to Operational Expenditure (OpEx).", "isCorrect": true}, {"text": "Converting Operational Expenditure (OpEx) to Capital Expenditure (CapEx).", "isCorrect": false}, {"text": "Eliminating all IT operational costs.", "isCorrect": false}, {"text": "Increasing the total cost of ownership (TCO).", "isCorrect": false}]	Converting Capital Expenditure (CapEx) to Operational Expenditure (OpEx).	One of the main financial drivers for cloud adoption is the shift from CapEx to OpEx. Instead of making large, upfront investments in hardware and data centers (CapEx), companies can pay a monthly, operational fee for the services they consume (OpEx).	{"summary": "Cloud shifts IT spending from CapEx to OpEx.", "breakdown": ["CapEx: Large, upfront investments in physical assets.", "OpEx: Ongoing, pay-as-you-go expenses.", "This shift improves cash flow and allows businesses to be more agile without large capital outlays."], "otherOptions": "This is the opposite of what happens.\\nCloud computing reduces some operational costs but does not eliminate them.\\nThe goal of moving to the cloud is typically to reduce the TCO."}	0	\N	188	\N	\N
166	182	Security	Application	Security	An application requires access to a database password to connect to its database. What is the MOST secure way to provide this credential to the application running on a virtual machine in the cloud?	[{"text": "Store the password in a plain text file on the virtual machine.", "isCorrect": false}, {"text": "Hardcode the password directly into the application's source code.", "isCorrect": false}, {"text": "Use a secrets management service to store the password and an IAM role to grant the application permission to retrieve it at runtime.", "isCorrect": true}, {"text": "Pass the password to the virtual machine as an environment variable during startup.", "isCorrect": false}]	Use a secrets management service to store the password and an IAM role to grant the application permission to retrieve it at runtime.	Storing secrets in a dedicated service like AWS Secrets Manager or Azure Key Vault is the security best practice. The application can then be given a specific IAM role that grants it permission to fetch only the secrets it needs, just-in-time. This avoids storing credentials on the instance or in the code.	{"summary": "Use a dedicated secrets management service with IAM roles.", "breakdown": ["It avoids hardcoding secrets, which is a major security risk.", "It allows for centralized management and rotation of secrets.", "Access to secrets can be tightly controlled and audited using IAM policies.", "This is the standard for modern, secure cloud applications."], "otherOptions": "A, B, Storing secrets in plain text, in code, or as environment variables are all insecure practices that expose the credentials."}	0	\N	189	\N	\N
167	183	Databases	Comprehension	Cloud Architecture and Design	What is a key benefit of using a managed database service (e.g., Amazon RDS, Azure SQL Database) compared to running a database on a self-managed virtual machine?	[{"text": "It provides full root access to the underlying operating system.", "isCorrect": false}, {"text": "It offloads operational tasks like patching, backups, and high availability to the cloud provider.", "isCorrect": true}, {"text": "It is always less expensive than a self-managed database.", "isCorrect": false}, {"text": "It allows you to use any database engine, including proprietary or custom ones.", "isCorrect": false}]	It offloads operational tasks like patching, backups, and high availability to the cloud provider.	The primary value of a managed database service is the reduction of operational burden. The cloud provider handles time-consuming administrative tasks, allowing the customer to focus on their application and data schema.	{"summary": "Managed databases reduce operational overhead.", "breakdown": ["Automated patching and software updates.", "Automated backups and point-in-time recovery.", "Simplified high availability and read replica setup.", "Built-in monitoring and alerting."], "otherOptions": "Managed services abstract the OS, so you do not get root access.\\nIt can sometimes be more expensive in terms of direct cost, but the TCO is often lower due to reduced operational effort.\\nManaged services support a specific set of popular database engines."}	0	\N	190	\N	\N
168	184	DevOps	Knowledge	Deployment	In a CI/CD pipeline, what is the purpose of the 'build' stage?	[{"text": "To provision the infrastructure needed for the application.", "isCorrect": false}, {"text": "To compile the source code into a deployable artifact, such as a binary, package, or container image.", "isCorrect": true}, {"text": "To run security scans on the production environment.", "isCorrect": false}, {"text": "To deploy the application to the end-users.", "isCorrect": false}]	To compile the source code into a deployable artifact, such as a binary, package, or container image.	The build stage is a fundamental part of the CI process. It takes the developer's source code, resolves dependencies, and compiles it into an executable or packaged format that can be tested and eventually deployed.	{"summary": "The build stage creates a deployable artifact.", "breakdown": ["It is typically triggered by a code commit to the repository.", "It may involve compiling code, running linters, and packaging assets.", "A successful build produces a single, versioned artifact that is used in all subsequent stages of the pipeline."], "otherOptions": "This is done during a provisioning stage, often using IaC.\\nSecurity scans can be part of the pipeline, but the build stage is specifically about compiling code.\\nThis is the deployment stage."}	0	\N	191	\N	\N
169	185	Business Continuity	Application	Troubleshooting	A company's disaster recovery plan states that they must have a fully functional copy of their infrastructure running in a secondary region, but it should handle no live traffic until a failover is initiated. During the failover, traffic is redirected to the secondary region. Which DR strategy is this?	[{"text": "Backup and Restore", "isCorrect": false}, {"text": "Pilot Light", "isCorrect": false}, {"text": "Warm Standby", "isCorrect": true}, {"text": "Multi-Site Active-Active", "isCorrect": false}]	Warm Standby	A warm standby strategy involves having a scaled-down but fully functional copy of your infrastructure running in the DR region. Upon failover, the system is scaled up to handle the full production load. It is faster than pilot light but less expensive than active-active.	{"summary": "This describes a warm standby DR strategy.", "breakdown": ["A scaled-down version of the full infrastructure is always running.", "Data is actively being replicated or backed up to the DR site.", "Recovery involves scaling up the resources and redirecting traffic.", "It provides a good balance between cost and recovery time (RTO)."], "otherOptions": "Backup and restore involves creating new infrastructure from backups, which is much slower.\\nPilot light only keeps the most critical core components running (like a database replica), not a full, scaled-down environment.\\nActive-active would involve the secondary site actively handling a portion of the live traffic."}	0	\N	192	\N	\N
170	186	Storage	Comprehension	Cloud Architecture and Design	What is a key difference between file storage and object storage?	[{"text": "File storage is for unstructured data, while object storage is for structured data.", "isCorrect": false}, {"text": "File storage presents data in a hierarchical file system, while object storage uses a flat address space.", "isCorrect": true}, {"text": "File storage is more scalable than object storage.", "isCorrect": false}, {"text": "File storage is accessed via an API, while object storage is mounted as a network drive.", "isCorrect": false}]	File storage presents data in a hierarchical file system, while object storage uses a flat address space.	This is the fundamental architectural difference. File storage (like NFS/SMuses a familiar hierarchy of directories and files. Object storage uses a flat model where each object is retrieved via a unique ID, along with its data and metadata, without a folder structure.	{"summary": "File storage is hierarchical; object storage is flat.", "breakdown": ["File Storage: Uses a file-and-folder structure, accessed via file paths.", "Object Storage: Stores objects in a single, massive pool, accessed via a unique object ID.", "This flat structure allows object storage to scale to virtually unlimited size, which is difficult for traditional file systems."], "otherOptions": "This is reversed; object storage is ideal for unstructured data.\\nThis is reversed; object storage is significantly more scalable.\\nThis is reversed; object storage is accessed via API, file storage is mounted."}	0	\N	193	\N	\N
171	195	Cloud Concepts	Knowledge	Cloud Architecture and Design	Which of the following are characteristics of a public cloud deployment model? (Choose TWO)	[{"text": "Resources are owned and operated by a third-party cloud provider.", "isCorrect": true}, {"text": "The infrastructure is shared among multiple organizations (multi-tenant).", "isCorrect": true}, {"text": "The infrastructure is dedicated to a single customer.", "isCorrect": false}, {"text": "It requires significant upfront capital expenditure (CapEx) from the customer.", "isCorrect": false}, {"text": "The customer has full physical control over the hardware.", "isCorrect": false}]	A, B	A public cloud is defined by two key characteristics: the infrastructure is owned and managed by a third-party provider (like AWS, Azure, or Google), and that infrastructure is shared by multiple customers in a multi-tenant model.	{"summary": "Public cloud is owned by a third party and is multi-tenant.", "breakdown": ["The provider is responsible for all hardware and data center management.", "Customers share the underlying physical resources, which is what drives the economies of scale.", "It operates on a pay-as-you-go, operational expenditure (OpEx) model."], "otherOptions": "C, Dedicated infrastructure with full physical control describes an on-premises data center or a private cloud.\\nPublic cloud is designed to reduce or eliminate customer CapEx."}	1	{A,B}	194	\N	\N
172	187	Security	Comprehension	Security	Which of the following statements are true about a stateless network firewall like a Network ACL (NACL)? (Choose TWO)	[{"text": "It automatically allows return traffic for an allowed inbound request.", "isCorrect": false}, {"text": "You must explicitly define both inbound and outbound rules for a request and its response to be successful.", "isCorrect": true}, {"text": "It is attached directly to virtual machine instances.", "isCorrect": false}, {"text": "It processes rules in order, starting from the lowest numbered rule, and stops when it finds a match.", "isCorrect": true}, {"text": "It can only have `allow` rules, not `deny` rules.", "isCorrect": false}]	B, D	Stateless firewalls do not track the state of connections. This means you must explicitly create rules for both the ingress (inbound) and egress (outbound) traffic. They evaluate rules in numerical order, and the first rule that matches the traffic is immediately applied.	{"summary": "NACLs are stateless and process rules in order.", "breakdown": ["Stateless: It does not remember previous packets. You must create an outbound rule to allow the return traffic for an inbound request.", "Rule Order: Rules are evaluated by number, from lowest to highest. The first matching rule is executed, and subsequent rules are ignored."], "otherOptions": "This describes a stateful firewall, like a security group.\\nNACLs are attached to subnets, not instances. Security groups are attached to instances.\\nNACLs can have both 'allow' and 'deny' rules."}	1	{B,D}	195	\N	\N
174	189	Operations	Comprehension	Operations and Support	An application team is consistently deploying new code that causes performance issues in production. The operations team wants to implement a process to catch these issues before they impact all users. Which of the following is the BEST strategy?	[{"text": "Require all developers to get senior management approval before committing code.", "isCorrect": false}, {"text": "Implement a blue-green deployment strategy where the new version is extensively load-tested in the 'green' environment before switching traffic.", "isCorrect": true}, {"text": "Double the amount of monitoring alerts to catch issues faster.", "isCorrect": false}, {"text": "Stop all new deployments until the application is rewritten.", "isCorrect": false}]	Implement a blue-green deployment strategy where the new version is extensively load-tested in the 'green' environment before switching traffic.	This strategy allows the new code to be deployed to a separate, identical production environment (green) where it can undergo rigorous performance and load testing without affecting any live users on the current (blue) environment. Only after it passes these tests is traffic switched over.	{"summary": "Blue-green deployment allows for safe pre-release testing.", "breakdown": ["The 'green' environment is a full-scale replica of production.", "It enables realistic load testing to identify performance regressions.", "If issues are found, the release is aborted with zero impact on live users.", "This is a key pattern for safe, high-quality releases."], "otherOptions": "This is a bureaucratic process that will slow down development without solving the technical problem.\\nMore alerts don't prevent the issues, they just report on them after they have already impacted users.\\nHalting deployments is not a sustainable business strategy."}	0	\N	196	\N	\N
175	191	Security	Application	Security	A company wants to ensure that all objects uploaded to a specific cloud storage bucket are automatically scanned for malware. What type of solution should they implement?	[{"text": "A security group attached to the storage bucket.", "isCorrect": false}, {"text": "An event-driven security workflow that triggers a scanning function on every object creation event.", "isCorrect": true}, {"text": "A lifecycle policy that deletes objects after 24 hours.", "isCorrect": false}, {"text": "A network ACL on the subnet where the bucket resides.", "isCorrect": false}]	An event-driven security workflow that triggers a scanning function on every object creation event.	Modern cloud platforms can generate events when actions occur, such as an object being uploaded. An event-driven architecture can listen for these 'object created' events and trigger a serverless function or container that runs a malware scanner on the newly uploaded object.	{"summary": "Use event-driven automation for real-time security scanning.", "breakdown": ["Storage services can emit events for actions like 'PutObject'.", "These events can trigger compute services (like AWS Lambda).", "The triggered function can then perform an action, such as scanning the object with an antivirus engine.", "This provides automated, real-time threat detection."], "otherOptions": "A, Security groups and NACLs are network firewalls; they cannot inspect the content of files for malware.\\nA lifecycle policy manages the storage class or deletion of objects over time; it does not scan them."}	0	\N	197	\N	\N
176	192	Databases	Comprehension	Cloud Architecture and Design	What is the primary use case for an in-memory database or cache (e.g., Redis, Memcached)?	[{"text": "For long-term, durable storage of relational data.", "isCorrect": false}, {"text": "To store data with extremely low latency requirements for rapid access, such as for caching database query results or user sessions.", "isCorrect": true}, {"text": "To store large binary files like videos and images.", "isCorrect": false}, {"text": "To meet strict data compliance and archival requirements.", "isCorrect": false}]	To store data with extremely low latency requirements for rapid access, such as for caching database query results or user sessions.	In-memory databases store data in RAM instead of on disk, which provides microsecond read and write latency. This makes them ideal for caching layers that absorb load from slower, disk-based databases, and for use cases that require real-time speed, like leaderboards or session stores.	{"summary": "In-memory caches provide ultra-low latency data access.", "breakdown": ["Data is stored in RAM, which is orders of magnitude faster than SSDs.", "Commonly used to cache frequently accessed data to reduce latency and database load.", "Can be used as a primary database for specific, high-performance use cases."], "otherOptions": "In-memory databases are typically not designed for long-term durability in the same way as disk-based databases.\\nWhile they can store binary data, they are not cost-effective for large files; object storage is better.\\nThis is a use case for archive-tier object storage, not an in-memory cache."}	0	\N	198	\N	\N
177	194	Troubleshooting	Comprehension	Troubleshooting	An administrator notices that a specific API call in their application is occasionally very slow. They want to understand the complete path of this request, including the time it spends in each downstream microservice it calls. Which observability tool would be BEST for this analysis?	[{"text": "Metrics dashboards", "isCorrect": false}, {"text": "Log aggregation", "isCorrect": false}, {"text": "Distributed tracing", "isCorrect": true}, {"text": "Uptime monitoring", "isCorrect": false}]	Distributed tracing	Distributed tracing is specifically designed to track the lifecycle of a single request as it propagates through a complex, distributed system like a microservices architecture. It provides a detailed, flame-graph view of the request path, showing the latency contributed by each service call.	{"summary": "Distributed tracing is the tool for analyzing request paths.", "breakdown": ["It captures the parent-child relationships between service calls.", "It allows you to pinpoint which specific downstream service is causing a slowdown.", "It is a key part of the 'three pillars of observability' (along with metrics and logs)."], "otherOptions": "Metrics can show you that the API is slow, but not *why* or *where* in the call stack the slowness is occurring.\\nLogs can provide details from each service, but correlating them for a single request without a trace ID is very difficult.\\nUptime monitoring only tells you if the service is up or down, not its performance."}	0	\N	199	\N	\N
178	196	Storage	Application	Deployment	An application needs a shared file system that can be mounted and accessed simultaneously by hundreds of Linux-based virtual machines running in different availability zones. The file system must support the NFS protocol. Which type of cloud storage solution is required?	[{"text": "Block storage", "isCorrect": false}, {"text": "Object storage", "isCorrect": false}, {"text": "A distributed file system service (e.g., Amazon EFS, Azure Files)", "isCorrect": true}, {"text": "Ephemeral storage", "isCorrect": false}]	A distributed file system service (e.g., Amazon EFS, Azure Files)	A distributed file system service is designed for this exact use case. It provides a managed, scalable file system that can be concurrently accessed by many clients using standard file protocols like NFS or SMB.	{"summary": "A managed, distributed file system is the correct solution.", "breakdown": ["It allows simultaneous access from many VMs.", "It can span multiple availability zones for high availability.", "It supports standard protocols like NFS, meaning no application changes are required.", "It scales automatically as data is added."], "otherOptions": "Block storage volumes cannot be mounted by hundreds of VMs simultaneously.\\nObject storage is accessed via API and cannot be mounted as a file system in this way.\\nEphemeral storage is temporary and not shared."}	0	\N	200	\N	\N
179	197	Security	Comprehension	Security	Which of the following BEST describes the concept of "defense in depth"?	[{"text": "Using only the strongest possible encryption for all data.", "isCorrect": false}, {"text": "Layering multiple, different security controls to protect an asset.", "isCorrect": true}, {"text": "Focusing all security efforts on protecting the network perimeter.", "isCorrect": false}, {"text": "Relying on a single, highly advanced security appliance.", "isCorrect": false}]	Layering multiple, different security controls to protect an asset.	Defense in depth is a security strategy that uses multiple layers of security controls. The idea is that if one layer fails, another layer is there to stop the attack. For example, protecting a database with a network firewall, host-based firewall, IAM permissions, and encryption.	{"summary": "Defense in depth is a layered security approach.", "breakdown": ["It provides redundancy in security.", "It protects against a wide variety of attack vectors.", "It acknowledges that no single security control is perfect.", "Layers can be physical, technical, and administrative."], "otherOptions": "Encryption is just one layer of a defense-in-depth strategy.\\nThis is the outdated 'perimeter security' model, which defense in depth improves upon.\\nRelying on a single control is the opposite of a layered approach."}	0	\N	201	\N	\N
180	198	Operations	Knowledge	Troubleshooting	An administrator is reviewing logs and sees a series of failed login attempts for a privileged account, followed by a successful login from an unfamiliar IP address. What type of incident might this indicate?	[{"text": "A denial-of-service (DoS) attack.", "isCorrect": false}, {"text": "A brute-force attack leading to an account compromise.", "isCorrect": true}, {"text": "A misconfigured network firewall.", "isCorrect": false}, {"text": "A server hardware failure.", "isCorrect": false}]	A brute-force attack leading to an account compromise.	The pattern of numerous failed logins followed by a success is the classic signature of a brute-force or dictionary attack, where an attacker tries many passwords until they guess the correct one. The login from an unfamiliar IP further suggests that the account has been compromised.	{"summary": "This pattern indicates a brute-force account compromise.", "breakdown": ["Multiple failed logins suggest an automated password guessing attack.", "The final successful login means the attack succeeded.", "The unfamiliar IP address is a strong indicator of an external attacker.", "This is a critical security incident that requires immediate response."], "otherOptions": "A DoS attack would involve overwhelming the service with traffic, not login attempts.\\nA firewall misconfiguration would block traffic, not cause failed logins.\\nA hardware failure would not manifest as login activity."}	0	\N	202	\N	\N
181	199	DevOps	Comprehension	Deployment	What is the relationship between containers and microservices?	[{"text": "They are the same thing.", "isCorrect": false}, {"text": "Microservices is an architectural style, and containers are a common technology used to package and deploy them.", "isCorrect": true}, {"text": "Containers are a type of microservice.", "isCorrect": false}, {"text": "You must use microservices in order to use containers.", "isCorrect": false}]	Microservices is an architectural style, and containers are a common technology used to package and deploy them.	Microservices is an approach to building an application as a collection of small, independent services. Containers (like Docker) are an ideal technology for running microservices because they provide lightweight isolation and a consistent runtime environment, making it easy to deploy and scale each service independently.	{"summary": "Microservices are an architecture; containers are a deployment technology.", "breakdown": ["You can run microservices without containers (e.g., on VMs).", "You can run monolithic applications inside containers.", "However, the two are a very popular and effective combination because containers make managing many small services much easier."], "otherOptions": "A, C, These statements represent common misconceptions about the relationship between the two concepts."}	0	\N	203	\N	\N
182	200	Cost Management	Comprehension	Operations and Support	A company has a number of virtual machines that are used for development and testing. These workloads are not critical and can tolerate interruptions. Which cloud pricing model would offer the greatest cost savings for these VMs?	[{"text": "On-demand", "isCorrect": false}, {"text": "Reserved instances", "isCorrect": false}, {"text": "Spot instances / Low-priority VMs", "isCorrect": true}, {"text": "Dedicated hosts", "isCorrect": false}]	Spot instances / Low-priority VMs	Spot instances (or Low-priority VMs) leverage a cloud provider's spare, unused compute capacity at a very large discount (often up to 90%) compared to on-demand prices. The trade-off is that the provider can reclaim this capacity at any time. This makes it a perfect fit for non-critical, fault-tolerant workloads like development, testing, and batch processing.	{"summary": "Spot instances offer the largest discounts for interruptible workloads.", "breakdown": ["They provide access to spare capacity at a steep discount.", "They can be terminated with very short notice.", "They are ideal for workloads that can be stopped and restarted without negative impact."], "otherOptions": "On-demand is the most expensive and flexible model.\\nReserved instances offer a discount for a long-term commitment and are for persistent workloads.\\nDedicated hosts are the most expensive option and are for workloads with specific compliance or licensing needs."}	0	\N	204	\N	\N
183	201	Networking	Comprehension	Cloud Architecture and Design	You need to create a logically isolated section of the public cloud where you can launch resources in a virtual network that you define. What is this isolated network environment called?	[{"text": "An availability zone", "isCorrect": false}, {"text": "A Virtual Private Cloud (VPor Virtual Network (VNet)", "isCorrect": true}, {"text": "A subnet", "isCorrect": false}, {"text": "A security group", "isCorrect": false}]	A Virtual Private Cloud (VPor Virtual Network (VNet)	A VPC (in AWS) or VNet (in Azure) is a private, isolated virtual network within the public cloud. It allows you to provision your own logically isolated section of the cloud where you can launch resources with full control over the IP address range, subnets, route tables, and network gateways.	{"summary": "This is a Virtual Private Cloud (VPor Virtual Network (VNet).", "breakdown": ["It provides network-level isolation for your cloud resources.", "You have control over the virtual networking environment.", "It is a fundamental building block for any cloud deployment."], "otherOptions": "An availability zone is a physical data center location.\\nA subnet is a segment or subdivision of a VPC/VNet.\\nA security group is a firewall for your virtual machines."}	0	\N	205	\N	\N
184	302	Databases	Comprehension	Cloud Architecture and Design	Which type of database is best suited for storing and querying data with complex relationships and a predefined schema, such as a customer relationship management (CRM) system?	[{"text": "A NoSQL key-value store", "isCorrect": false}, {"text": "A relational database (e.g., SQL)", "isCorrect": true}, {"text": "An in-memory cache", "isCorrect": false}, {"text": "An object storage system", "isCorrect": false}]	A relational database (e.g., SQL)	Relational databases excel at managing structured data with well-defined relationships between different data entities (e.g., customers, orders, products). They use a predefined schema to enforce data integrity and support complex queries using SQL (Structured Query Language).	{"summary": "Relational databases are for structured data with complex relationships.", "breakdown": ["Data is stored in tables with rows and columns.", "A schema defines the structure of the data.", "They enforce ACID (Atomicity, Consistency, Isolation, Durability) properties for transactions.", "Examples include MySQL, PostgreSQL, and Microsoft SQL Server."], "otherOptions": "A key-value store is a type of NoSQL database best for simple lookups, not complex relationships.\\nAn in-memory cache is for performance, not for durable, structured data storage.\\nObject storage is for unstructured binary data, not for structured, queryable data."}	0	\N	206	\N	\N
185	303	Automation	Comprehension	Deployment	What is a primary benefit of using version control (e.g., Git) for Infrastructure as Code (IaC)?	[{"text": "It guarantees that the infrastructure will never fail.", "isCorrect": false}, {"text": "It provides an auditable history of all changes made to the infrastructure and enables collaboration.", "isCorrect": true}, {"text": "It automatically optimizes the cost of the deployed infrastructure.", "isCorrect": false}, {"text": "It eliminates the need to test infrastructure changes.", "isCorrect": false}]	It provides an auditable history of all changes made to the infrastructure and enables collaboration.	Treating infrastructure as code allows you to store the configuration files in a version control system like Git. This provides a full, auditable log of every change, who made it, and when. It also enables collaboration through features like branching and pull requests.	{"summary": "Version control provides auditability and collaboration for IaC.", "breakdown": ["Every change to the infrastructure is tracked and can be reviewed.", "It makes it easy to roll back to a previous known-good configuration if a change causes problems.", "It allows multiple team members to work on the infrastructure configuration concurrently."], "otherOptions": "No system can guarantee zero failures.\\nCost optimization is a separate practice; IaC does not do it automatically.\\nOn the contrary, IaC makes it *easier* to test infrastructure changes before deploying them."}	0	\N	207	\N	\N
186	304	Security	Comprehension	Security	Which of the following cloud identity concepts allows an application to obtain temporary, limited-privilege credentials to access other cloud resources?	[{"text": "A user account with a permanent password.", "isCorrect": false}, {"text": "An IAM Role or Service Principal.", "isCorrect": true}, {"text": "An API key stored in a configuration file.", "isCorrect": false}, {"text": "A shared administrative account.", "isCorrect": false}]	An IAM Role or Service Principal.	IAM Roles (in AWS) or Service Principals (in Azure) are identities that applications or services can assume to securely obtain temporary security credentials. This is the recommended best practice for service-to-service authentication, as it avoids the use of long-lived, static credentials like API keys.	{"summary": "IAM Roles provide secure, temporary credentials for applications.", "breakdown": ["The application assumes the role at runtime to get temporary tokens.", "Permissions are defined on the role, not the application, following the principle of least privilege.", "This eliminates the risk of static, long-lived credentials being leaked."], "otherOptions": "A, C, Permanent passwords, stored API keys, and shared accounts are all insecure practices that should be avoided."}	0	\N	208	\N	\N
187	305	Cloud Concepts	Comprehension	Cloud Architecture and Design	A company is using a cloud provider that has multiple geographic locations around the world, such as "US East", "EU West", and "Asia Pacific (Tokyo)". What are these top-level geographic locations called?	[{"text": "Availability Zones", "isCorrect": false}, {"text": "Regions", "isCorrect": true}, {"text": "Data Centers", "isCorrect": false}, {"text": "Subnets", "isCorrect": false}]	Regions	A region is a distinct geographic area where a cloud provider has a collection of data centers. Regions are isolated from each other to provide fault tolerance and to allow customers to place resources closer to their end-users or to meet data sovereignty requirements.	{"summary": "These geographic locations are called Regions.", "breakdown": ["Regions are the highest level of geographic division in a cloud provider's infrastructure.", "Each region contains multiple, isolated Availability Zones.", "Choosing the right region is important for latency and data residency."], "otherOptions": "Availability Zones are the data centers *within* a region.\\nData center is a more generic term; Region is the specific term used by cloud providers.\\nSubnets are network segments *within* a virtual network."}	0	\N	209	\N	\N
188	306	Operations	Knowledge	Operations and Support	An administrator needs to automate the process of applying operating system security patches to a large fleet of virtual machines. Which type of tool is best suited for this task?	[{"text": "A monitoring tool", "isCorrect": false}, {"text": "A configuration management tool (e.g., Ansible, Puppet, Chef)", "isCorrect": true}, {"text": "An Infrastructure as Code tool (e.g., Terraform)", "isCorrect": false}, {"text": "A CI/CD tool (e.g., Jenkins)", "isCorrect": false}]	A configuration management tool (e.g., Ansible, Puppet, Chef)	Configuration management tools are designed to maintain the state of existing servers. This includes tasks like installing software, configuring services, and applying patches. They can be used to ensure that an entire fleet of servers is consistently patched and configured correctly.	{"summary": "Configuration management tools automate patching.", "breakdown": ["They can connect to a fleet of servers and execute tasks.", "They can check the current state and only apply changes if needed (idempotency).", "They are essential for managing large numbers of servers at scale."], "otherOptions": "A monitoring tool can tell you if a server needs patches, but it cannot apply them.\\nIaC tools are for provisioning the initial infrastructure, not for ongoing maintenance like patching.\\nCI/CD tools are for deploying application code, not for managing the OS."}	0	\N	210	\N	\N
189	307	Troubleshooting	Application	Troubleshooting	Users are reporting that a web application is extremely slow. A quick check shows that the database server has very high disk read latency and its I/O operations queue is full. Which of the following are the MOST likely solutions to this problem? (Choose TWO)	[{"text": "Increase the network bandwidth to the database server.", "isCorrect": false}, {"text": "Migrate the database to a storage volume with higher IOPS (e.g., from HDD to SSD).", "isCorrect": true}, {"text": "Implement a caching layer to reduce the number of read requests hitting the database.", "isCorrect": true}, {"text": "Increase the CPU cores of the web servers.", "isCorrect": false}, {"text": "Restart the database server.", "isCorrect": false}]	B, C	The symptomshigh read latency and a full I/O queuepoint directly to a storage performance bottleneck at the database. The two most effective solutions are to increase the underlying storage performance (by moving to a higher IOPS volume like an SSand to reduce the load on the database by implementing a cache for frequently read data.	{"summary": "Address a storage I/O bottleneck by improving storage or reducing load.", "breakdown": ["Upgrading storage from a standard HDD to a Provisioned IOPS SSD will directly increase the I/O capacity of the database.", "A caching layer (like Redis or Memcached) can handle a large percentage of the read requests, preventing them from ever reaching the overburdened database."], "otherOptions": "The problem is with disk I/O, not the network.\\nThe bottleneck is at the database, not the web servers.\\nRestarting the server will not fix an underlying performance bottleneck and will only cause downtime."}	1	{B,C}	211	\N	\N
190	308	Deployment	Knowledge	Deployment	Which of the following BEST describes a container image?	[{"text": "A running instance of an application with its dependencies.", "isCorrect": false}, {"text": "A lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, and settings.", "isCorrect": true}, {"text": "A type of virtual machine that includes a full guest operating system.", "isCorrect": false}, {"text": "A script used to configure a server's operating system.", "isCorrect": false}]	A lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, and settings.	A container image is a static, immutable file that contains all the necessary code, dependencies, and configurations needed to run an application. When you run the image, it becomes a container. This packaging makes the application highly portable.	{"summary": "A container image is a portable, self-contained software package.", "breakdown": ["It is a template or blueprint for creating a container.", "It includes the application code and all its dependencies.", "This ensures that the application runs consistently in any environment (development, testing, production)."], "otherOptions": "This describes a container, which is a running instance of an image.\\nThis describes a virtual machine image, which is much larger as it contains a full OS.\\nThis describes a configuration management script."}	0	\N	212	\N	\N
191	309	Security	Comprehension	Security	Which of the following is a key security concern specifically related to a multi-tenant cloud environment?	[{"text": "The physical security of the data center.", "isCorrect": false}, {"text": "The risk of data leakage or interference between different tenants sharing the same physical hardware.", "isCorrect": true}, {"text": "The need to patch the host operating system.", "isCorrect": false}, {"text": "The risk of a power failure in the data center.", "isCorrect": false}]	The risk of data leakage or interference between different tenants sharing the same physical hardware.	In a multi-tenant architecture, multiple customers (tenants) are running their applications on the same shared physical infrastructure. The primary security challenge is ensuring that these tenants are logically isolated and that one tenant cannot access another tenant's data or impact their performance (the "noisy neighbor" problem).	{"summary": "Tenant isolation is the primary security concern in multi-tenancy.", "breakdown": ["Cloud providers invest heavily in the hypervisor and other technologies to ensure strong logical isolation.", "Vulnerabilities in the hypervisor could potentially lead to a tenant 'escape' and compromise the host.", "This is a key area of focus for cloud security professionals."], "otherOptions": "A, C, Physical security, host patching, and power redundancy are responsibilities of the cloud provider but are not concerns specific *to the customer* in a multi-tenant model."}	0	\N	213	\N	\N
192	323	Cloud Concepts	Knowledge	Cloud Architecture and Design	What is the term for the cloud computing characteristic that allows a provider's resources to be shared among multiple customers, with safeguards for isolation?	[{"text": "Elasticity", "isCorrect": false}, {"text": "Multi-tenancy", "isCorrect": true}, {"text": "High availability", "isCorrect": false}, {"text": "On-demand", "isCorrect": false}]	Multi-tenancy	Multi-tenancy is an architecture in which a single instance of a software application serves multiple customers. Each customer is called a tenant. Tenants may be given the ability to customize some parts of the application, but they cannot customize the application's code. This is the model that allows public cloud providers to achieve massive economies of scale.	{"summary": "Multi-tenancy is the architecture of shared resources.", "breakdown": ["It is a core principle of public cloud computing.", "It allows for efficient resource utilization and lower costs.", "Strong logical isolation between tenants is a critical security requirement."], "otherOptions": "Elasticity is the ability to scale resources.\\nHigh availability is about resilience to failure.\\nOn-demand is the ability to self-provision resources."}	0	\N	214	\N	\N
193	310	Operations	Application	Troubleshooting	You are running a web application that consists of several microservices. Users report intermittent errors. You need to trace a single user's request from the initial load balancer all the way through the various microservices it interacts with to pinpoint where the error is occurring. Which of the following would you need to implement?	[{"text": "Centralized logging with correlation IDs.", "isCorrect": true}, {"text": "A network packet capture on the load balancer.", "isCorrect": false}, {"text": "CPU and memory monitoring for each microservice.", "isCorrect": false}, {"text": "An uptime monitoring service that pings the main URL.", "isCorrect": false}]	Centralized logging with correlation IDs.	To trace a single request through a distributed system, you need two things: 1) Centralized logging to bring all the logs from different services into one place, and 2) A correlation ID (or trace Ithat is generated at the start of the request and passed along to every microservice it touches. This allows you to filter the centralized logs to see the complete path of that single request. This is the foundation of distributed tracing.	{"summary": "Centralized logging with correlation IDs enables request tracing.", "breakdown": ["A correlation ID is a unique identifier attached to every log message generated by a single request.", "Centralized logging collects all logs into a searchable system.", "By searching for the correlation ID, you can reconstruct the entire journey of the user's request."], "otherOptions": "A packet capture is too low-level and difficult to analyze for application logic.\\nMetrics can show that a service is unhealthy, but not the path of the request that caused the error.\\nUptime monitoring only tells you if the entry point is available."}	0	\N	215	\N	\N
194	311	Storage	Knowledge	Cloud Architecture and Design	Which type of data is NOT well-suited for storage in a relational database?	[{"text": "Customer records with defined fields like name, address, and phone number.", "isCorrect": false}, {"text": "Financial transactions that must be ACID compliant.", "isCorrect": false}, {"text": "Large, unstructured binary files like high-resolution videos and audio files.", "isCorrect": true}, {"text": "An inventory system with tables for products, suppliers, and warehouses.", "isCorrect": false}]	Large, unstructured binary files like high-resolution videos and audio files.	Relational databases are optimized for structured, transactional data. They are not designed to store large binary objects (BLOBs), and doing so is inefficient, expensive, and can severely degrade database performance. This type of unstructured data is best stored in an object storage system.	{"summary": "Relational databases are poor at storing large, unstructured files.", "breakdown": ["Storing large files in a database bloats its size and slows down backups and queries.", "Object storage is designed for this use case and is much more scalable and cost-effective.", "The common pattern is to store the file in object storage and then store the *URL* or *identifier* of that object in the relational database."], "otherOptions": "A, B, Customer records, financial transactions, and inventory systems are all classic examples of structured, relational data that are a perfect fit for a relational database."}	0	\N	216	\N	\N
195	312	Deployment	Knowledge	Deployment	What is the purpose of a 'golden image' in cloud deployments?	[{"text": "To provide a standardized, pre-configured template for creating new virtual machine instances.", "isCorrect": true}, {"text": "To store the final, production-ready version of the application code.", "isCorrect": false}, {"text": "To serve as the primary backup for an entire cloud environment.", "isCorrect": false}, {"text": "To provide a graphical user interface for managing cloud resources.", "isCorrect": false}]	To provide a standardized, pre-configured template for creating new virtual machine instances.	A golden image is a template for a virtual machine (VM), virtual desktop, or server. It is created by an administrator to pre-install and pre-configure the operating system, software, and settings that are required for a specific purpose. This ensures that all new instances are created in a consistent and secure state.	{"summary": "A golden image is a pre-configured VM template.", "breakdown": ["It includes the base OS, security hardening configurations, and common software.", "It speeds up the process of deploying new instances.", "It ensures consistency and reduces configuration drift across the fleet of servers."], "otherOptions": "This is a deployable artifact, which is different from a VM image.\\nBackups are separate from deployment templates.\\nThis describes a cloud management console."}	0	\N	217	\N	\N
197	314	Troubleshooting	Comprehension	Troubleshooting	An application is experiencing intermittent errors. The administrator suspects the issue is caused by a recent change but is unsure which change is the culprit. Which of the following is the BEST first step in troubleshooting?	[{"text": "Immediately roll back all changes made in the last 24 hours.", "isCorrect": false}, {"text": "Review change logs and deployment records to correlate the start of the errors with a specific change.", "isCorrect": true}, {"text": "Increase the server capacity to see if the errors go away.", "isCorrect": false}, {"text": "Ask users to clear their browser cache.", "isCorrect": false}]	Review change logs and deployment records to correlate the start of the errors with a specific change.	When troubleshooting, it's crucial to be systematic. The most likely cause of a new problem is a recent change. Before taking any action, the administrator should investigate change management logs, version control history, and deployment records to find a correlation between a specific change and when the errors began.	{"summary": "Correlate the problem with recent changes.", "breakdown": ["Change is the most common cause of failure in IT systems.", "A systematic review of logs and records provides evidence for a theory of probable cause.", "This avoids guessing and potentially making the problem worse by taking random actions."], "otherOptions": "Rolling back all changes at once is a drastic step and may not be necessary. It's better to identify the specific problematic change first.\\nThis is a random action that doesn't address the likely root cause.\\nThis is unlikely to be the cause of server-side application errors."}	0	\N	218	\N	\N
198	315	Cloud Concepts	Application	Operations and Support	A start-up company wants to minimize its upfront IT costs and adopt a pay-as-you-go pricing model. They need the ability to scale their services quickly as their user base grows. Which of the following models is most suitable for them?	[{"text": "A traditional on-premises data center.", "isCorrect": false}, {"text": "A co-location facility.", "isCorrect": false}, {"text": "A public cloud service.", "isCorrect": true}, {"text": "A private cloud.", "isCorrect": false}]	A public cloud service.	Public cloud services are designed for this exact scenario. They require no upfront capital expenditure (CapEx), operate on a pay-as-you-go operational expenditure (OpEx) model, and offer rapid elasticity, allowing the company to scale its resources on-demand to match its growth.	{"summary": "Public cloud is ideal for startups needing agility and low upfront cost.", "breakdown": ["No CapEx: No need to buy expensive hardware.", "Pay-as-you-go (OpEx): Aligns cost directly with usage.", "Scalability & Elasticity: Resources can be scaled up or down in minutes.", "This allows the startup to focus its capital on its core business, not on IT infrastructure."], "otherOptions": "A, B, On-premises, co-location, and private cloud all require significant upfront capital investment and do not offer the same level of elasticity as the public cloud."}	0	\N	219	\N	\N
199	316	High Availability	Knowledge	Cloud Architecture and Design	Which of the following describes the ability of a system to continue functioning even if one of its components fails?	[{"text": "Scalability", "isCorrect": false}, {"text": "Elasticity", "isCorrect": false}, {"text": "Fault tolerance", "isCorrect": true}, {"text": "Agility", "isCorrect": false}]	Fault tolerance	Fault tolerance is the property that enables a system to continue operating properly in the event of the failure of some of its components. This is typically achieved through redundancy, such as running multiple web servers, database replicas, or deploying across multiple data centers.	{"summary": "This is the definition of fault tolerance.", "breakdown": ["It is a key principle for building highly available and reliable systems.", "It is achieved by eliminating single points of failure.", "Examples include load balancers, database clustering, and multi-AZ deployments."], "otherOptions": "Scalability is the ability to handle increased load.\\nElasticity is the ability to automatically scale resources.\\nAgility is the ability to develop and deploy applications quickly."}	0	\N	220	\N	\N
200	317	Networking	Knowledge	Cloud Architecture and Design	You are designing a virtual network for your company. You need to divide the network into smaller, isolated segments to group related resources and apply specific security policies. What are these network segments called?	[{"text": "Regions", "isCorrect": false}, {"text": "Availability Zones", "isCorrect": false}, {"text": "Subnets", "isCorrect": true}, {"text": "Security Groups", "isCorrect": false}]	Subnets	A subnet (subnetwork) is a logical subdivision of an IP network. In a cloud VPC, you divide your network into subnets to isolate resources. For example, you typically place public-facing web servers in a public subnet and backend database servers in a private subnet with stricter security.	{"summary": "These network segments are called subnets.", "breakdown": ["Each subnet has its own CIDR block range, which is a subset of the VPC's CIDR block.", "You can associate route tables and Network ACLs with subnets to control traffic flow.", "They are a fundamental tool for network organization and security."], "otherOptions": "A, Regions and Availability Zones are physical infrastructure constructs, not logical network segments.\\nA security group is a firewall, not a network segment."}	0	\N	221	\N	\N
201	318	Databases	Knowledge	Cloud Architecture and Design	Which of the following database types uses a flexible, document-based data model (often using JSON-like documents) and is well-suited for applications that require a flexible schema?	[{"text": "Relational (SQL)", "isCorrect": false}, {"text": "NoSQL Document Database", "isCorrect": true}, {"text": "In-memory", "isCorrect": false}, {"text": "Data Warehouse", "isCorrect": false}]	NoSQL Document Database	Document databases are a type of NoSQL database that store data in documents, which are typically in a JSON, BSON, or XML format. This model allows for flexible schemas, meaning you don't have to define all the columns upfront, which is great for agile development and evolving applications.	{"summary": "This describes a NoSQL Document Database.", "breakdown": ["Data is stored in flexible, JSON-like documents.", "It does not require a fixed, predefined schema.", "It is horizontally scalable and good for a wide variety of modern applications.", "Popular examples include MongoDB and Amazon DynamoDB."], "otherOptions": "Relational databases use a rigid, predefined schema of tables and columns.\\nIn-memory describes where the data is stored (RAM), not the data model itself.\\nA data warehouse is a specialized database optimized for analytics, not for transactional applications."}	0	\N	222	\N	\N
202	319	Security	Application	Security	You need to give a third-party auditing firm read-only access to your cloud environment for a limited period of time. What is the MOST secure way to grant this access?	[{"text": "Create a new IAM user with a permanent password and add it to the 'administrators' group.", "isCorrect": false}, {"text": "Create a cross-account IAM role with a read-only permissions policy and an external ID. Grant the auditor's account permission to assume this role.", "isCorrect": true}, {"text": "Share the access key and secret key of an existing administrative user with the auditing firm.", "isCorrect": false}, {"text": "Create a new IAM user with read-only permissions and email the password to the auditing firm.", "isCorrect": false}]	Create a cross-account IAM role with a read-only permissions policy and an external ID. Grant the auditor's account permission to assume this role.	Using a cross-account IAM role is the standard, most secure method for granting third-party access. It provides temporary, limited credentials and avoids the need to create and manage permanent users or share long-lived keys. The external ID adds another layer of security to prevent the "confused deputy" problem.	{"summary": "A cross-account IAM role is the best practice for third-party access.", "breakdown": ["It provides temporary credentials, not permanent ones.", "Permissions are strictly defined in the role's policy (e.g., read-only).", "Access can be easily revoked by deleting the role or changing its trust policy.", "The external ID ensures that only the intended third party can assume the role."], "otherOptions": "A, C, Creating permanent users or sharing existing credentials are all insecure practices that violate the principle of least privilege and create unnecessary risk."}	0	\N	223	\N	\N
203	320	Automation	Comprehension	Deployment	Which two of the following are primary benefits of using Infrastructure as Code (IaC)? (Choose TWO)	[{"text": "It allows for consistent and repeatable environment creation.", "isCorrect": true}, {"text": "It eliminates the need for network security.", "isCorrect": false}, {"text": "It provides a version-controlled, auditable history of infrastructure changes.", "isCorrect": true}, {"text": "It reduces the cost of cloud computing by 50% or more.", "isCorrect": false}, {"text": "It removes the need for application developers to write code.", "isCorrect": false}]	A, C	The core benefits of IaC are consistency and auditability. By defining infrastructure in code, you can create identical environments every time, eliminating configuration drift. Storing this code in a version control system like Git gives you a complete history of every change made to your infrastructure.	{"summary": "IaC provides repeatability and version control for infrastructure.", "breakdown": ["Repeatability: Eliminates manual, error-prone setup processes.", "Version Control: You can see who changed what and when, and easily roll back to previous versions.", "This leads to more stable environments and faster recovery from errors."], "otherOptions": "IaC is used to *define* network security rules, not eliminate them.\\nIaC can help with cost management but does not guarantee a specific percentage of savings.\\nIaC is for infrastructure, not application code."}	1	{A,C}	224	\N	\N
204	321	Operations	Application	Operations and Support	An application is deployed in the cloud and has a Service Level Agreement (SLof 99.95% uptime. The operations team needs to be notified immediately if the application becomes unavailable. What should they configure?	[{"text": "A billing alarm that triggers when costs exceed the budget.", "isCorrect": false}, {"text": "An automated backup job that runs every hour.", "isCorrect": false}, {"text": "A monitoring system with a health check or uptime probe that sends an alert when it fails.", "isCorrect": true}, {"text": "A CI/CD pipeline to deploy new updates automatically.", "isCorrect": false}]	A monitoring system with a health check or uptime probe that sends an alert when it fails.	To meet an SLA, you must continuously monitor the availability of the application. An uptime probe or health check is an automated service that sends requests to your application endpoint at regular intervals. If the check fails for a specified period, it triggers an alert (e.g., email, SMS, PagerDuty) to notify the operations team.	{"summary": "Uptime monitoring and alerting are required to enforce SLAs.", "breakdown": ["The monitor constantly checks the application's availability.", "If the application goes down, an alert is triggered immediately.", "This allows the operations team to respond quickly to minimize downtime and meet the SLA."], "otherOptions": "A billing alarm is for cost management, not availability.\\nBackups are for data recovery, not for monitoring uptime.\\nA CI/CD pipeline is for deployments, not for monitoring."}	0	\N	225	\N	\N
205	322	Troubleshooting	Comprehension	Troubleshooting	A user is unable to log in to a web application. They are certain they are using the correct password. Which of the following is the MOST likely cause from a cloud infrastructure perspective?	[{"text": "The database server is offline.", "isCorrect": true}, {"text": "The web server has run out of disk space.", "isCorrect": false}, {"text": "The DNS records for the application are incorrect.", "isCorrect": false}, {"text": "The load balancer has failed its health check.", "isCorrect": false}]	The database server is offline.	Authentication almost always involves checking the user's credentials against a database. If the web server can't connect to the database, it can't verify the password, and the login will fail, even if the user is entering the correct credentials.	{"summary": "Authentication failure often points to a database connectivity issue.", "breakdown": ["The web server needs to communicate with the database to authenticate users.", "If the database is down or unreachable, the login process cannot be completed.", "The user-facing error might be a generic 'login failed' message."], "otherOptions": "Running out of disk space would likely cause a different error, or a total site failure.\\nIncorrect DNS records would prevent the user from reaching the application's login page in the first place.\\nIf the load balancer failed its health check on the web server, the user wouldn't even be able to load the login page."}	0	\N	226	\N	\N
206	324	Networking	Knowledge	Cloud Architecture and Design	What is the purpose of a subnet mask in IP networking?	[{"text": "To hide the IP address of a server from the public internet.", "isCorrect": false}, {"text": "To divide an IP network into two or more smaller networks (subnets).", "isCorrect": true}, {"text": "To encrypt the traffic flowing between two IP addresses.", "isCorrect": false}, {"text": "To assign a permanent IP address to a device.", "isCorrect": false}]	To divide an IP network into two or more smaller networks (subnets).	A subnet mask is used to determine which part of an IP address is the network portion and which part is the host portion. By using different subnet masks, a single large network can be divided into smaller, more manageable subnets.	{"summary": "A subnet mask divides a network into subnets.", "breakdown": ["It is a 32-bit number that masks an IP address and divides the IP address into network address and host address.", "It is used in conjunction with the IP address to determine the network and host IDs.", "For example, the subnet mask 255.255.255.0 divides the network at the last octet."], "otherOptions": "This is done by a NAT Gateway or by using private IP addresses.\\nThis is done using encryption protocols like TLS or IPsec.\\nThis is done via static IP assignment, not a subnet mask."}	0	\N	227	\N	\N
207	325	Security	Comprehension	Security	Which of the following BEST describes a "zero-day" vulnerability?	[{"text": "A vulnerability that is discovered and exploited by attackers before the software vendor is aware of it or has released a patch.", "isCorrect": true}, {"text": "A vulnerability that takes zero days to fix once it has been discovered.", "isCorrect": false}, {"text": "A type of vulnerability that only affects cloud services on their first day of release.", "isCorrect": false}, {"text": "A security audit that finds zero vulnerabilities in a system.", "isCorrect": false}]	A vulnerability that is discovered and exploited by attackers before the software vendor is aware of it or has released a patch.	A zero-day vulnerability is a security flaw that is known to attackers but not yet known to the vendor or the public. This means there is no patch available, making these vulnerabilities particularly dangerous as there is no immediate defense against an attack.	{"summary": "A zero-day is a vulnerability without a patch.", "breakdown": ["The 'zero-day' refers to the fact that the vendor has had zero days to create a fix.", "Attackers who discover these can sell them or use them in highly targeted attacks.", "Defense against zero-day attacks often relies on behavioral threat detection and intrusion prevention systems."], "otherOptions": "B, C, These are incorrect descriptions of the term."}	0	\N	228	\N	\N
208	326	Deployment	Comprehension	Cloud Architecture and Design	When migrating a workload to the cloud, the team decides to make a few optimizations to the application to take advantage of a managed database service, but they are not fully rebuilding the application. What is this migration strategy called?	[{"text": "Rehost (Lift and Shift)", "isCorrect": false}, {"text": "Replatform (Lift and Reshape)", "isCorrect": true}, {"text": "Rearchitect", "isCorrect": false}, {"text": "Retire", "isCorrect": false}]	Replatform (Lift and Reshape)	Replatforming is a migration strategy that involves making some modifications to an application to better leverage cloud capabilities, without changing the core architecture. Moving from a self-managed database to a managed database service (like Amazon RDS) is a classic example of replatforming.	{"summary": "This strategy is known as replatforming.", "breakdown": ["It is a middle ground between a simple lift-and-shift and a full re-architecting.", "It provides tangible benefits (like reduced operational overhead) with moderate effort.", "It allows you to start optimizing for the cloud without a major rewrite."], "otherOptions": "Rehosting would involve moving the database to a VM without changing to a managed service.\\nRearchitecting would involve a major rewrite of the application, for example, to a microservices architecture.\\nRetire means to decommission the application."}	0	\N	229	\N	\N
209	327	Operations - Log Management	Application	Operations and Support	A company deploys 150 Linux-based servers in the cloud. The project team is tasked with storing system logs in the cloud. Logs older than 180 days should be archived automatically. Which techniques should the project team use to create the optimal solution? (Select TWO)	[{"text": "Implement lifecycle policies to automatically move logs to cold storage after 180 days", "isCorrect": true}, {"text": "Configure centralized logging with automated log rotation and compression", "isCorrect": true}, {"text": "Store all logs in high-performance SSD storage for faster access", "isCorrect": false}, {"text": "Manually transfer logs to archive storage every 6 months", "isCorrect": false}, {"text": "Delete all logs after 180 days to save storage costs", "isCorrect": false}]	Implement lifecycle policies to automatically move logs to cold storage after 180 days, Configure centralized logging with automated log rotation and compression	The optimal solution requires lifecycle policies to automatically transition logs to cost-effective cold storage after 180 days, and centralized logging with automated rotation and compression to efficiently manage the high volume of logs from 150 servers while optimizing storage costs.	{"summary": "Optimal cloud log management strategy for large-scale deployments:", "breakdown": ["Lifecycle policies automate storage tier transitions based on age (hot  warm  cold)", "Cold storage provides cost-effective archiving for compliance and historical analysis", "Centralized logging aggregates logs from all 150 servers for unified management", "Automated log rotation prevents storage overflow and manages disk space", "Compression reduces storage requirements and associated costs", "Automated processes eliminate manual intervention and human error"], "otherOptions": "SSD storage is expensive for long-term archival of old logs\\\\nManual processes do not scale for 150 servers and introduce operational risk\\\\nDeleting logs may violate compliance requirements and eliminate valuable troubleshooting data"}	1	{"Implement lifecycle policies to automatically move logs to cold storage after 180 days","Configure centralized logging with automated log rotation and compression"}	230	\N	\N
210	328	Deployments -	Application	Cloud Deployment	A cloud engineer has deployed an ecommerce system consisting of multiple components. To reduce latency for customers, the engineer plans to deploy additional compute capacity on web front-ends. However, horizontal scaling options are too expensive. Which solution BEST meets the requirements?	[{"text": "Deploy additional VCPU resources on front-end VMs.", "isCorrect": true}, {"text": "Convert VM hosts from type 1 to type 2 hypervisor.", "isCorrect": false}, {"text": "Place the front-end VMs in an auto-scaling group.", "isCorrect": false}, {"text": "Deploy additional front-end VMs with faster processors.", "isCorrect": false}]	Deploy additional VCPU resources on front-end VMs.	The engineer should deploy additional virtualized CPU (vCPU) resources on front-end virtual machines (VMs).  A vCPU is a compute resource presented by a hypervisor to a guest OS.\n\n In this case, adding more vCPU capacity to the front-end VMs should reduce processing latency. Expanding the resources on a single node is known as vertical scaling.	{"summary": "Deploy additional VCPU resources on front-end VMs.", "breakdown": ["The engineer should not deploy additional front-end VMs with faster processors. Like auto-scaling, this is a horizontal scaling approach.", "The engineer should not convert VM hosts from type 1 to type 2 hypervisors. A hypervisor is hardware, software, and/or firmware that sits  between VMs and physical hardware and a type 1 hypervisor runs directly on hardware. A type 2 hypervisor runs on a full operating system such  as Linux or Microsoft Windows.", "The engineer should not place the front-end VMs in an auto-scaling group. This would increase performance and reduce latency. However, this  violates the requirement to not use horizontal scaling options."], "otherOptions": "presented by a hypervisor to a guest OS. In this case, adding more vCPU capacity to the front-end VMs should reduce processing latency.\\n\\nExpanding the resources on a single node is known as vertical scaling.\\n\\nThe engineer should not convert VM hosts from type 1 to type 2 hypervisors. A hypervisor is hardware, software, and/or firmware that sits\\n\\nbetween VMs and physical hardware and a type 1 hypervisor runs directly on hardware. A type 2 hypervisor runs on a full operating system such\\n\\nas Linux or Microsoft Windows.\\n\\nThe engineer should not place the front-end VMs in an auto-scaling group. This would increase performance and reduce latency. However, this\\n\\nviolates the requirement to not use horizontal scaling options.\\n\\nThe engineer should not deploy additional front-end VMs with faster processors. Like auto-scaling, this is a horizontal scaling approach."}	\N	\N	231	\N	\N
211	190	Performance - Database Query Optimization	Expert	Operations and Support	A data warehouse runs complex analytical queries that take 30+ minutes to complete. The queries scan 100TB+ datasets but only return aggregated results. Users need interactive query performance (<5 seconds). Which solution provides the BEST query acceleration?	[{"text": "Amazon Redshift with materialized views and result caching", "isCorrect": true}, {"text": "Athena with partitioned data and columnar formats", "isCorrect": false}, {"text": "Aurora with query plan caching and read replicas", "isCorrect": false}, {"text": "DynamoDB with pre-computed aggregation tables", "isCorrect": false}]	Amazon Redshift with materialized views and result caching	Redshift materialized views pre-compute complex aggregations, result caching returns identical queries instantly, and columnar storage with compression optimizes scan performance.	{"summary": "Data warehouse query acceleration:", "breakdown": ["Materialized views: Pre-computed aggregations refresh automatically", "Result caching: Identical queries return in milliseconds", "Columnar storage: Optimized for analytical query patterns", "Redshift Spectrum: Can query 100TB+ datasets efficiently"], "otherOptions": "Athena still requires scanning large datasets for complex aggregations\\nAurora optimized for OLTP, not analytical workloads\\nDynamoDB requires complete data model restructuring"}	\N	\N	232	\N	\N
1	3	Cloud Architecture - Service Models	Updated Knowledge Level	Cloud Architecture and Models	Which cloud service model provides virtualized computing resources over the internet, allowing users to rent servers, storage, and networking without purchasing physical hardware?	[{"text": "Infrastructure as a Service (IaaS)", "isCorrect": true}, {"text": "Platform as a Service (PaaS)", "isCorrect": false}, {"text": "Function as a Service (FaaS)", "isCorrect": false}, {"text": "Software as a Service (SaaS)", "isCorrect": false}]	Infrastructure as a Service (IaaS)	Updated explanation for testing purposes	{"summary": "IaaS characteristics include:", "breakdown": ["Virtualized computing resources over the internet", "Users rent infrastructure components rather than buying hardware", "Control over operating systems and applications", "Examples: AWS EC2, Azure VMs, Google Compute Engine"], "otherOptions": "SaaS delivers complete software applications\\nPaaS provides development platforms\\nFaaS provides serverless function execution"}	\N	\N	233	\N	\N
212	193	Cost Optimization - Serverless vs Container Economics	Expert	Cloud Architecture and Models	A microservices platform runs 200 services with varying traffic patterns: 50 services get constant low traffic, 100 services have predictable business-hour spikes, 50 services have unpredictable traffic. Current container costs are $300,000/month. Which architecture mix optimizes costs?	[{"text": "All services migrate to Lambda for serverless benefits", "isCorrect": false}, {"text": "Fargate for all services with auto-scaling enabled", "isCorrect": false}, {"text": "Lambda for unpredictable traffic, Fargate Spot for spiky services, ECS EC2 Reserved for constant traffic", "isCorrect": true}, {"text": "EKS with cluster auto-scaling for all services", "isCorrect": false}]	Lambda for unpredictable traffic, Fargate Spot for spiky services, ECS EC2 Reserved for constant traffic	This hybrid approach optimizes for traffic patterns: Lambda eliminates idle costs for unpredictable traffic, Fargate Spot reduces costs for spiky workloads, Reserved instances provide maximum savings for constant traffic.	{"summary": "Hybrid architecture cost optimization:", "breakdown": ["Lambda (50 services): Pay-per-request eliminates idle time costs", "Fargate Spot (100 services): 70% savings for fault-tolerant spiky workloads", "ECS Reserved (50 services): 70% savings for predictable constant traffic", "Estimated total savings: 60% reduction from current $300,000"], "otherOptions": "Lambda cold starts and execution time limits problematic for all services\\nFargate On-Demand expensive for constant traffic\\nEKS has control plane costs and doesn't optimize for traffic patterns"}	\N	\N	234	\N	\N
213	329	Security - Roles	Knowledge	Security	A cloud admin is having trouble managing permissions that have been assigned to individual users. The\nadmin needs to implement an access control model that allows users to be grouped and permissions\nassigned based on job type. Which solution BEST addresses this requirement?	[{"text": "Assign labels", "isCorrect": false}, {"text": "Set attributes", "isCorrect": false}, {"text": "Configure DAC", "isCorrect": false}, {"text": "Deploy RBAC", "isCorrect": true}]	Deploy RBAC	The cloud admin should deploy Role Based Access Control (RBAC). RBAC is designed to enhance security\nby streamlining the assignment of permissions and system privileges to users. Roles are typically defined\nbased on job descriptions and are then assigned to users with that job. For example, the Accountant role\ncould be created and then granted privileges to files and applications that accountants in an organization\nneed to access. The Accountant role can then be assigned to users in the Accounting department.	{"summary": "The cloud admin should deploy Role Based Access Control (RBAC).", "breakdown": ["The admin should not assign labels. This relates to Mandatory Access Control (MAC). MAC is considered the most secure access control method and is primarily used in government systems. The basis of the MAC model is the application of security labels, which are applied to all system resources.", "The admin should not configure Discretionary Access Control (DAC). In a DAC model, file owners or those with similar privileges can grant access to other groups or users ", "The admin should not set attributes. This describes Attribute-Based Access Control (ABAC). ABAC permits access based on a set of attributes, such as a user's name, the time of day, or a file's name."], "otherOptions": ""}	\N	\N	235	\N	\N
214	331	Cloud Architecture and Design	Knowledge	Cloud Architecture and Models	An organization has deployed an app using VMs it manages in the cloud. To extract maximum performance\nwhile minimizing costs, the organization wants to use parallel computations per core when possible. Which\nsolution BEST meets the requirements?	[{"text": "Enable the simultaneous multithreading functionality on guest VMs", "isCorrect": true}, {"text": "Add the VMs to an auto-scaling group", "isCorrect": false}, {"text": "Assign two or more vCPUs to each VM", "isCorrect": false}, {"text": "Migrate to a bare metal deployment", "isCorrect": false}]	Enable the simultaneous multithreading functionality on guest VMs	The organization should enable simultaneous multi-threading (SMT) on each guest virtual machine (VM).\nSMT is a Central Processing Unit (CPU) feature that allows a single core to perform parallel computations.\nThis should result in increased performance without requiring additional CPU cycles. SMT must be\nsupported by the CPU, the hypervisor, and the guest VAs.	{"summary": "The organization should enable simultaneous multi-threading (SMT) on each guest virtual machine (VM).", "breakdown": ["A virtualized CPU (vCPU) is a compute resource presented by a hypervisor to a guest OS. Increasing vCPU count will not facilitate parallel computations per core.", "Bare metal deployments run virtualization software directly on hardware by providing their own operating system. This alone does not facilitate parallel computations on a single core.", "An auto-scaling group is designed to provision and decommission VMs automatically, based on workload thresholds. This alone does not facilitate parallel computations on a single core.", "SMT is a Central Processing Unit (CPU) feature that allows a single core to perform parallel computations. This should result in increased performance without requiring additional CPU cycles. SMT must be supported by the CPU, the hypervisor, and the guest VAs."], "otherOptions": "A virtualized CPU (vCPU) is a compute resource presented by a hypervisor to a guest OS. Increasing vCPU count will not facilitate parallel computations per core.\\n\\nBare metal deployments run virtualization software directly on hardware by providing their own operating\\nsystem. This alone does not facilitate parallel computations on a single core.\\n\\nAn auto-scaling group is designed to provision and decommission VMs automatically, based on workload\\nthresholds. This alone does not facilitate parallel computations on a single core."}	\N	\N	236	\N	\N
215	333	Operations - Monitoring	Analysis	Operations and Support	A cloud admin must ensure that traffic can be captured, and session statistics can be analyzed and stored\nover time. Additionally, the admin must use the information to identify performance anomalies. Which\nsolution is the admin MOST LIKELY to implement?	[{"text": "Configure nodes to forward data to a network flow collector", "isCorrect": true}, {"text": "Place instances in a security group and configure traffic rules", "isCorrect": false}, {"text": "Deploy SNMP management and install agents on each node", "isCorrect": false}, {"text": "Configure a packet analyzer and perform a packet capture", "isCorrect": false}]	Configure nodes to forward data to a network flow collector	Network flows can be captured using a network flow connector. Once captured, the flows can be analyzed\nto identify traffic trends. In most implementations, network devices are configured with the Internet Protocol\n(IP) address of a flow collector - a dedicated system that collects network flow data. The collector may have\nadvanced analytical, reporting, and alerting functionality.	{"summary": "Network flows can be captured using a network flow connector.", "breakdown": ["Network flows can be captured using a network flow connector. Once captured, the flows can be analyzed to identify traffic trends.", "Network devices are configured with the Internet Protocol (IP) address of a flow collector which is a dedicated system that collects network flow data.", "The collector may have advanced analytical, reporting, and alerting functionality."], "otherOptions": "Simple Network Management Protocol (SNMP) is used to collect performance and event information from\\nnetwork devices and modify device configurations. However, SNMP is not used to capture traffic or perform\\ndetailed session analysis.\\n\\nA packet analyzer is used to capture and view the contents of network packets. While a packet analyzer\\ncould be used in this case, it is not designed to capture and store information over time."}	\N	\N	237	\N	\N
69	69	Cloud Identity Management	Application	Security	A company acquires three subsidiaries, each with different identity providers (AD, Google Workspace, Okta). They need unified cloud access for 5,000 total users while maintaining each subsidiary's existing identity system. Compliance requires MFA and privileged access management. Which identity architecture best meets these requirements?	[{"text": "Implement federated identity with SAML/OIDC, centralized MFA, and PAM solution", "isCorrect": true}, {"text": "Create cloud accounts for each subsidiary with separate identity systems", "isCorrect": false}, {"text": "Sync all identities to cloud provider's native directory service", "isCorrect": false}, {"text": "Migrate all users to a single corporate identity provider", "isCorrect": false}]	Implement federated identity with SAML/OIDC, centralized MFA, and PAM solution	Federation allows each subsidiary to maintain their identity provider while SAML/OIDC provides secure cloud access. Centralized MFA and PAM ensure consistent security controls.	{"summary": "Federated identity architecture benefits:", "breakdown": ["SAML/OIDC federation preserves existing identity investments", "Users maintain single credentials (reduced password fatigue)", "Centralized MFA policy applies regardless of source IdP", "PAM solution provides consistent privileged access controls"], "otherOptions": "Migration disrupts 5,000 users and requires retraining\\nSeparate accounts prevent unified access and compliance\\nSync creates password management and security challenges"}	\N	\N	238	\N	\N
130	147	Security	Comprehension	Security	Which of the following are valid authentication factors used in Multi-Factor Authentication (MFA)? (Choose THREE)	[{"text": "Something you know (e.g., a password)", "isCorrect": true}, {"text": "Something you have (e.g., a hardware token or mobile phone)", "isCorrect": true}, {"text": "Something you are (e.g., a fingerprint or face scan)", "isCorrect": true}, {"text": "Somewhere you are (e.g., a specific IP address)", "isCorrect": false}, {"text": "Something you do (e.g., a specific keystroke pattern)", "isCorrect": false}, {"text": "Something you create (e.g., a new user account)", "isCorrect": false}]	Something you know (e.g., a password), Something you have (e.g., a hardware token or mobile phone), Something you are (e.g., a fingerprint or face scan)	Multi-Factor Authentication is a security system that requires more than one method of authentication from independent categories of credentials to verify the user's identity for a login or other transaction. The three standard categories are knowledge, possession, and inherence.	{"summary": "MFA is based on knowledge, possession, and inherence.", "breakdown": ["Something you know: Password, PIN, security question answer.", "Something you have: Mobile phone (for SMS or authenticator app), USB security key, smart card.", "Something you are: Biometrics like a fingerprint, facial recognition, or iris scan."], "otherOptions": "Location and behavioral patterns are sometimes used as signals in adaptive authentication but are not considered one of the three core MFA factors.\\n\\nCreating an account is not an authentication factor."}	1	{"Something you know (e.g., a password)","Something you have (e.g., a hardware token or mobile phone)","Something you are (e.g., a fingerprint or face scan)"}	239	\N	\N
245	3314	Performance Monitoring	Intermediate	Cloud Operations	A cloud operations team is preparing for a major e-commerce platform migration to AWS. Before moving production workloads, they want to establish comprehensive system baselines to ensure optimal post-migration performance management. Which two activities would provide the GREATEST value for ongoing operations? (Choose TWO.)	[{"text": "Document current CPU, memory, and network utilization patterns during peak business hours", "isCorrect": true}, {"text": "Conduct vulnerability assessments on all application components", "isCorrect": false}, {"text": "Establish thresholds for automated alerting when metrics deviate from normal ranges", "isCorrect": true}, {"text": "Perform code reviews on all custom applications", "isCorrect": false}, {"text": "Test backup and restore procedures for all databases", "isCorrect": false}]	Document current CPU, memory, and network utilization patterns during peak business hours and Establish thresholds for automated alerting when metrics deviate from normal ranges	Effective baseline establishment requires capturing normal operational patterns and implementing proactive monitoring. Documenting utilization patterns during peak business hours provides the foundation for understanding typical resource consumption and identifying future performance trends. Establishing deviation thresholds enables automated detection of anomalies that could indicate performance degradation, security incidents, or capacity constraints. Together, these activities create a comprehensive performance management framework essential for maintaining service quality post-migration.	{"summary": "Baselines enable proactive performance management through pattern documentation and automated anomaly detection. Vulnerability assessments, code reviews, and backup testing are important but separate operational activities not directly related to performance baseline establishment.", "keyDomains": ["Performance Management", "Monitoring and Alerting", "Migration Planning"], "realWorldScenario": "E-commerce platforms require predictable performance during traffic spikes. Baseline metrics captured during Black Friday events become critical references for capacity planning and incident response throughout the year."}	1	{"Document current CPU, memory, and network utilization patterns during peak business hours","Establish thresholds for automated alerting when metrics deviate from normal ranges"}	6626	Analysis	Intermediate
246	3315	Network Architecture	Intermediate	Cloud Architecture	A healthcare organization is designing their cloud infrastructure to host patient management systems and testing environments. Compliance regulations require strict separation between production patient data and development activities, while the IT team wants to minimize IP address consumption and simplify network management. What network design approach would BEST meet these requirements?	[{"text": "Deploy each environment in completely separate VPCs with VPC peering", "isCorrect": false}, {"text": "Create distinct subnets for production and development within a unified VPC", "isCorrect": true}, {"text": "Use a shared subnet with security groups to control access between environments", "isCorrect": false}, {"text": "Implement separate availability zones with identical IP ranges for each environment", "isCorrect": false}]	Create distinct subnets for production and development within a unified VPC	Using separate subnets within a single VPC provides the optimal balance of isolation, IP efficiency, and management simplicity. This design achieves logical separation through subnet boundaries and security policies while maintaining centralized network administration. Each subnet can be sized appropriately for its environment's needs, optimizing IP address utilization. Network Access Control Lists (NACLs) and security groups provide layered security enforcement between environments, ensuring compliance with healthcare data separation requirements while avoiding the complexity and IP overhead of multiple VPCs.	{"summary": "Subnet-based isolation within a single VPC enables compliance-grade separation with efficient IP management and simplified operations. Separate VPCs create unnecessary complexity, shared subnets compromise isolation requirements, and identical IP ranges cause routing conflicts.", "keyDomains": ["Network Security", "Compliance Architecture", "IP Management"], "realWorldScenario": "Healthcare and financial organizations commonly use this pattern to separate production PHI/PII data from development activities while maintaining operational efficiency and meeting regulatory audit requirements."}	0	\N	6627	Application	Intermediate
240	229	Cloud Architecture - Disaster Recovery	Advanced	Cloud Architecture and Design	A development services company has an RTO of 4 hours and RPO of 30 minutes. Which disaster recovery model should they implement?	[{"text": "Cold standby with daily backups", "isCorrect": false}, {"text": "Warm standby with continuous replication", "isCorrect": true}, {"text": "Hot standby with active-active configuration", "isCorrect": false}, {"text": "Backup and restore with weekly snapshots", "isCorrect": false}]	Warm standby with continuous replication	Warm standby with continuous replication meets both the 4-hour RTO and 30-minute RPO requirements. In a warm standby configuration, core infrastructure components are already provisioned and running at reduced capacity in the DR site, allowing for relatively quick failover within the 4-hour window. Continuous replication ensures data is synchronized frequently enough to meet the 30-minute RPO, meaning minimal data loss occurs during a disaster event.	{"summary": "DR Models and RTO/RPO Alignment:", "breakdown": ["Warm standby: Core systems running, can scale up within hours", "Continuous replication meets 30-minute RPO requirement", "More cost-effective than hot standby for 4-hour RTO", "Balances recovery time with infrastructure costs", "Automated failover processes enable timely recovery"], "otherOptions": "Cold standby with daily backups cannot meet 30-minute RPO\\nHot standby is unnecessarily expensive for 4-hour RTO\\nWeekly snapshots far exceed 30-minute RPO requirement"}	\N	\N	26	\N	\N
241	230	Cloud Architecture - Disaster Recovery	Advanced	Cloud Architecture and Design	A backup strategy requires cross-region replication with a 15-minute recovery point objective. Which configuration is most suitable?	[{"text": "Asynchronous replication with hourly snapshots", "isCorrect": false}, {"text": "Synchronous cross-region replication", "isCorrect": false}, {"text": "Continuous asynchronous replication with automated failover", "isCorrect": true}, {"text": "Daily full backups with transaction logs", "isCorrect": false}]	Continuous asynchronous replication with automated failover	Continuous asynchronous replication is the optimal solution for cross-region backup with a 15-minute RPO. It continuously replicates data changes to the secondary region with minimal delay, ensuring data loss is limited to approximately 15 minutes or less. Asynchronous replication is necessary for cross-region scenarios because synchronous replication would introduce unacceptable latency over long distances. Automated failover capabilities ensure quick recovery when needed.	{"summary": "Cross-Region Replication Strategy:", "breakdown": ["Continuous replication maintains near-real-time data synchronization", "Asynchronous mode avoids latency issues over geographic distances", "Meets 15-minute RPO requirement with frequent data transfers", "Automated failover reduces RTO during disaster events", "More cost-effective than maintaining synchronous cross-region links"], "otherOptions": "Hourly snapshots exceed the 15-minute RPO requirement\\nSynchronous cross-region replication introduces high latency\\nDaily backups with transaction logs cannot meet 15-minute RPO"}	\N	\N	28	\N	\N
243	232	Virtualization	Intermediate	Cloud Architecture and Design	Your organization needs to run legacy applications that require specific hardware configurations. Which virtualization approach should you use?	[{"text": "Paravirtualization with modified guest OS", "isCorrect": false}, {"text": "Operating system-level virtualization", "isCorrect": false}, {"text": "Container-based virtualization", "isCorrect": false}, {"text": "Full virtualization with hardware emulation", "isCorrect": true}]	Full virtualization with hardware emulation	Full virtualization with hardware emulation is ideal for legacy applications requiring specific hardware configurations because it can emulate the exact hardware environment the application expects without requiring modifications to the guest OS or application. The hypervisor presents virtualized hardware that mimics the original hardware specifications, allowing legacy applications to run unmodified while believing they have direct access to the required hardware.	{"summary": "Full Virtualization for Legacy Apps:", "breakdown": ["Emulates specific hardware without modifying guest OS or applications", "Provides complete hardware abstraction layer", "Supports unmodified legacy operating systems and applications", "Can simulate deprecated or obsolete hardware", "Isolates legacy workloads from modern infrastructure"], "otherOptions": "Paravirtualization requires guest OS modifications, incompatible with legacy apps\\nOS-level virtualization shares kernel, cannot emulate specific hardware\\nContainer-based virtualization lacks hardware emulation capabilities"}	\N	\N	30	\N	\N
244	3313	Storage Management	Intermediate	Cloud Operations	A company's security team requires 90 days of log retention for compliance auditing, but the current log storage is consuming excessive disk space and causing performance issues. The operations team needs to optimize storage costs while maintaining full access to historical logs for forensic analysis. Which two approaches would BEST address both requirements?	[{"text": "Delete logs older than 30 days to free up space", "isCorrect": false}, {"text": "Implement log compression to reduce storage footprint", "isCorrect": true}, {"text": "Archive older logs to low-cost object storage", "isCorrect": true}, {"text": "Increase log rotation frequency to daily instead of weekly", "isCorrect": false}, {"text": "Store logs in memory instead of persistent storage", "isCorrect": false}]	Implement log compression to reduce storage footprint and Archive older logs to low-cost object storage	Log management requires balancing storage optimization with data retention requirements. Compression reduces the storage footprint of logs while preserving all data integrity and searchability. Archiving moves older, less frequently accessed logs to cost-effective object storage tiers while maintaining availability for compliance and forensic needs. This combination addresses both immediate storage pressure and long-term cost optimization without sacrificing data preservation requirements.	{"summary": "Compression optimizes current storage usage, while archiving provides cost-effective long-term retention. Deleting logs violates compliance requirements, memory storage is volatile and unsuitable for persistence, and increased rotation frequency doesn't address storage optimization.", "keyDomains": ["Data Lifecycle Management", "Cost Optimization", "Compliance"], "realWorldScenario": "Production environments typically use this approach: compress recent logs for active analysis, archive logs 30+ days old to S3 Glacier or equivalent, enabling sub-second search on recent data while maintaining years of historical data at low cost."}	1	{"Implement log compression to reduce storage footprint","Archive older logs to low-cost object storage"}	6625	Application	Intermediate
247	3316	Incident Response	Advanced	Security	Your security operations center detects that a zero-day vulnerability with CVSS 9.6 is being actively exploited against your customer-facing API gateway. The vendor patch requires a 4-hour service outage, but your SLA guarantees 99.9% uptime. Threat intelligence shows automated scanning for this vulnerability has increased 400% in the last 6 hours. What is your MOST appropriate immediate action?	[{"text": "Schedule patching during the next planned maintenance window in 2 weeks", "isCorrect": false}, {"text": "Immediately take the API gateway offline until patching is complete", "isCorrect": false}, {"text": "Deploy rate limiting and IP blocking rules while preparing for emergency change approval", "isCorrect": true}, {"text": "Enable additional monitoring and wait for confirmed exploitation attempts", "isCorrect": false}]	Deploy rate limiting and IP blocking rules while preparing for emergency change approval	When facing an actively exploited zero-day vulnerability, immediate risk mitigation through compensating controls is essential while coordinating emergency patching procedures. Rate limiting and IP blocking can significantly reduce attack surface and slow potential exploitation attempts. Emergency change approval processes allow for expedited patching outside normal maintenance windows when risk justifies the SLA impact. This balanced approach provides immediate protection while preparing for definitive remediation without unnecessarily prolonging exposure.	{"summary": "Critical vulnerabilities with active exploitation require immediate compensating controls followed by emergency patching procedures. Waiting for scheduled maintenance or confirmed exploitation is unacceptable risk acceptance. Complete service shutdown may be excessive when effective mitigations are available.", "keyDomains": ["Incident Response", "Risk Management", "Change Management"], "realWorldScenario": "The 2021 Log4j vulnerability required exactly this approach - immediate WAF rules and network controls while coordinating emergency patching across entire infrastructure stacks, often requiring SLA exceptions for critical security updates."}	0	\N	6628	Analysis	Advanced
250	3321	Networking	medium	2.0 Cloud Networking	A multinational enterprise operates a hybrid cloud environment connecting three on-premises data centers to multiple cloud regions via Direct Connect and ExpressRoute links. The network must maintain optimal performance during link failures and automatically reroute traffic when paths become congested. What routing approach provides the most resilient and efficient connectivity?	["Configure static routes manually defining all network paths between sites", "Implement dynamic routing protocols that automatically discover and adapt to network topology changes", "Deploy stateless address autoconfiguration for all network interfaces", "Use broadcast routing to propagate traffic across all available links"]	Implement dynamic routing protocols that automatically discover and adapt to network topology changes	Hybrid cloud networks require dynamic routing protocols that can automatically respond to changing network conditions, link failures, and traffic patterns without manual intervention, ensuring continuous connectivity and optimal path selection.	{"key_concepts": ["Border Gateway Protocol (BGP) is the standard dynamic routing protocol for hybrid cloud connectivity, enabling automatic route advertisement between on-premises networks and cloud provider edge routers. BGP exchanges routing information dynamically, allowing routers to build comprehensive network topology maps and select optimal paths based on multiple factors including path length, network policies, and link quality. Major cloud providers support BGP for Direct Connect (AWS), ExpressRoute (Azure), and Cloud Interconnect (GCP) connections.", "Automatic failover capabilities built into dynamic routing detect link failures within seconds and immediately reroute traffic through alternate paths without requiring manual configuration changes or administrator intervention. When a primary connection fails, BGP automatically withdraws affected routes and converges on backup paths, maintaining connectivity with minimal disruption. This self-healing behavior is essential for meeting high availability requirements in production environments.", "Path optimization through dynamic routing continuously evaluates multiple available routes based on metrics such as bandwidth, latency, packet loss, and administrative cost. Protocols like OSPF and BGP use sophisticated algorithms to calculate the most efficient path for each destination, automatically adjusting when network conditions change. This ensures traffic takes the fastest, most reliable route available at any given time.", "Route summarization and aggregation reduce routing table size and improve network stability by combining multiple specific routes into broader summary routes. This decreases the amount of routing information exchanged between networks, reduces router memory and CPU requirements, and minimizes the impact of network changes. Dynamic routing protocols handle summarization automatically based on configured policies.", "Multi-path routing capabilities allow dynamic protocols to distribute traffic across multiple equal-cost or weighted paths simultaneously, maximizing bandwidth utilization and providing load balancing. Equal-Cost Multi-Path (ECMP) routing spreads traffic across parallel connections, while weighted routing directs more traffic through higher-capacity links. This traffic engineering capability is impossible with static routing."], "best_practices": ["Use BGP for external routing between on-premises networks and cloud provider edge networks", "Implement OSPF or EIGRP for internal dynamic routing within on-premises data centers", "Configure BGP route filtering to prevent unauthorized route advertisements and route leaks", "Enable BFD (Bidirectional Forwarding Detection) for sub-second failure detection and faster convergence", "Use BGP communities to tag routes and implement consistent routing policies across the network", "Implement route summarization at network boundaries to reduce routing table size and improve stability", "Configure maximum path limits for ECMP to control load distribution across parallel links", "Monitor BGP session states and route advertisements to detect configuration errors and security threats", "Document routing policies and maintain network diagrams showing all dynamic routing relationships", "Test failover scenarios regularly to verify automatic rerouting operates as expected"], "protocol_comparison": {"BGP": "Best for: Inter-domain routing between autonomous systems, cloud provider connectivity. Characteristics: Slow convergence (30-90 seconds), highly scalable, policy-based routing.", "OSPF": "Best for: Internal enterprise networks, campus networks. Characteristics: Fast convergence (sub-second with BFD), hierarchical design, link-state protocol.", "EIGRP": "Best for: Cisco-centric networks, simple configuration requirements. Characteristics: Moderate convergence, hybrid protocol, vendor-specific (Cisco).", "Static_Routes": "Best for: Simple networks with few routes, backup routes, default gateways. Characteristics: No convergence (requires manual changes), minimal overhead, no automation."}, "real_world_application": "Enterprise hybrid cloud implementations typically deploy BGP as the primary routing protocol between on-premises networks and cloud providers. For example, an AWS Direct Connect configuration establishes BGP peering sessions between customer routers and AWS edge routers, automatically advertising on-premises networks (e.g., 10.0.0.0/8) to AWS and receiving AWS VPC CIDR blocks in return. When organizations have redundant connectionssuch as dual Direct Connect links or backup VPN tunnelsBGP automatically manages failover, typically achieving convergence times under 60 seconds. Advanced implementations use BGP communities and AS-PATH prepending to influence path selection, implementing traffic engineering policies like preferring Direct Connect over VPN for latency-sensitive applications or distributing egress traffic across multiple internet service providers based on destination.", "why_other_options_are_wrong": {"static_routing": "Static routes require manual configuration of every network path and cannot automatically adapt to failures or network changes. When a link fails, static routes remain in the routing table pointing to the failed path, causing traffic blackholes until an administrator manually removes the route and configures alternatives. In complex hybrid cloud environments with dozens or hundreds of networks, maintaining accurate static routes becomes operationally impossible and error-prone. Static routing also cannot provide load balancing across multiple paths or automatically optimize for changing network conditions.", "broadcast_routing": "Broadcast routing sends traffic to all nodes on a network segment simultaneously rather than selecting optimal paths to specific destinations. This approach creates massive inefficiency by flooding all links with duplicate traffic, consuming enormous bandwidth and causing network congestion. Broadcast traffic does not cross router boundaries by design, making it completely unsuitable for routing between geographically separated sites. Modern cloud networks specifically filter broadcast traffic at network edges to prevent this type of inefficient communication pattern.", "stateless_autoconfiguration": "Stateless Address Autoconfiguration (SLAAC) is an IPv6 mechanism for automatic IP address assignment to end hosts, not a routing protocol for determining network paths between sites. SLAAC allows devices to configure their own IP addresses using router advertisements but does not handle route propagation, path selection, or traffic forwarding between networks. This technology operates at the address configuration layer, not the routing layer, and cannot provide the path determination required for hybrid cloud connectivity."}}	0	\N	6633	Application	Intermediate
248	3317	Multi-Cloud Security	Advanced	Security	A global financial services firm operates critical trading applications across AWS, Azure, and Google Cloud Platform. Regulatory compliance requires identical security posture and incident response capabilities regardless of cloud provider, while maximizing each platform's unique security advantages. What architectural strategy would BEST achieve these requirements?	[{"text": "Implement provider-specific security solutions optimized for each cloud platform", "isCorrect": false}, {"text": "Standardize on a single cloud provider's security services across all environments", "isCorrect": false}, {"text": "Deploy a centralized security framework with cloud-agnostic policies and provider-specific integrations", "isCorrect": true}, {"text": "Focus security investments primarily on the highest-volume trading platform", "isCorrect": false}]	Deploy a centralized security framework with cloud-agnostic policies and provider-specific integrations	A centralized security framework enables consistent policy enforcement and compliance reporting across all cloud providers while allowing integration with each platform's native security capabilities. This approach maintains uniform security standards through cloud-agnostic identity management, encryption policies, and incident response procedures, while leveraging provider-specific advantages like AWS GuardDuty, Azure Sentinel, or Google Security Command Center. The framework ensures regulatory compliance through standardized controls and unified monitoring, regardless of underlying cloud infrastructure.	{"summary": "Multi-cloud security requires balancing consistency with flexibility. Provider-specific solutions create management complexity, single-provider standardization wastes unique capabilities, and uneven security investment creates vulnerabilities. A centralized framework with provider integrations delivers both compliance and optimization.", "keyDomains": ["Multi-Cloud Architecture", "Regulatory Compliance", "Identity Management"], "realWorldScenario": "Financial institutions commonly use this pattern with tools like HashiCorp Vault for secrets management, Splunk for unified logging, and custom automation that translates security policies into provider-specific implementations across AWS IAM, Azure AD, and GCP IAM."}	0	\N	6629	Synthesis	Expert
249	3319	Security	hard	4.0 Security	A serverless application architecture processes sensitive customer data through multiple cloud functions and microservices. Traditional security monitoring approaches are insufficient for this dynamic environment. How should you implement security monitoring for serverless architectures?	[{"text": "Apply traditional server-based monitoring to function execution environments", "isCorrect": false}, {"text": "Implement cloud-native security monitoring with function-level visibility, API monitoring, and event-driven security responses", "isCorrect": true}, {"text": "Focus only on application-level logging without infrastructure monitoring", "isCorrect": false}, {"text": "Use periodic vulnerability scans instead of continuous monitoring", "isCorrect": false}]	Implement cloud-native security monitoring with function-level visibility, API monitoring, and event-driven security responses	Serverless architectures require a fundamentally different approach to security monitoring due to their ephemeral, event-driven nature and distributed execution model.	{"key_concepts": ["Function-level visibility provides granular monitoring of individual function executions, including invocation patterns, execution duration, memory usage, and cold start frequencies. Traditional host-based monitoring cannot capture the behavior of short-lived, ephemeral functions.", "API monitoring is critical in serverless architectures where functions communicate through API gateways and service-to-service calls. This includes tracking API request patterns, authentication attempts, rate limiting violations, payload inspection, and detecting API abuse or unauthorized access attempts.", "Event-driven security responses enable automated reactions to security events in real-time. For example, if a function exhibits unusual behavior (excessive IAM permission requests, unexpected network connections, or anomalous data access patterns), security automation can immediately isolate the function, revoke temporary credentials, or trigger incident response workflows.", "Distributed tracing is essential for understanding the complete request flow across multiple functions and services. Tools like AWS X-Ray, Azure Application Insights, or Google Cloud Trace provide end-to-end visibility and help identify security issues in complex serverless chains."], "best_practices": ["Implement structured logging with correlation IDs to trace requests across multiple functions", "Enable cloud provider security services (AWS GuardDuty, Azure Security Center, GCP Security Command Center)", "Use runtime application self-protection (RASP) for serverless to detect attacks during execution", "Monitor IAM permission usage and detect over-privileged functions using least privilege analysis", "Implement automated security responses through serverless security orchestration", "Track function dependencies and third-party library vulnerabilities continuously", "Monitor data access patterns and implement anomaly detection for sensitive data operations"], "real_world_application": "In production serverless environments processing sensitive data, organizations implement comprehensive monitoring using cloud-native tools: AWS CloudWatch Logs/Metrics/Insights for Lambda, combined with AWS GuardDuty for threat detection, AWS Config for configuration monitoring, and third-party tools like Datadog, New Relic, or Snyk for enhanced visibility. These tools provide function-level metrics, detect runtime vulnerabilities, monitor IAM permission usage, track data access patterns, and enable automated security responses through integration with AWS EventBridge or similar event platforms.", "why_other_options_are_wrong": {"application_logging_only": "While application logging is important, it provides only one dimension of security visibility. Without infrastructure monitoring, you miss critical security events like IAM permission changes, network traffic patterns, resource consumption anomalies, cold start exploitation attempts, and provider-level security events.", "periodic_vulnerability_scans": "Periodic scans are insufficient for the dynamic nature of serverless applications where functions are deployed frequently, dependencies change rapidly, and security threats emerge in real-time. Serverless requires continuous monitoring because the attack surface and deployment state change constantly.", "traditional_server_monitoring": "Server-based monitoring tools expect long-lived processes with persistent runtime environments. Serverless functions are stateless, ephemeral (lasting only milliseconds to minutes), and managed by the cloud provider. Traditional agents cannot be installed, and host-level metrics are not accessible or relevant in serverless environments."}}	0	\N	6631	Application	Advanced
251	3322	Cloud Architecture - Service Models	\N	Cloud Architecture and Design	A development team wants to deploy a web application without managing the underlying operating system, runtime, or middleware. They need automatic scaling and built-in monitoring but want to maintain control over application code and configuration. Which service model BEST fits these requirements?	[{"text": "Infrastructure as a Service (IaaS)", "isCorrect": false}, {"text": "Platform as a Service (PaaS)", "isCorrect": true}, {"text": "Software as a Service (SaaS)", "isCorrect": false}, {"text": "Function as a Service (FaaS)", "isCorrect": false}]	Platform as a Service (PaaS)	PaaS provides a complete development and deployment environment with automatic scaling and monitoring, while the provider manages the OS, runtime, and middleware. Developers maintain control over their application code and configuration.	{"summary": "PaaS provides the optimal balance for this scenario:", "breakdown": ["Managed infrastructure: Provider handles OS, runtime, middleware", "Application control: Developers manage code and configuration", "Built-in scaling: Automatic resource scaling included", "Integrated monitoring: Platform includes monitoring capabilities", "No server management: Team focuses on application, not infrastructure"], "otherOptions": "IaaS requires managing OS and runtime\\nSaaS provides no code control\\nFaaS is event-driven, not suited for web applications requiring persistent state"}	0	\N	6634	Application	Intermediate
252	3323	Cloud Architecture - Disaster Recovery	\N	Cloud Architecture and Design	A financial services company requires a maximum of 2 hours of data loss and must restore operations within 4 hours after a disaster. Database backups run hourly and full system backups run every 6 hours. Which disaster recovery metrics does this scenario describe?	[{"text": "RTO = 2 hours, RPO = 4 hours", "isCorrect": false}, {"text": "RTO = 4 hours, RPO = 2 hours", "isCorrect": true}, {"text": "RTO = 6 hours, RPO = 1 hour", "isCorrect": false}, {"text": "RTO = 1 hour, RPO = 6 hours", "isCorrect": false}]	RTO = 4 hours, RPO = 2 hours	RTO (Recovery Time Objective) is the maximum acceptable downtime - 4 hours to restore operations. RPO (Recovery Point Objective) is the maximum acceptable data loss - 2 hours. The hourly backups support the 2-hour RPO requirement.	{"summary": "Understanding RTO and RPO:", "breakdown": ["RTO: Time to restore operations after disaster", "RPO: Amount of data loss that is acceptable", "In this scenario: 4-hour RTO, 2-hour RPO", "Backup frequency must support RPO target", "Hourly backups enable 2-hour RPO compliance"], "otherOptions": "Option A reverses RTO and RPO values\\nOptions C and D use incorrect backup intervals that dont match requirements"}	0	\N	6635	Analysis	Advanced
253	3324	Cloud Architecture - Storage	\N	Cloud Architecture and Design	An e-commerce company stores product images that are frequently accessed during the first 30 days, occasionally accessed for 90 days, and rarely accessed after that but must be retained for 7 years for compliance. Which storage tiering strategy optimizes costs while meeting requirements?	[{"text": "Hot tier for 30 days  Warm tier for 90 days  Cold tier for remainder", "isCorrect": true}, {"text": "Hot tier for all data with lifecycle policies", "isCorrect": false}, {"text": "Cold tier for all data to minimize costs", "isCorrect": false}, {"text": "Warm tier for 120 days  Archive tier for remainder", "isCorrect": false}]	Hot tier for 30 days  Warm tier for 90 days  Cold tier for remainder	This tiering strategy matches access patterns to appropriate storage tiers: hot (frequent access, high cost), warm (occasional access, moderate cost), cold (rare access, low cost). This approach optimizes cost while ensuring data availability matches usage patterns.	{"summary": "Storage tiering based on access patterns:", "breakdown": ["Hot tier: Frequent access, first 30 days, highest cost", "Warm tier: Occasional access, 30-120 days, moderate cost", "Cold tier: Rare access, long-term retention, lowest cost", "Automatic lifecycle transitions reduce manual management", "Cost optimization: Pay only for access level needed"], "otherOptions": "Option B: Hot tier for all data is expensive for rarely accessed content\\nOption C: Cold tier impacts performance for frequently accessed data\\nOption D: Doesnt optimize for the first 30 days of frequent access"}	0	\N	6636	Application	Intermediate
254	3325	Cloud Architecture - Networking	\N	Cloud Architecture and Design	A company manages 25 VPCs that all need to communicate with each other and with an on-premises data center. Currently using VPC peering requires 300 individual connections (N*(N-1)/2). Which networking solution simplifies this architecture while enabling centralized management?	[{"text": "Transit Gateway with hub-and-spoke topology", "isCorrect": true}, {"text": "Additional VPC peering connections", "isCorrect": false}, {"text": "VPN gateway in each VPC", "isCorrect": false}, {"text": "Direct Connect to each VPC", "isCorrect": false}]	Transit Gateway with hub-and-spoke topology	Transit Gateway acts as a central hub connecting all VPCs and on-premises networks through a single gateway. This reduces 300 peer connections to 25 attachments plus one on-premises connection, dramatically simplifying management and routing.	{"summary": "Transit Gateway benefits for complex networking:", "breakdown": ["Hub-and-spoke: Central connection point for all VPCs", "Reduced complexity: 26 connections vs 300 peer connections", "Centralized routing: Single route table for all traffic", "On-premises integration: Single connection to data center", "Scalability: Easy to add new VPCs without exponential growth"], "otherOptions": "VPC peering becomes unmanageable at scale (300 connections)\\nVPN gateways require complex mesh configuration\\nMultiple Direct Connects are expensive and complex to manage"}	0	\N	6637	Analysis	Advanced
255	3326	Cloud Architecture - Design Patterns	\N	Cloud Architecture and Design	A monolithic application is being redesigned for the cloud. The team wants to enable independent scaling, faster deployments of individual features, and use different technologies for different components. Which architectural pattern should they adopt?	[{"text": "Microservices with loosely coupled components", "isCorrect": true}, {"text": "Three-tier architecture with load balancing", "isCorrect": false}, {"text": "Serverless functions with shared database", "isCorrect": false}, {"text": "Containerized monolith with auto-scaling", "isCorrect": false}]	Microservices with loosely coupled components	Microservices architecture decomposes applications into small, independent services that can be developed, deployed, and scaled independently. Each service can use different technologies and be updated without affecting others.	{"summary": "Microservices enable modern cloud-native development:", "breakdown": ["Independent scaling: Scale services based on individual demand", "Faster deployments: Update services without full application redeployment", "Technology diversity: Each service can use optimal tech stack", "Loose coupling: Services communicate via APIs, reducing dependencies", "Resilience: Failure in one service doesnt crash entire application"], "otherOptions": "Three-tier still requires coordinated deployments\\nServerless with shared DB creates tight coupling and scaling issues\\nContainerized monolith maintains single deployment unit limitations"}	0	\N	6638	Application	Advanced
256	3327	Cloud Architecture - Containers	\N	Cloud Architecture and Design	A database container needs to persist data even when the container is destroyed and recreated. The data must survive pod failures and be accessible from any node in the Kubernetes cluster. Which storage approach should be implemented?	[{"text": "Persistent Volumes (PV) with network-attached storage", "isCorrect": true}, {"text": "EmptyDir volumes for local storage", "isCorrect": false}, {"text": "Container filesystem with regular backups", "isCorrect": false}, {"text": "HostPath volumes on each node", "isCorrect": false}]	Persistent Volumes (PV) with network-attached storage	Persistent Volumes with network-attached storage provide durable storage that survives pod failures and can be accessed from any node. PVs are independent of pod lifecycle and persist data across container restarts and rescheduling.	{"summary": "Persistent storage in Kubernetes:", "breakdown": ["Persistent Volumes: Storage that outlives container lifecycle", "Network-attached: Accessible from any node in cluster", "Independent lifecycle: Survives pod deletion and recreation", "Dynamic provisioning: Can be automatically provisioned when needed", "Data persistence: Critical for stateful applications like databases"], "otherOptions": "EmptyDir is ephemeral and deleted with pod\\nContainer filesystem data is lost when container stops\\nHostPath ties data to specific node, breaking portability"}	0	\N	6639	Application	Advanced
257	3328	Cloud Architecture - Networking	\N	Cloud Architecture and Design	A web application needs to route traffic based on URL paths: /api/* to backend servers, /images/* to image servers, and /admin/* to admin servers. Which type of load balancer provides this capability?	[{"text": "Application Load Balancer (Layer 7)", "isCorrect": true}, {"text": "Network Load Balancer (Layer 4)", "isCorrect": false}, {"text": "Gateway Load Balancer", "isCorrect": false}, {"text": "Classic Load Balancer", "isCorrect": false}]	Application Load Balancer (Layer 7)	Application Load Balancers operate at Layer 7 (application layer) and can route traffic based on content such as URL paths, HTTP headers, and query parameters. This enables intelligent routing to different backend services.	{"summary": "Layer 7 load balancing capabilities:", "breakdown": ["Content-based routing: Route by URL path, headers, methods", "Application awareness: Inspects HTTP/HTTPS requests", "Path-based routing: /api/*  API servers, /images/*  image servers", "Host-based routing: Different domains to different targets", "SSL termination: Offload encryption/decryption from backends"], "otherOptions": "Network LB operates at Layer 4, only sees IP/port\\nGateway LB is for virtual appliances, not web routing\\nClassic LB lacks advanced Layer 7 routing features"}	0	\N	6640	Application	Intermediate
258	3329	Cloud Architecture - Networking	\N	Cloud Architecture and Design	A cloud environment needs to programmatically configure network routing, create virtual networks on demand, and adjust firewall rules through APIs without touching physical hardware. What networking approach enables this capability?	[{"text": "Software-Defined Networking (SDN)", "isCorrect": true}, {"text": "Traditional VLAN configuration", "isCorrect": false}, {"text": "Static route tables", "isCorrect": false}, {"text": "Hardware-based routing", "isCorrect": false}]	Software-Defined Networking (SDN)	SDN separates the network control plane from the data plane, enabling programmatic network configuration through software APIs. This allows dynamic network creation, routing changes, and security policy updates without physical hardware changes.	{"summary": "SDN advantages in cloud environments:", "breakdown": ["Programmability: Configure networks via APIs and code", "Automation: Create/modify networks programmatically", "Agility: Rapid network changes without hardware reconfiguration", "Centralized control: Manage entire network from single controller", "Virtual networks: Create isolated networks on shared infrastructure"], "otherOptions": "VLANs require manual configuration on physical switches\\nStatic routes lack flexibility and automation\\nHardware routing requires physical changes for updates"}	0	\N	6641	Comprehension	Intermediate
259	3330	Cloud Architecture - Microservices	\N	Cloud Architecture and Design	A microservices application has services that scale up and down, changing IP addresses frequently. Services need to find and communicate with each other without hardcoded IP addresses. What mechanism solves this challenge?	[{"text": "Service discovery with DNS-based or registry-based lookup", "isCorrect": true}, {"text": "Static IP addresses with manual configuration", "isCorrect": false}, {"text": "Hardcoded endpoints in configuration files", "isCorrect": false}, {"text": "Load balancer with fixed backend pool", "isCorrect": false}]	Service discovery with DNS-based or registry-based lookup	Service discovery automatically registers services as they start and allows other services to discover them dynamically by name rather than IP address. This handles dynamic scaling and IP address changes transparently.	{"summary": "Service discovery in dynamic environments:", "breakdown": ["Dynamic registration: Services auto-register when started", "Name-based lookup: Find services by name, not IP address", "Health checking: Automatically remove failed instances", "Load distribution: Discover all instances of a service", "Decoupling: Services dont need to know instance details"], "otherOptions": "Static IPs break in auto-scaling environments\\nHardcoded endpoints require manual updates\\nFixed load balancer pools dont adapt to scaling"}	0	\N	6642	Application	Advanced
260	3331	Cloud Architecture - Virtualization	\N	Cloud Architecture and Design	A hypervisor host has 64GB of physical RAM but is running 10 VMs each allocated 8GB RAM (total 80GB allocated). Most VMs only use 40-50% of their allocated memory. What virtualization technique allows this configuration to work efficiently?	[{"text": "Memory overcommitment with ballooning and swapping", "isCorrect": true}, {"text": "Physical RAM upgrade to 80GB", "isCorrect": false}, {"text": "Reducing VM memory allocations", "isCorrect": false}, {"text": "Limiting number of VMs to 8", "isCorrect": false}]	Memory overcommitment with ballooning and swapping	Memory overcommitment allows allocating more virtual memory than physical RAM, based on the assumption that VMs wont use all allocated memory simultaneously. The hypervisor uses techniques like ballooning (reclaiming unused memory) and swapping to manage this.	{"summary": "Memory overcommitment in virtualization:", "breakdown": ["Overcommitment: Allocate more virtual than physical memory", "Statistical multiplexing: VMs dont use full allocation simultaneously", "Ballooning: Hypervisor reclaims unused VM memory", "Transparent page sharing: Deduplicate identical memory pages", "Swap: Move inactive memory to disk when needed"], "otherOptions": "Physical RAM upgrade works but is expensive and inflexible\\nReducing allocations may impact VM performance\\nLimiting VMs wastes resources if VMs arent fully utilizing memory"}	0	\N	6643	Analysis	Advanced
261	3332	Cloud Architecture - Availability	\N	Cloud Architecture and Design	An application requires 99.99% availability and must survive the failure of an entire data center. The application consists of web servers, application servers, and a database. What architectural approach ensures this level of availability?	[{"text": "Deploy across multiple availability zones with load balancing and database replication", "isCorrect": true}, {"text": "Deploy redundant servers in a single data center", "isCorrect": false}, {"text": "Use auto-scaling in one availability zone", "isCorrect": false}, {"text": "Configure backup and restore procedures", "isCorrect": false}]	Deploy across multiple availability zones with load balancing and database replication	Multi-AZ deployment ensures that if one data center fails, the application continues running in other availability zones. Load balancers distribute traffic across zones, and database replication maintains data consistency.	{"summary": "Multi-AZ architecture for high availability:", "breakdown": ["Multiple data centers: Survive complete facility failure", "Load balancing: Distribute traffic across zones", "Database replication: Keep data synchronized across zones", "Automatic failover: Redirect traffic from failed zones", "99.99% availability: Less than 1 hour downtime per year"], "otherOptions": "Single data center deployment cannot survive facility failure\\nSingle AZ auto-scaling vulnerable to zone-wide outage\\nBackup/restore addresses disaster recovery, not high availability"}	0	\N	6644	Application	Intermediate
262	3333	Cloud Architecture - Networking	\N	Cloud Architecture and Design	A video streaming company has users worldwide experiencing high latency when accessing content from the origin server in the US. Static video files and images make up 80% of traffic. What solution reduces latency and origin server load most effectively?	[{"text": "Content Delivery Network (CDN) with edge caching", "isCorrect": true}, {"text": "Multiple origin servers in each region", "isCorrect": false}, {"text": "Faster network connections to origin", "isCorrect": false}, {"text": "Compression of video files", "isCorrect": false}]	Content Delivery Network (CDN) with edge caching	CDN caches static content at edge locations near users worldwide, dramatically reducing latency and origin server load. Users retrieve content from nearby edge servers instead of the distant origin server.	{"summary": "CDN benefits for global content delivery:", "breakdown": ["Edge caching: Content cached close to users globally", "Reduced latency: Users access nearby edge servers", "Origin offload: 80% of traffic served from cache", "Global reach: Hundreds of edge locations worldwide", "Cost savings: Reduced origin bandwidth and compute"], "otherOptions": "Multiple origins are expensive and complex to synchronize\\nFaster connections dont address geographic distance latency\\nCompression helps but doesnt solve geographic latency problem"}	0	\N	6645	Application	Intermediate
263	3334	Cloud Architecture - Storage	\N	Cloud Architecture and Design	A cloud architect needs to choose between object storage and block storage. The application stores millions of images and documents, accessed via REST API, with no need for file system operations. Which storage type is most appropriate?	[{"text": "Object storage with flat namespace and metadata", "isCorrect": true}, {"text": "Block storage with file system formatting", "isCorrect": false}, {"text": "File storage with hierarchical directories", "isCorrect": false}, {"text": "Database storage with BLOB fields", "isCorrect": false}]	Object storage with flat namespace and metadata	Object storage is ideal for large-scale unstructured data accessed via APIs. It uses a flat namespace with metadata, scales massively, and provides REST API access without requiring file system operations.	{"summary": "Object storage characteristics:", "breakdown": ["Flat namespace: No directory structure needed", "REST API: HTTP-based access for web applications", "Massive scale: Handles billions of objects", "Metadata: Rich attributes for search and organization", "Cost-effective: Typically cheaper than block storage"], "otherOptions": "Block storage requires VM attachment and file system\\nFile storage implies directory structure not needed here\\nDatabase BLOB storage is expensive and complex for millions of files"}	0	\N	6646	Comprehension	Intermediate
264	3335	Cloud Architecture - Design Patterns	\N	Cloud Architecture and Design	When a customer places an order, the system needs to update inventory, send confirmation email, trigger shipping workflow, and update analyticsall concurrently. Which architectural pattern enables this parallel processing from a single event?	[{"text": "Fan-out pattern with event-driven architecture", "isCorrect": true}, {"text": "Sequential workflow processing", "isCorrect": false}, {"text": "Batch processing at scheduled intervals", "isCorrect": false}, {"text": "Synchronous API calls to each service", "isCorrect": false}]	Fan-out pattern with event-driven architecture	Fan-out pattern publishes a single event to multiple subscribers that process it concurrently. This decouples the order system from downstream processes and enables parallel execution for better performance.	{"summary": "Fan-out pattern benefits:", "breakdown": ["Parallel processing: All tasks execute simultaneously", "Decoupling: Order system independent of downstream services", "Event-driven: Single event triggers multiple subscribers", "Scalability: Each subscriber scales independently", "Reliability: Failure in one subscriber doesnt block others"], "otherOptions": "Sequential processing is slow and creates dependencies\\nBatch processing delays time-sensitive actions like email\\nSync API calls create tight coupling and timeout risks"}	0	\N	6647	Application	Advanced
265	3336	Deployment - Infrastructure as Code	\N	Cloud Deployment	A team uses Terraform to manage infrastructure across AWS, Azure, and GCP. They need to ensure all team members work with the same infrastructure state and prevent concurrent modifications from causing conflicts. What Terraform feature addresses this requirement?	[{"text": "Remote state with state locking in shared backend", "isCorrect": true}, {"text": "Local state files in version control", "isCorrect": false}, {"text": "Manual coordination through email", "isCorrect": false}, {"text": "Separate Terraform workspaces per developer", "isCorrect": false}]	Remote state with state locking in shared backend	Remote state stored in a shared backend (like S3 or Terraform Cloud) with state locking prevents multiple users from modifying infrastructure simultaneously. Locking ensures only one operation runs at a time, preventing conflicts.	{"summary": "Terraform state management best practices:", "breakdown": ["Remote state: Centralized storage accessible to all team members", "State locking: Prevents concurrent modifications", "Consistency: All team members see same infrastructure state", "Backends: S3, Azure Blob, GCS, Terraform Cloud support locking", "Collaboration: Safe multi-user infrastructure management"], "otherOptions": "Local state files cause conflicts and inconsistencies\\nManual coordination is error-prone and doesnt scale\\nSeparate workspaces create divergent environments, not shared state"}	0	\N	6648	Application	Advanced
266	3337	Deployment - Strategies	\N	Cloud Deployment	A company wants to deploy a new application version with zero downtime and ability to instantly rollback if issues occur. They maintain two identical production environments. Which deployment strategy should they use?	[{"text": "Blue-green deployment with traffic switching", "isCorrect": true}, {"text": "Rolling deployment updating instances gradually", "isCorrect": false}, {"text": "Canary deployment with gradual traffic shift", "isCorrect": false}, {"text": "In-place deployment with service restart", "isCorrect": false}]	Blue-green deployment with traffic switching	Blue-green maintains two identical environments: blue (current) and green (new version). Deploy to green, test it, then switch all traffic instantly. If problems occur, switch back to blue immediately.	{"summary": "Blue-green deployment advantages:", "breakdown": ["Zero downtime: Traffic switches instantly between environments", "Instant rollback: Switch back to previous environment immediately", "Full testing: New version tested in production environment", "Two environments: Blue (current) and green (new)", "Clean cutover: No partial deployment state"], "otherOptions": "Rolling deployment has gradual rollout but slower rollback\\nCanary is for gradual testing, not instant switchover\\nIn-place deployment causes downtime during restart"}	0	\N	6649	Application	Intermediate
267	3338	Deployment - Strategies	\N	Cloud Deployment	A company wants to test a new feature with 5% of users before full rollout, monitoring error rates and performance. If metrics look good after 24 hours, they will gradually increase to 100%. What deployment strategy fits this approach?	[{"text": "Canary deployment with progressive traffic shifting", "isCorrect": true}, {"text": "Blue-green deployment with full cutover", "isCorrect": false}, {"text": "Rolling deployment across all servers", "isCorrect": false}, {"text": "A/B testing with random assignment", "isCorrect": false}]	Canary deployment with progressive traffic shifting	Canary deployment routes a small percentage of traffic to the new version, monitors metrics, and progressively increases traffic if successful. This limits impact of potential issues while gathering real user feedback.	{"summary": "Canary deployment for risk mitigation:", "breakdown": ["Small initial rollout: Start with 5% of users", "Metrics monitoring: Track errors, performance, user feedback", "Progressive increase: Gradually expand if metrics are good", "Limited blast radius: Issues affect only small user subset", "Production validation: Real traffic tests new features"], "otherOptions": "Blue-green switches all traffic at once, no gradual increase\\nRolling deployment updates servers, not gradual traffic shift\\nA/B testing is for feature comparison, not deployment strategy"}	0	\N	6650	Application	Advanced
268	3339	Deployment - Migration	\N	Cloud Deployment	A company migrates an application to the cloud, replacing self-managed MySQL with managed database service and self-managed load balancer with cloud load balancer, but keeping application code unchanged. What migration strategy is this?	[{"text": "Replatforming (lift, tinker, and shift)", "isCorrect": true}, {"text": "Rehosting (lift and shift)", "isCorrect": false}, {"text": "Re-architecting (refactoring)", "isCorrect": false}, {"text": "Replacing (drop and shop)", "isCorrect": false}]	Replatforming (lift, tinker, and shift)	Replatforming involves making cloud optimizations like using managed services while keeping core application architecture intact. Its a middle ground between rehosting (no changes) and re-architecting (major redesign).	{"summary": "Replatforming migration characteristics:", "breakdown": ["Targeted optimizations: Replace specific components with managed services", "Application code unchanged: Core logic remains the same", "Cloud-native services: Use managed DB, load balancers, etc.", "Balance: Benefits of cloud without full redesign", "Also called: Lift, tinker, and shift"], "otherOptions": "Rehosting moves unchanged application without optimizations\\nRe-architecting redesigns application architecture significantly\\nReplacing means buying new SaaS solution instead of migrating"}	0	\N	6651	Analysis	Advanced
269	3340	Deployment - Configuration Management	\N	Cloud Deployment	A team needs to configure 100 web servers with identical software packages, configurations, and security settings. They want an agentless solution that can be version-controlled. Which tool best fits these requirements?	[{"text": "Ansible with YAML playbooks", "isCorrect": true}, {"text": "Manual SSH commands to each server", "isCorrect": false}, {"text": "Custom shell scripts run individually", "isCorrect": false}, {"text": "Server images with baked-in configuration", "isCorrect": false}]	Ansible with YAML playbooks	Ansible is agentless (uses SSH), uses human-readable YAML for configuration, ensures idempotent operations, and can be version-controlled. Playbooks define desired state and Ansible ensures servers match that state.	{"summary": "Ansible for configuration management:", "breakdown": ["Agentless: No software installation on managed servers", "YAML playbooks: Human-readable configuration files", "Idempotent: Safe to run multiple times, same result", "Version control: Playbooks stored in Git", "Scale: Manage hundreds of servers efficiently"], "otherOptions": "Manual SSH doesnt scale and is error-prone\\nCustom scripts lack idempotency and are hard to maintain\\nServer images work but are harder to update and test changes"}	0	\N	6652	Application	Intermediate
270	3341	Deployment - Infrastructure as Code	\N	Cloud Deployment	A team is writing infrastructure configuration files and needs to choose between YAML and JSON formats. Their priority is readability for humans who will maintain the code. Which format is more appropriate?	[{"text": "YAML for better readability with whitespace and comments", "isCorrect": true}, {"text": "JSON for strict syntax and machine parsing", "isCorrect": false}, {"text": "XML for hierarchical structure", "isCorrect": false}, {"text": "Plain text without structured format", "isCorrect": false}]	YAML for better readability with whitespace and comments	YAML uses indentation and whitespace for structure, supports comments, and is more human-readable than JSON. Its widely used in tools like Kubernetes, Ansible, and Docker Compose for this reason.	{"summary": "YAML advantages for infrastructure code:", "breakdown": ["Readability: Uses indentation, no brackets or commas", "Comments: Supports inline documentation", "Less verbose: More concise than JSON or XML", "Human-friendly: Easier to write and maintain", "Widely adopted: Kubernetes, Ansible, Docker Compose use YAML"], "otherOptions": "JSON is more verbose and lacks comment support\\nXML is even more verbose with closing tags\\nPlain text lacks structure for complex configurations"}	0	\N	6653	Knowledge	Beginner
271	3342	Deployment - Infrastructure as Code	\N	Cloud Deployment	Infrastructure was deployed using Terraform, but team members made manual changes through the cloud console. Now Terraform operations fail due to mismatches. What process identifies these unauthorized changes?	[{"text": "Drift detection comparing actual state to code-defined state", "isCorrect": true}, {"text": "Manual audit of all cloud resources", "isCorrect": false}, {"text": "Reviewing cloud provider bills", "isCorrect": false}, {"text": "Checking version control commit history", "isCorrect": false}]	Drift detection comparing actual state to code-defined state	Drift detection compares actual cloud resource state with the state defined in IaC code. It identifies manual changes made outside of Terraform, allowing teams to correct drift and maintain infrastructure consistency.	{"summary": "Drift detection in IaC:", "breakdown": ["State comparison: Actual infrastructure vs IaC definitions", "Identify changes: Detect manual modifications", "Maintain consistency: Ensure infrastructure matches code", "Tools: Terraform plan, drift detection features", "Remediation: Update code or revert manual changes"], "otherOptions": "Manual audit is time-consuming and error-prone\\nBills show cost changes but not configuration drift\\nCommit history shows code changes, not infrastructure drift"}	0	\N	6654	Analysis	Advanced
272	3343	Deployment - Strategies	\N	Cloud Deployment	An application runs on 20 servers behind a load balancer. The team wants to update to a new version by updating 2 servers at a time, testing, then continuing. What deployment strategy is this?	[{"text": "Rolling deployment with batch updates", "isCorrect": true}, {"text": "Blue-green deployment", "isCorrect": false}, {"text": "Canary deployment", "isCorrect": false}, {"text": "In-place deployment", "isCorrect": false}]	Rolling deployment with batch updates	Rolling deployment updates servers in batches (waves), allowing the application to remain available during the update. Each batch is updated and tested before proceeding to the next batch.	{"summary": "Rolling deployment characteristics:", "breakdown": ["Batch updates: Update servers in groups", "Continuous availability: Application stays running", "Gradual rollout: Batches updated sequentially", "Load balancer integration: Removes servers during update", "Configurable batches: 2 servers, 10%, etc."], "otherOptions": "Blue-green switches entire environments, not batches\\nCanary routes traffic percentage, not server batches\\nIn-place updates all servers simultaneously"}	0	\N	6655	Application	Intermediate
273	3344	Deployment - Resource Provisioning	\N	Cloud Deployment	A database workload requires high IOPS (50,000+), low latency (<1ms), and sustained throughput. Which storage type should be provisioned?	[{"text": "SSD-based block storage with provisioned IOPS", "isCorrect": true}, {"text": "HDD-based block storage with standard IOPS", "isCorrect": false}, {"text": "Object storage with lifecycle policies", "isCorrect": false}, {"text": "File storage with NFS protocol", "isCorrect": false}]	SSD-based block storage with provisioned IOPS	SSD provides the high IOPS and low latency required for database workloads. Provisioned IOPS ensures consistent performance by reserving dedicated throughput and IOPS capacity.	{"summary": "Storage selection for database workloads:", "breakdown": ["SSD: Required for <1ms latency and high IOPS", "Block storage: Attached to VM, supports databases", "Provisioned IOPS: Guaranteed performance levels", "Sustained throughput: Consistent performance under load", "Database optimized: Ideal for transactional workloads"], "otherOptions": "HDD too slow for database requirements (<1ms latency)\\nObject storage API-based, not suitable for databases\\nFile storage adds NFS latency, not optimal for databases"}	0	\N	6656	Application	Intermediate
274	3345	Deployment - Infrastructure as Code	\N	Cloud Deployment	A team uses Terraform to manage infrastructure. They want to track changes over time, enable rollback to previous configurations, and review changes before applying. What practice enables this?	[{"text": "Version control (Git) for infrastructure code with pull requests", "isCorrect": true}, {"text": "Manual backups of Terraform files", "isCorrect": false}, {"text": "Documentation of changes in spreadsheet", "isCorrect": false}, {"text": "Periodic snapshots of cloud resources", "isCorrect": false}]	Version control (Git) for infrastructure code with pull requests	Version control systems like Git track every change to infrastructure code, enable rollback to any previous version, and support code review through pull requests before changes are applied.	{"summary": "Version control for IaC:", "breakdown": ["Change tracking: Every modification recorded with history", "Rollback capability: Revert to any previous version", "Code review: Pull requests enable team review", "Audit trail: Who changed what and when", "Collaboration: Multiple team members work safely"], "otherOptions": "Manual backups dont provide change tracking or review\\nSpreadsheet documentation is manual and error-prone\\nResource snapshots dont capture code changes"}	0	\N	6657	Application	Intermediate
275	3346	Deployment - Cloud Models	\N	Cloud Deployment	A company keeps sensitive customer data on-premises for compliance but wants to use public cloud for web servers and application processing. What cloud deployment model describes this architecture?	[{"text": "Hybrid cloud connecting private and public infrastructure", "isCorrect": true}, {"text": "Public cloud with VPN access", "isCorrect": false}, {"text": "Private cloud with internet gateway", "isCorrect": false}, {"text": "Community cloud for regulated industries", "isCorrect": false}]	Hybrid cloud connecting private and public infrastructure	Hybrid cloud combines private infrastructure (on-premises data center) with public cloud services, connected via secure networking. This allows sensitive data on-premises while leveraging public cloud for other workloads.	{"summary": "Hybrid cloud characteristics:", "breakdown": ["Private + public: Combines both deployment models", "Data residency: Keep sensitive data on-premises", "Cloud benefits: Use public cloud for appropriate workloads", "Secure connectivity: VPN or dedicated connection", "Flexibility: Choose optimal location for each workload"], "otherOptions": "Public cloud VPN doesnt include on-premises infrastructure\\nPrivate cloud alone doesnt include public cloud services\\nCommunity cloud is shared by specific community, not hybrid"}	0	\N	6658	Application	Intermediate
276	3347	Security - Zero Trust	\N	Cloud Security	A company adopts Zero Trust security principles. What is the fundamental assumption that Zero Trust is based on?	[{"text": "Never trust, always verify - trust nothing by default inside or outside the network", "isCorrect": true}, {"text": "Trust everything inside the corporate network", "isCorrect": false}, {"text": "Trust verified users but verify devices", "isCorrect": false}, {"text": "Trust devices with security certificates", "isCorrect": false}]	Never trust, always verify - trust nothing by default inside or outside the network	Zero Trust assumes breach and verifies every request as if it originates from an untrusted network. There is no implicit trust based on network locationeverything must be explicitly verified.	{"summary": "Zero Trust security principles:", "breakdown": ["Assume breach: Treat all networks as potentially compromised", "Verify always: Authenticate and authorize every request", "Least privilege: Minimum necessary access for each resource", "Microsegmentation: Isolate resources and applications", "Continuous monitoring: Monitor and validate continuously"], "otherOptions": "Traditional security trusts internal network (perimeter model)\\nPartial trust violates Zero Trust never trust principle\\nCertificates alone dont provide sufficient verification"}	0	\N	6659	Application	Advanced
277	3348	Security - Encryption	\N	Cloud Security	A healthcare application must protect patient data both when stored in databases and when transmitted between services. Which encryption approach is required? (Choose TWO)	[{"text": "Encryption at rest for stored data", "isCorrect": true}, {"text": "Encryption in transit with TLS/SSL", "isCorrect": true}, {"text": "Hashing for data integrity only", "isCorrect": false}, {"text": "VPN for all connections", "isCorrect": false}]	Encryption at rest for stored data, Encryption in transit with TLS/SSL	Complete data protection requires both encryption at rest (protects stored data) and encryption in transit (protects data during transmission). TLS/SSL encrypts network traffic between services.	{"summary": "Comprehensive encryption strategy:", "breakdown": ["At rest: Encrypts stored data in databases, storage", "In transit: Encrypts data moving across networks", "TLS/SSL: Standard protocol for secure communication", "HIPAA compliant: Healthcare requires both types", "Defense in depth: Multiple layers of protection"], "otherOptions": "Hashing verifies integrity but doesnt encrypt data\\nVPN alone insufficient, need application-level encryption"}	1	\N	6660	Application	Intermediate
278	3349	Security - Secrets Management	\N	Cloud Security	Applications need database passwords, API keys, and certificates to function. Hardcoding these in configuration files creates security risks. What approach securely manages these sensitive values?	[{"text": "Centralized secrets management service with encryption and access control", "isCorrect": true}, {"text": "Environment variables in container images", "isCorrect": false}, {"text": "Configuration files in version control", "isCorrect": false}, {"text": "Shared folder on file server", "isCorrect": false}]	Centralized secrets management service with encryption and access control	Secrets management services (like HashiCorp Vault, AWS Secrets Manager) provide encrypted storage, access controls, audit logging, and secret rotation. Applications retrieve secrets at runtime rather than embedding them.	{"summary": "Secrets management best practices:", "breakdown": ["Centralized storage: Single source for all secrets", "Encryption: Secrets encrypted at rest and in transit", "Access control: Fine-grained permissions per application", "Audit logging: Track who accessed what secrets when", "Rotation: Automatic periodic credential rotation"], "otherOptions": "Environment variables in images expose secrets in layers\\nVersion control exposes secrets in commit history\\nFile server lacks encryption, access control, audit"}	0	\N	6661	Application	Advanced
279	3350	Security - Access Control	\N	Cloud Security	A developer needs read access to application logs for debugging but currently has full administrator access. Following security best practices, what should be changed?	[{"text": "Grant only read access to specific log resources needed", "isCorrect": true}, {"text": "Keep administrator access for convenience", "isCorrect": false}, {"text": "Grant read-only access to entire cloud account", "isCorrect": false}, {"text": "Create shared admin account for development team", "isCorrect": false}]	Grant only read access to specific log resources needed	Principle of least privilege means granting only the minimum permissions necessary to perform required tasks. Developer needs only read access to specific logs, not admin access to entire account.	{"summary": "Principle of least privilege implementation:", "breakdown": ["Minimum permissions: Only what is required for the task", "Specific resources: Access to particular logs, not everything", "Risk reduction: Limits damage from compromised credentials", "Regular review: Periodically audit and remove unused permissions", "Just-in-time: Grant elevated access only when needed"], "otherOptions": "Admin access violates least privilege and creates risk\\nFull account read access is excessive for specific logs\\nShared admin accounts impossible to audit effectively"}	0	\N	6662	Application	Intermediate
280	3351	Security - Container Security	\N	Cloud Security	A containerized application runs with privileged mode enabled, giving it root access to the host system. This violates security policies. What should be done to improve security?	[{"text": "Run containers in unprivileged mode with minimal capabilities", "isCorrect": true}, {"text": "Add more firewall rules to container", "isCorrect": false}, {"text": "Encrypt container filesystem", "isCorrect": false}, {"text": "Use stronger password for container access", "isCorrect": false}]	Run containers in unprivileged mode with minimal capabilities	Containers should run unprivileged (non-root) with only the Linux capabilities they specifically need. Privileged mode breaks container isolation and gives containers dangerous host access.	{"summary": "Container security best practices:", "breakdown": ["Unprivileged mode: Run without root/privileged access", "Minimal capabilities: Drop unnecessary Linux capabilities", "User namespaces: Map container root to non-root host user", "Read-only filesystems: Prevent unauthorized modifications", "Host isolation: Maintain container-to-host separation"], "otherOptions": "Firewall rules dont address privileged access risk\\nEncryption doesnt fix privilege escalation vulnerability\\nPasswords irrelevant to privileged container mode"}	0	\N	6663	Analysis	Advanced
281	3352	Security - Network Controls	\N	Cloud Security	Web servers need to accept HTTPS traffic from the internet but should not accept SSH connections from anywhere except a specific management subnet. What security control implements this?	[{"text": "Network security group with inbound rules for HTTPS (anywhere) and SSH (management subnet only)", "isCorrect": true}, {"text": "OS-level firewall on each web server", "isCorrect": false}, {"text": "VPN requirement for all connections", "isCorrect": false}, {"text": "Application-level authentication only", "isCorrect": false}]	Network security group with inbound rules for HTTPS (anywhere) and SSH (management subnet only)	Network security groups are stateful firewalls that control inbound and outbound traffic. Rules specify protocols, ports, and source/destination IP ranges, enabling precise access control at the network layer.	{"summary": "Network security group configuration:", "breakdown": ["Inbound rules: Control incoming traffic by protocol and source", "Stateful: Return traffic automatically allowed", "Source filtering: Allow HTTPS from anywhere, SSH from specific subnet", "Cloud-native: Integrated with cloud networking", "Defense in depth: Network-level security layer"], "otherOptions": "OS firewall requires configuration on each server\\nVPN blocks legitimate HTTPS traffic from customers\\nAuthentication doesnt prevent network-level access attempts"}	0	\N	6664	Application	Intermediate
282	3353	Security - DDoS Protection	\N	Cloud Security	A public website experiences a DDoS attack with millions of requests overwhelming servers. Which cloud service provides protection against volumetric DDoS attacks?	[{"text": "DDoS protection service with traffic scrubbing and rate limiting", "isCorrect": true}, {"text": "Larger server instances with more CPU", "isCorrect": false}, {"text": "Database read replicas", "isCorrect": false}, {"text": "Content compression", "isCorrect": false}]	DDoS protection service with traffic scrubbing and rate limiting	Cloud DDoS protection services detect malicious traffic patterns, scrub attack traffic, and rate-limit requests. They have massive capacity to absorb volumetric attacks that would overwhelm normal infrastructure.	{"summary": "DDoS protection mechanisms:", "breakdown": ["Traffic scrubbing: Filter out malicious requests", "Rate limiting: Limit requests per source", "Massive capacity: Absorb large-volume attacks", "Always-on monitoring: Detect attacks immediately", "Geo-blocking: Block traffic from attack sources"], "otherOptions": "Larger servers still overwhelmed by massive attack volume\\nRead replicas dont address incoming attack traffic\\nCompression doesnt reduce number of malicious requests"}	0	\N	6665	Application	Intermediate
283	3354	Security - Identity and Access Management	\N	Cloud Security	A company wants to ensure developers can create and manage resources in development accounts but cannot access production. What IAM approach provides this control?	[{"text": "Separate IAM policies per environment with explicit allow/deny rules", "isCorrect": true}, {"text": "Single policy with full access, rely on developers being careful", "isCorrect": false}, {"text": "Shared credentials for all environments", "isCorrect": false}, {"text": "Manual approval for each resource creation", "isCorrect": false}]	Separate IAM policies per environment with explicit allow/deny rules	IAM policies should explicitly define permissions per environment. Development policies allow resource creation in dev accounts, production policies restrict access. Explicit deny prevents privilege escalation.	{"summary": "Environment separation with IAM:", "breakdown": ["Separate policies: Different permissions per environment", "Explicit permissions: Allow specific actions in specific accounts", "Deny rules: Explicitly block production access", "Account isolation: Use separate cloud accounts per environment", "Role-based: Assign policies based on job function"], "otherOptions": "Full access with reliance on behavior is risky\\nShared credentials impossible to audit or restrict\\nManual approval doesnt scale and slows development"}	0	\N	6666	Application	Advanced
284	3355	Security - Data Protection	\N	Cloud Security	A company needs to prevent employees from accidentally or intentionally sending sensitive customer data (credit cards, social security numbers) via email or cloud storage. What security control addresses this requirement?	[{"text": "Data Loss Prevention (DLP) with content inspection and policy enforcement", "isCorrect": true}, {"text": "Firewall rules blocking external email", "isCorrect": false}, {"text": "Employee security awareness training only", "isCorrect": false}, {"text": "Encryption of all email attachments", "isCorrect": false}]	Data Loss Prevention (DLP) with content inspection and policy enforcement	DLP solutions scan content (documents, emails, uploads) for sensitive data patterns, enforce policies to block or quarantine violations, and alert security teams. DLP can detect credit card numbers, SSNs, and other sensitive patterns.	{"summary": "Data Loss Prevention capabilities:", "breakdown": ["Content inspection: Scan files, emails, uploads for sensitive data", "Pattern matching: Detect credit cards, SSNs, custom patterns", "Policy enforcement: Block, quarantine, or encrypt violations", "Alerts: Notify security team of attempts", "Multiple channels: Email, cloud storage, endpoints"], "otherOptions": "Blocking external email breaks legitimate business communication\\nTraining alone cannot prevent all intentional or accidental leaks\\nEncryption doesnt prevent data from being sent externally"}	0	\N	6667	Application	Intermediate
285	3356	Security - Compliance	\N	Cloud Security	An e-commerce company processes credit card payments and must comply with security standards for protecting cardholder data. Which compliance framework specifically applies to credit card data?	[{"text": "PCI DSS (Payment Card Industry Data Security Standard)", "isCorrect": true}, {"text": "HIPAA (Health Insurance Portability and Accountability Act)", "isCorrect": false}, {"text": "SOC 2 (Service Organization Control 2)", "isCorrect": false}, {"text": "GDPR (General Data Protection Regulation)", "isCorrect": false}]	PCI DSS (Payment Card Industry Data Security Standard)	PCI DSS is specifically designed for organizations that handle credit card information. It defines security controls for storing, processing, and transmitting cardholder data to prevent fraud and data breaches.	{"summary": "PCI DSS key requirements:", "breakdown": ["Secure network: Firewalls and network segmentation", "Cardholder data protection: Encryption and access controls", "Vulnerability management: Regular patching and testing", "Access control: Restrict access to cardholder data", "Monitoring: Log and monitor all access"], "otherOptions": "HIPAA is for healthcare data, not credit cards\\nSOC 2 is for service provider controls, not card-specific\\nGDPR is for personal data privacy, broader than payments"}	0	\N	6668	Knowledge	Intermediate
286	3357	Security - Monitoring	\N	Cloud Security	What is the key difference between an Intrusion Detection System (IDS) and an Intrusion Prevention System (IPS)?	[{"text": "IDS alerts on threats, IPS actively blocks threats", "isCorrect": true}, {"text": "IDS is hardware, IPS is software", "isCorrect": false}, {"text": "IDS monitors internal traffic, IPS monitors external", "isCorrect": false}, {"text": "IDS is cheaper, IPS is more expensive", "isCorrect": false}]	IDS alerts on threats, IPS actively blocks threats	IDS passively monitors traffic and generates alerts when suspicious activity is detected. IPS actively blocks or drops malicious traffic in real-time before it reaches targets, providing automated protection.	{"summary": "IDS vs IPS functionality:", "breakdown": ["IDS: Detect and alert on suspicious activity", "IPS: Detect and automatically block threats", "IDS: Passive monitoring, no traffic blocking", "IPS: Active prevention, inline blocking", "Both: Use signature and anomaly detection"], "otherOptions": "Both can be hardware or software implementations\\nBoth can monitor any traffic type\\nCost varies based on implementation, not type"}	0	\N	6669	Comprehension	Intermediate
287	3358	Operations - Logging	\N	Cloud Operations and Support	50 microservices generate logs stored locally on each container. When troubleshooting issues, engineers must SSH to multiple containers to find relevant logs. What approach simplifies log analysis?	[{"text": "Centralized logging with log aggregation service", "isCorrect": true}, {"text": "Increase local disk space on each container", "isCorrect": false}, {"text": "Create scripts to SSH and search each container", "isCorrect": false}, {"text": "Store logs in shared network folder", "isCorrect": false}]	Centralized logging with log aggregation service	Centralized logging collects logs from all services into a single location, enabling search, correlation, and analysis across all systems. Engineers can query all logs from one interface without SSH access.	{"summary": "Centralized logging benefits:", "breakdown": ["Single interface: Query all logs from one place", "Correlation: Connect events across multiple services", "Search: Full-text search across all applications", "Retention: Store logs longer than ephemeral containers exist", "No SSH: Access logs without server access"], "otherOptions": "More disk space doesnt address distributed log problem\\nSSH scripts dont scale and are operationally complex\\nShared folders have performance and retention issues"}	0	\N	6670	Application	Intermediate
288	3359	Operations - Observability	\N	Cloud Operations and Support	A user request flows through 8 different microservices before completing. One service is slow, causing the entire request to take 10 seconds instead of 2 seconds. What observability technique helps identify which service is slow?	[{"text": "Distributed tracing with request correlation across services", "isCorrect": true}, {"text": "Application logs from each service", "isCorrect": false}, {"text": "Server CPU and memory metrics", "isCorrect": false}, {"text": "Network latency monitoring", "isCorrect": false}]	Distributed tracing with request correlation across services	Distributed tracing tracks requests across service boundaries with unique trace IDs. It shows the complete path, timing for each service, and helps pinpoint bottlenecks in complex microservices architectures.	{"summary": "Distributed tracing capabilities:", "breakdown": ["Request path: Visualize flow through all services", "Timing breakdown: See time spent in each service", "Bottleneck identification: Find slow services quickly", "Correlation: Connect spans across service boundaries", "Service dependencies: Understand system relationships"], "otherOptions": "Logs dont show timing relationships across services\\nCPU/memory dont reveal which service is slow in the chain\\nNetwork latency doesnt identify slow service logic"}	0	\N	6671	Application	Advanced
289	3360	Operations - Auto-Scaling	\N	Cloud Operations and Support	A web application experiences traffic spikes when CPU utilization exceeds 70%. The architecture should automatically add servers when load increases and remove them when load decreases. Which scaling configuration is appropriate?	[{"text": "Auto-scaling group with CPU utilization triggers at 70% threshold", "isCorrect": true}, {"text": "Manual scaling when engineers notice slowness", "isCorrect": false}, {"text": "Scheduled scaling based on time of day", "isCorrect": false}, {"text": "Fixed number of servers for peak capacity", "isCorrect": false}]	Auto-scaling group with CPU utilization triggers at 70% threshold	Triggered auto-scaling uses metrics (CPU, memory, network, custom) to automatically add or remove resources based on actual demand. This ensures performance during spikes while optimizing costs during low traffic.	{"summary": "Triggered auto-scaling configuration:", "breakdown": ["Metric-based: Scale on CPU, memory, custom metrics", "Automatic: No manual intervention required", "Scale out: Add resources when threshold exceeded", "Scale in: Remove resources when load decreases", "Cost optimization: Pay only for capacity needed"], "otherOptions": "Manual scaling is slow and requires constant monitoring\\nScheduled scaling doesnt respond to unexpected load\\nFixed capacity wastes money during low traffic"}	0	\N	6672	Application	Intermediate
290	3361	Operations - Backup and Recovery	\N	Cloud Operations and Support	A backup strategy performs full backup Sunday, then daily backups capturing only changed blocks since the last backup. What backup type describes the daily backups?	[{"text": "Incremental backup", "isCorrect": true}, {"text": "Differential backup", "isCorrect": false}, {"text": "Full backup", "isCorrect": false}, {"text": "Synthetic backup", "isCorrect": false}]	Incremental backup	Incremental backups capture only changes since the last backup (any type). Each incremental depends on the previous backup. To restore, you need the full backup plus all incrementals since then.	{"summary": "Incremental backup characteristics:", "breakdown": ["Changes only: Backs up data modified since last backup", "Fast backup: Smallest backup size, fastest backup time", "Chain dependency: Each backup depends on previous", "Restore complexity: Requires full + all incrementals", "Space efficient: Minimizes backup storage needed"], "otherOptions": "Differential backs up changes since last FULL backup\\nFull backup copies all data every time\\nSynthetic backup is a constructed full backup"}	0	\N	6673	Comprehension	Intermediate
291	3362	Operations - Patch Management	\N	Cloud Operations and Support	A critical security vulnerability is discovered in a web server software used by 200 production servers. What is the FIRST step in the patch management process?	[{"text": "Test the patch in non-production environment first", "isCorrect": true}, {"text": "Immediately apply patch to all production servers", "isCorrect": false}, {"text": "Wait for vendor to release second version of patch", "isCorrect": false}, {"text": "Document the vulnerability in ticketing system", "isCorrect": false}]	Test the patch in non-production environment first	Even for critical patches, testing in non-production prevents patches from causing outages. Test for compatibility, verify the fix works, then deploy to production using staged rollout.	{"summary": "Patch management process:", "breakdown": ["Test first: Verify patch in non-production", "Assess impact: Understand what patch changes", "Staged rollout: Deploy to production in phases", "Backup before: Create recovery point before patching", "Monitor after: Watch for issues post-deployment"], "otherOptions": "Immediate production deployment risks outages from bad patches\\nWaiting increases vulnerability exposure window\\nDocumentation doesnt address the immediate security risk"}	0	\N	6674	Application	Intermediate
292	3363	Operations - Storage Management	\N	Cloud Operations and Support	A container stores application logs locally. When the container restarts, the logs disappear. The team needs logs to persist beyond container lifecycle for compliance. What storage type should be used?	[{"text": "Persistent volume mounted to container", "isCorrect": true}, {"text": "Ephemeral container filesystem", "isCorrect": false}, {"text": "Container image layers", "isCorrect": false}, {"text": "Environment variables for storage path", "isCorrect": false}]	Persistent volume mounted to container	Persistent volumes provide storage that survives container restarts and deletions. They mount external storage to containers, ensuring data persists independently of container lifecycle.	{"summary": "Persistent vs ephemeral storage:", "breakdown": ["Persistent: Survives container lifecycle", "Ephemeral: Deleted when container stops", "Volume mounting: External storage attached to container", "Data isolation: Volume separate from container", "Compliance: Persistent storage required for logs, data"], "otherOptions": "Ephemeral storage deleted when container stops\\nImage layers are read-only and not for runtime data\\nEnvironment variables dont provide storage"}	0	\N	6675	Application	Intermediate
293	3364	Operations - Monitoring	\N	Cloud Operations and Support	What is the primary difference between metrics and logs in observability?	[{"text": "Metrics are aggregated numerical data over time, logs are discrete event records", "isCorrect": true}, {"text": "Metrics are for developers, logs are for operations", "isCorrect": false}, {"text": "Metrics are free, logs cost money to store", "isCorrect": false}, {"text": "Metrics show errors, logs show normal operations", "isCorrect": false}]	Metrics are aggregated numerical data over time, logs are discrete event records	Metrics are time-series numerical data (CPU%, request count, latency) aggregated over intervals. Logs are individual event records with detailed context about specific occurrences. Both are essential for observability.	{"summary": "Metrics vs logs in observability:", "breakdown": ["Metrics: Numerical aggregates, trends, alerts", "Logs: Detailed event records, debugging, forensics", "Metrics: Low cardinality, efficient for dashboards", "Logs: High cardinality, detailed troubleshooting", "Complementary: Use both for complete visibility"], "otherOptions": "Both are used by developers and operations\\nBoth have storage costs at scale\\nBoth show errors and normal operations"}	0	\N	6676	Comprehension	Intermediate
294	3365	Operations - Alerting	\N	Cloud Operations and Support	An operations team receives 500+ alerts daily, most for minor issues, causing alert fatigue. Critical alerts are missed among noise. What approach reduces alert fatigue while ensuring critical issues are addressed?	[{"text": "Alert prioritization with severity levels and threshold tuning", "isCorrect": true}, {"text": "Disable all alerts except critical ones", "isCorrect": false}, {"text": "Send all alerts to email instead of paging", "isCorrect": false}, {"text": "Increase alert thresholds for everything", "isCorrect": false}]	Alert prioritization with severity levels and threshold tuning	Alert prioritization assigns severity (critical, warning, info), tunes thresholds to reduce noise, and routes alerts appropriately. Critical alerts page on-call, warnings go to tickets, info to dashboards.	{"summary": "Effective alerting strategy:", "breakdown": ["Severity levels: Critical, warning, info classification", "Threshold tuning: Adjust to reduce false positives", "Routing: Page for critical, ticket for warning", "Actionable: Alert only when action is required", "Regular review: Continuously tune based on feedback"], "otherOptions": "Disabling alerts risks missing important issues\\nEmail for everything doesnt address prioritization\\nBlanket threshold increases can hide real problems"}	0	\N	6677	Application	Advanced
295	3366	Operations - Scaling	\N	Cloud Operations and Support	What is the key difference between horizontal and vertical scaling?	[{"text": "Horizontal adds more servers, vertical increases server size", "isCorrect": true}, {"text": "Horizontal is for databases, vertical is for web servers", "isCorrect": false}, {"text": "Horizontal is cheaper, vertical is more expensive", "isCorrect": false}, {"text": "Horizontal is manual, vertical is automatic", "isCorrect": false}]	Horizontal adds more servers, vertical increases server size	Horizontal scaling (scale out) adds more instances to distribute load. Vertical scaling (scale up) increases resources (CPU, RAM) of existing instances. Horizontal is generally preferred for cloud applications.	{"summary": "Scaling approaches:", "breakdown": ["Horizontal: Add more servers/instances", "Vertical: Increase server CPU, RAM, storage", "Horizontal advantages: Better fault tolerance, no downtime", "Vertical advantages: Simpler, no code changes needed", "Cloud preference: Horizontal scaling for elasticity"], "otherOptions": "Both can be used for any application type\\nCost depends on requirements, not scaling type\\nBoth can be manual or automatic"}	0	\N	6678	Comprehension	Beginner
296	3367	Operations - Lifecycle Management	\N	Cloud Operations and Support	A project ends and 50 cloud resources (VMs, storage, databases) are no longer needed. What is the proper decommissioning process to ensure clean removal?	[{"text": "Take final backups, remove dependencies, delete resources, verify cleanup", "isCorrect": true}, {"text": "Immediately delete all resources to stop costs", "isCorrect": false}, {"text": "Keep resources running but powered off", "isCorrect": false}, {"text": "Transfer resources to another project without documentation", "isCorrect": false}]	Take final backups, remove dependencies, delete resources, verify cleanup	Proper decommissioning requires backing up necessary data, removing resource dependencies (security groups, load balancers), deleting resources in correct order, and verifying complete cleanup to avoid orphaned resources.	{"summary": "Resource decommissioning best practices:", "breakdown": ["Backup data: Preserve needed information", "Remove dependencies: Detach from load balancers, networks", "Delete order: Delete in reverse of creation order", "Verify cleanup: Ensure no orphaned resources", "Document: Record what was deleted and why"], "otherOptions": "Immediate deletion risks data loss and broken dependencies\\nPowered off resources still incur storage charges\\nUndocumented transfers cause confusion and security issues"}	0	\N	6679	Application	Intermediate
297	3368	DevOps - CI/CD	\N	DevOps Fundamentals	A development team commits code multiple times per day. They want automated testing to run on every commit and automatic deployment to staging if tests pass. What implements this workflow?	[{"text": "CI/CD pipeline with automated build, test, and deployment stages", "isCorrect": true}, {"text": "Manual code review and deployment by senior engineer", "isCorrect": false}, {"text": "Weekly deployment schedule regardless of code changes", "isCorrect": false}, {"text": "Direct commits to production without testing", "isCorrect": false}]	CI/CD pipeline with automated build, test, and deployment stages	CI/CD pipelines automate the software delivery process: Continuous Integration (build + test on every commit) and Continuous Deployment (automatic deployment when tests pass). This enables rapid, reliable releases.	{"summary": "CI/CD pipeline stages:", "breakdown": ["Build: Compile code and create artifacts", "Test: Run automated unit, integration tests", "Deploy: Automatically release to staging/production", "Monitor: Track deployment success", "Rollback: Automatic revert if issues detected"], "otherOptions": "Manual processes dont scale for multiple daily commits\\nScheduled deploys delay feedback and slow development\\nDirect production commits bypass quality gates"}	0	\N	6680	Application	Intermediate
298	3369	DevOps - Source Control	\N	DevOps Fundamentals	Developers work on multiple features simultaneously without interfering with each others code. When features are complete, they merge into the main codebase. What Git concept enables this workflow?	[{"text": "Feature branches with merge/pull requests", "isCorrect": true}, {"text": "Everyone committing directly to main branch", "isCorrect": false}, {"text": "Separate Git repositories per developer", "isCorrect": false}, {"text": "Daily code backups to shared folder", "isCorrect": false}]	Feature branches with merge/pull requests	Feature branches isolate work on specific features. Developers create branches from main, develop independently, then merge back via pull requests after review. This enables parallel development without conflicts.	{"summary": "Git branching workflow:", "breakdown": ["Feature branches: Isolated development per feature", "Parallel development: Multiple developers work simultaneously", "Pull requests: Code review before merging", "Main branch: Stable codebase for releases", "Merge: Integrate completed features"], "otherOptions": "Direct main commits create conflicts and break stability\\nSeparate repos lose ability to share code easily\\nShared folders dont provide version control benefits"}	0	\N	6681	Application	Intermediate
299	3370	DevOps - Containers	\N	DevOps Fundamentals	An application works in development but fails in production due to different library versions and configurations. What technology ensures consistency across environments?	[{"text": "Docker containers with image-based deployments", "isCorrect": true}, {"text": "Documentation of required dependencies", "isCorrect": false}, {"text": "Manual configuration management", "isCorrect": false}, {"text": "Virtual machines with different OS versions", "isCorrect": false}]	Docker containers with image-based deployments	Docker containers package application code with all dependencies, libraries, and configurations into immutable images. The same image runs identically in dev, test, and production, eliminating "works on my machine" issues.	{"summary": "Docker container benefits:", "breakdown": ["Consistency: Same image across all environments", "Isolation: Dependencies packaged with application", "Immutability: Image doesnt change once built", "Portability: Runs anywhere Docker is installed", "Fast startup: Containers start in seconds"], "otherOptions": "Documentation doesnt prevent configuration drift\\nManual configuration is error-prone and doesnt scale\\nDifferent OS VMs perpetuate environment inconsistency"}	0	\N	6682	Application	Intermediate
300	3371	DevOps - Container Orchestration	\N	DevOps Fundamentals	A team runs 50 microservices as containers across multiple servers. They need automatic scaling, service discovery, load balancing, and automatic restart of failed containers. What tool provides these capabilities?	[{"text": "Kubernetes for container orchestration", "isCorrect": true}, {"text": "Docker Compose for multi-container apps", "isCorrect": false}, {"text": "Shell scripts to manage containers", "isCorrect": false}, {"text": "Manual container deployment per server", "isCorrect": false}]	Kubernetes for container orchestration	Kubernetes orchestrates containers at scale with automatic scaling, service discovery, load balancing, self-healing, and rolling updates. It manages container placement, networking, and lifecycle across cluster nodes.	{"summary": "Kubernetes orchestration features:", "breakdown": ["Auto-scaling: Scale pods based on metrics", "Service discovery: Find services by DNS name", "Load balancing: Distribute traffic across pods", "Self-healing: Restart failed containers automatically", "Rolling updates: Deploy new versions without downtime"], "otherOptions": "Docker Compose is for single-host development, not production orchestration\\nShell scripts cant provide dynamic orchestration at scale\\nManual deployment doesnt scale to 50+ services"}	0	\N	6683	Application	Advanced
301	3372	DevOps - Infrastructure as Code	\N	DevOps Fundamentals	A team needs to provision cloud infrastructure (VMs, networks, load balancers) AND configure the operating systems and applications on those VMs. Which tools combination is most appropriate?	[{"text": "Terraform for infrastructure provisioning, Ansible for configuration management", "isCorrect": true}, {"text": "Ansible for both infrastructure and configuration", "isCorrect": false}, {"text": "Terraform for both infrastructure and configuration", "isCorrect": false}, {"text": "Manual cloud console for infrastructure, Terraform for configuration", "isCorrect": false}]	Terraform for infrastructure provisioning, Ansible for configuration management	Terraform excels at infrastructure provisioning (creating cloud resources). Ansible excels at configuration management (installing software, managing files, services). Using both plays to their strengths.	{"summary": "Terraform vs Ansible use cases:", "breakdown": ["Terraform: Provision infrastructure (VMs, networks, storage)", "Ansible: Configure systems (packages, services, files)", "Terraform state: Tracks infrastructure resources", "Ansible playbooks: Define system configuration", "Complementary: Use together for complete automation"], "otherOptions": "Ansible can provision but Terraform is more robust for infrastructure\\nTerraform can configure but Ansible is more flexible for systems\\nManual console defeats purpose of infrastructure as code"}	0	\N	6684	Analysis	Advanced
302	3373	DevOps - Architecture Patterns	\N	DevOps Fundamentals	When a file is uploaded to storage, multiple actions must occur: virus scan, thumbnail generation, metadata extraction, and database updateall triggered by the upload event. What architectural pattern best fits this requirement?	[{"text": "Event-driven architecture with publish-subscribe pattern", "isCorrect": true}, {"text": "Synchronous API calls from upload service to each processor", "isCorrect": false}, {"text": "Scheduled batch job processing all uploads hourly", "isCorrect": false}, {"text": "Monolithic application handling all steps sequentially", "isCorrect": false}]	Event-driven architecture with publish-subscribe pattern	Event-driven architecture publishes events (file uploaded) that multiple subscribers (virus scan, thumbnail, metadata) consume independently and asynchronously. This decouples services and enables parallel processing.	{"summary": "Event-driven architecture benefits:", "breakdown": ["Decoupling: Services dont directly depend on each other", "Parallel processing: All actions happen simultaneously", "Scalability: Each subscriber scales independently", "Flexibility: Easy to add new event subscribers", "Resilience: Failure in one subscriber doesnt affect others"], "otherOptions": "Sync API calls create tight coupling and sequential processing\\nBatch processing delays time-sensitive operations\\nMonolith creates single failure point and coupling"}	0	\N	6685	Application	Advanced
303	3374	DevOps - CI/CD	\N	DevOps Fundamentals	A CI/CD pipeline builds container images, compiled binaries, and deployment packages. These artifacts need to be stored, versioned, and retrieved for deployment. Where should these artifacts be stored?	[{"text": "Artifact repository (like JFrog Artifactory, Nexus, or container registry)", "isCorrect": true}, {"text": "Git repository alongside source code", "isCorrect": false}, {"text": "Developer workstation filesystem", "isCorrect": false}, {"text": "Production server file system", "isCorrect": false}]	Artifact repository (like JFrog Artifactory, Nexus, or container registry)	Artifact repositories are designed for storing, versioning, and distributing build artifacts. Container registries store container images. These provide proper artifact lifecycle management and distribution.	{"summary": "Artifact repository benefits:", "breakdown": ["Versioning: Track all artifact versions", "Storage: Optimized for binary artifacts", "Distribution: Efficient delivery to deployment targets", "Security: Scan artifacts for vulnerabilities", "Retention: Policy-based cleanup of old artifacts"], "otherOptions": "Git is for source code, not binary artifacts\\nLocal filesystems dont provide distribution or versioning\\nProduction servers arent appropriate artifact storage"}	0	\N	6686	Application	Intermediate
304	3375	DevOps - Code Quality	\N	DevOps Fundamentals	A team wants to ensure all code changes are reviewed by another developer before merging to the main branch. What Git workflow enables this peer review process?	[{"text": "Pull requests (or merge requests) with required approvals", "isCorrect": true}, {"text": "Direct commits to main branch with post-commit review", "isCorrect": false}, {"text": "Email code changes to team for review", "isCorrect": false}, {"text": "Weekly code review meetings", "isCorrect": false}]	Pull requests (or merge requests) with required approvals	Pull requests enable developers to propose changes, discuss them with teammates, get approvals, and then merge. Branch protection rules can require approvals before merging, enforcing code review.	{"summary": "Pull request workflow:", "breakdown": ["Propose changes: Create PR with feature branch", "Peer review: Team reviews code, suggests improvements", "Automated checks: Run tests and linters", "Approval: Require approvals before merge", "Merge: Integrate approved changes to main"], "otherOptions": "Post-commit review allows unreviewed code in main branch\\nEmail doesnt integrate with version control\\nWeekly meetings dont scale and delay feedback"}	0	\N	6687	Application	Intermediate
305	3376	Troubleshooting - Network	\N	Troubleshooting	A web application cannot connect to its database. The database is running and accessible from other servers. Network connectivity exists between subnets. What should be checked FIRST?	[{"text": "Security group rules and network ACLs allowing database port", "isCorrect": true}, {"text": "Database disk space and memory", "isCorrect": false}, {"text": "Web application code syntax errors", "isCorrect": false}, {"text": "DNS resolution for database hostname", "isCorrect": false}]	Security group rules and network ACLs allowing database port	When connectivity exists but specific service communication fails, security group rules and network ACLs are the most common cause. Check if database port (e.g., 3306 for MySQL) is allowed from web server subnet.	{"summary": "Network connectivity troubleshooting:", "breakdown": ["Security groups: Instance-level firewall rules", "Network ACLs: Subnet-level firewall rules", "Port requirements: Ensure database port allowed", "Source filtering: Check allowed source IPs/subnets", "Stateful vs stateless: Security groups stateful, NACLs stateless"], "otherOptions": "Database runs and is accessible from other servers, eliminating DB issues\\nCode errors would show different symptoms\\nDNS less likely if database accessible from other servers"}	0	\N	6688	Analysis	Advanced
306	3377	Troubleshooting - Performance	\N	Troubleshooting	Users in Europe report 2-second page load times while US users experience <200ms loads. The application origin is in US. What is the most likely cause and solution?	[{"text": "Geographic distance causing latency; implement CDN or regional deployment", "isCorrect": true}, {"text": "European users have slower internet connections", "isCorrect": false}, {"text": "Database queries slower for European users", "isCorrect": false}, {"text": "Web server under-provisioned", "isCorrect": false}]	Geographic distance causing latency; implement CDN or regional deployment	Geographic distance creates network latency. Light travels at finite speed, and multiple network hops between continents add latency. CDN caches content closer to users; regional deployments serve from nearby locations.	{"summary": "Geographic latency solutions:", "breakdown": ["CDN: Cache static content at edge locations globally", "Multi-region: Deploy application closer to users", "Database replication: Read replicas in user regions", "Traffic routing: Route users to nearest deployment", "Physics: Cant eliminate distance, but minimize hops"], "otherOptions": "If internet speed were issue, speeds would be slow consistently\\nDatabase queries take same time regardless of user location\\nServer provisioning wouldnt cause regional difference"}	0	\N	6689	Analysis	Advanced
307	3378	Troubleshooting - Security	\N	Troubleshooting	An application suddenly receives "Access Denied" errors when trying to read from storage, even though it worked yesterday. No code changes were deployed. What is the most likely cause?	[{"text": "IAM permissions or access policies were modified", "isCorrect": true}, {"text": "Storage service is experiencing outage", "isCorrect": false}, {"text": "Application code has bugs", "isCorrect": false}, {"text": "Network connectivity issues", "isCorrect": false}]	IAM permissions or access policies were modified	When authentication succeeds but authorization fails (Access Denied) without code changes, permissions were likely modified. Check IAM policies, bucket policies, and service permissions for recent changes.	{"summary": "Troubleshooting access denied errors:", "breakdown": ["Check IAM policies: Recent permission changes", "Review logs: CloudTrail shows policy modifications", "Verify credentials: Ensure correct keys/roles used", "Bucket policies: Check resource-level permissions", "No code change: Points to infrastructure/config change"], "otherOptions": "Outage would show different error (service unavailable)\\nCode bugs ruled out by no changes deployed\\nNetwork issues show timeout, not access denied"}	0	\N	6690	Analysis	Intermediate
308	3379	Troubleshooting - Containers	\N	Troubleshooting	A containerized application crashes with "OOMKilled" status. The pod has 512MB memory limit and the application requests 256MB. What is the likely issue and solution?	[{"text": "Application uses more than 512MB; increase memory limit or optimize application", "isCorrect": true}, {"text": "Container image is corrupted", "isCorrect": false}, {"text": "Network connectivity issues", "isCorrect": false}, {"text": "CPU throttling causing crashes", "isCorrect": false}]	Application uses more than 512MB; increase memory limit or optimize application	OOMKilled means Out Of Memory Killed - the container exceeded its memory limit and was terminated by the kernel. Solution: increase memory limit, optimize application to use less memory, or investigate memory leaks.	{"summary": "Container OOM troubleshooting:", "breakdown": ["OOMKilled: Container exceeded memory limit", "Check metrics: Review actual memory usage", "Increase limit: Raise memory if legitimately needed", "Optimize code: Fix memory leaks, reduce usage", "Monitor: Set up alerts before OOM occurs"], "otherOptions": "Corrupted image would show different error\\nNetwork issues dont cause OOM kills\\nCPU throttling slows app but doesnt cause OOM"}	0	\N	6691	Analysis	Advanced
309	3380	Troubleshooting - Configuration	\N	Troubleshooting	After deploying infrastructure changes via Terraform, some resources work while others fail with "dependency error." What is the most likely cause?	[{"text": "Resources created in wrong order or missing dependencies", "isCorrect": true}, {"text": "Cloud provider API is down", "isCorrect": false}, {"text": "Terraform version incompatibility", "isCorrect": false}, {"text": "Network firewall blocking Terraform", "isCorrect": false}]	Resources created in wrong order or missing dependencies	Dependency errors occur when Terraform tries to create resources before their dependencies exist. For example, creating a subnet before the VPC, or attaching a disk before creating the VM. Use explicit depends_on when needed.	{"summary": "Terraform dependency troubleshooting:", "breakdown": ["Creation order: Some resources must exist before others", "Explicit dependencies: Use depends_on when needed", "Implicit dependencies: Terraform infers from resource references", "Terraform plan: Shows creation order", "Error messages: Usually identify missing dependency"], "otherOptions": "API down would fail all resources, not some\\nVersion issues would fail during init/plan\\nFirewall would prevent any Terraform operations"}	0	\N	6692	Analysis	Intermediate
310	3381	Troubleshooting - DNS	\N	Troubleshooting	Users can access a website by IP address (52.44.123.45) but not by domain name (www.example.com). The website was working by domain yesterday. What service should be investigated?	[{"text": "DNS resolution and records", "isCorrect": true}, {"text": "Web server configuration", "isCorrect": false}, {"text": "Load balancer settings", "isCorrect": false}, {"text": "SSL certificate validity", "isCorrect": false}]	DNS resolution and records	When IP access works but domain name fails, DNS is the problem. DNS translates domain names to IP addresses. Check DNS records (A record), DNS server configuration, and TTL values for recent changes.	{"summary": "DNS troubleshooting steps:", "breakdown": ["Verify DNS records: Check A record points to correct IP", "Check TTL: Recent changes may not have propagated", "Test DNS resolution: Use nslookup or dig commands", "DNS server: Ensure authoritative servers responding", "Propagation: Changes can take 24-48 hours globally"], "otherOptions": "Web server works (accessible via IP)\\nLoad balancer works (IP access succeeds)\\nSSL certificate would show different error"}	0	\N	6693	Analysis	Intermediate
311	3382	Security - Incident Response	\N	Security	A cloud administrator notices unusual login attempts from multiple IP addresses targeting service accounts during off-hours. CloudTrail logs show 500+ failed authentication events in the last hour from different geographic locations. What is the MOST likely attack being attempted?	[{"text": "Distributed Denial of Service (DDoS) attack", "isCorrect": false}, {"text": "Credential stuffing attack using compromised username/password pairs", "isCorrect": true}, {"text": "SQL injection attack against the authentication database", "isCorrect": false}, {"text": "Man-in-the-middle attack intercepting login credentials", "isCorrect": false}]	Credential stuffing attack using compromised username/password pairs	Multiple IP addresses with failed authentication attempts from different geographic locations is the signature pattern of credential stuffing attacks, where attackers use automated tools to test stolen username/password combinations.	{"summary": "Attack Pattern Recognition:", "response": "Immediately enable account lockout policies, review service account security, implement MFA, and analyze compromised credentials", "breakdown": ["Credential stuffing: Automated testing of stolen credentials from multiple IPs", "DDoS: Would target availability, not authentication specifically", "SQL injection: Would target database directly, not authentication API", "MITM: Would show successful logins from unexpected locations"]}	0	\N	6694	Analysis	Intermediate
327	3398	High Availability Design	\N	Cloud Architecture	A financial services company requires their cloud application to maintain 99.99% availability with RTO of 1 hour and RPO of 15 minutes. They have users in North America and Europe. Which architecture BEST meets these requirements?	[{"text": "Single region deployment with backup to object storage", "isCorrect": false}, {"text": "Active-passive multi-region with automatic failover and cross-region replication", "isCorrect": true}, {"text": "Active-active multi-region with global load balancing", "isCorrect": false}, {"text": "Single region with multiple availability zones", "isCorrect": false}]	Active-passive multi-region with automatic failover and cross-region replication	Active-passive multi-region architecture provides automatic failover to meet the 1-hour RTO while cross-region replication ensures 15-minute RPO is achievable. This balances cost with high availability requirements.	{"summary": "Multi-Region HA Architecture:", "breakdown": ["RTO 1 hour: Automatic failover enables quick recovery", "RPO 15 minutes: Cross-region replication minimizes data loss", "Active-passive: Lower cost than active-active while meeting SLA", "Geographic distribution: Serves NA and EU users effectively"], "otherOptions": "Single region cannot meet 99.99% availability\\nActive-active is costlier than needed for these requirements\\nMulti-AZ alone cannot protect against regional failures"}	0	\N	6711	Evaluation	Advanced
328	3399	Migration Strategy	\N	Deployment	An enterprise is migrating a legacy three-tier application to the cloud. The application uses a monolithic architecture with tightly coupled components and a proprietary database. The business requires minimal downtime and wants to leverage cloud benefits quickly. Which migration strategy is MOST appropriate?	[{"text": "Rehost (lift-and-shift) to get to cloud quickly, then iteratively modernize", "isCorrect": true}, {"text": "Refactor completely to microservices before migration", "isCorrect": false}, {"text": "Rebuild the entire application as cloud-native from scratch", "isCorrect": false}, {"text": "Retain on-premises and use cloud for new features only", "isCorrect": false}]	Rehost (lift-and-shift) to get to cloud quickly, then iteratively modernize	Rehosting allows quick migration with minimal downtime while enabling iterative modernization. This balances business needs (speed, minimal disruption) with technical constraints (legacy architecture, proprietary components).	{"summary": "Migration Strategy Selection:", "breakdown": ["Rehost first: Minimize risk and downtime during initial migration", "Iterative modernization: Gradually refactor components after stable in cloud", "Quick cloud benefits: Start using cloud services while planning larger changes", "6 Rs framework: Rehost, Replatform, Refactor, Repurchase, Retire, Retain"], "otherOptions": "Complete refactoring delays migration and increases risk\\nRebuild from scratch has highest cost and longest timeline\\nRetaining on-premises misses cloud benefits entirely"}	0	\N	6712	Evaluation	Intermediate
329	3400	Cost Management	\N	Operations & Support	A development team notices their monthly cloud bill has increased 300% over three months. Analysis shows 80% of costs come from compute resources running 24/7, though actual usage metrics indicate peak load occurs only 8 hours per weekday. Which combination of strategies will MOST effectively reduce costs while maintaining performance?	[{"text": "Right-size instances and implement auto-scaling policies", "isCorrect": false}, {"text": "Use reserved instances for baseline load and spot instances for peak demand", "isCorrect": false}, {"text": "Implement auto-scaling with schedule-based policies and use reserved instances for baseline", "isCorrect": true}, {"text": "Migrate everything to serverless architecture", "isCorrect": false}]	Implement auto-scaling with schedule-based policies and use reserved instances for baseline	Schedule-based auto-scaling automatically scales down during off-hours (16 hours/day + weekends), while reserved instances provide cost savings for the baseline load that runs continuously. This addresses the specific usage pattern identified.	{"summary": "Cost Optimization Strategy:", "breakdown": ["Schedule-based scaling: Automatically reduce resources during off-hours", "Reserved instances: 40-60% savings for predictable baseline load", "Usage pattern: 8 hours weekday peak = significant scaling opportunity", "Cost impact: Potential 60-70% reduction from current 24/7 operation"], "otherOptions": "Right-sizing alone doesn't address 24/7 running\\nSpot instances unsuitable for primary workload due to interruptions\\nServerless migration too complex and may not fit application architecture"}	0	\N	6713	Synthesis	Advanced
330	3401	Zero Trust Architecture	\N	Security	A healthcare organization must implement Zero Trust security for their cloud infrastructure to meet compliance requirements. Their current architecture uses a perimeter-based security model. Which implementation approach BEST establishes Zero Trust principles?	[{"text": "Implement stronger firewall rules and add additional network segmentation", "isCorrect": false}, {"text": "Require multi-factor authentication for all users and administrators", "isCorrect": false}, {"text": "Implement identity-based access controls, encrypt all data in transit, enable continuous monitoring and verification", "isCorrect": true}, {"text": "Deploy intrusion detection systems at all network boundaries", "isCorrect": false}]	Implement identity-based access controls, encrypt all data in transit, enable continuous monitoring and verification	Zero Trust requires "never trust, always verify" approach with identity-based access (not network-based), encryption everywhere, and continuous verification. This fundamentally shifts from perimeter-based to identity-based security.	{"summary": "Zero Trust Core Principles:", "breakdown": ["Identity-based access: Verify identity/device for every access request", "Least privilege: Grant minimum necessary permissions", "Encryption everywhere: Protect data in transit and at rest", "Continuous verification: Monitor and validate all access continuously", "Assume breach: Design controls assuming network is compromised"], "otherOptions": "Stronger firewalls still rely on perimeter security\\nMFA alone is one component but not complete Zero Trust\\nIDS at boundaries maintains perimeter-based model"}	0	\N	6714	Synthesis	Advanced
331	3402	Database Design	\N	Cloud Architecture	A SaaS application experiences database performance degradation during peak hours. The relational database serves both transactional (write-heavy) and analytical (read-heavy) workloads. The database is currently a single instance at 90% CPU utilization during peak. What is the MOST effective solution to improve performance while maintaining data consistency?	[{"text": "Vertically scale the database instance to handle increased load", "isCorrect": false}, {"text": "Implement read replicas for analytical queries and route writes to primary instance", "isCorrect": true}, {"text": "Migrate to a NoSQL database for better scalability", "isCorrect": false}, {"text": "Add caching layer in front of database for all queries", "isCorrect": false}]	Implement read replicas for analytical queries and route writes to primary instance	Read replicas separate read-heavy analytical workloads from write-heavy transactional operations, reducing load on primary instance. This maintains data consistency while improving performance for both workload types.	{"summary": "Database Scaling Strategy:", "breakdown": ["Read replicas: Offload read queries to replica instances", "Write routing: Keep writes on primary for consistency", "Workload separation: Analytical vs transactional isolation", "Replication lag: Monitor for acceptable data freshness", "Horizontal scaling: Distributes load across multiple instances"], "otherOptions": "Vertical scaling has limits and higher cost\\nNoSQL migration loses ACID guarantees needed for transactions\\nCaching helps but doesn't solve underlying capacity issue"}	0	\N	6715	Analysis	Intermediate
332	3403	Disaster Recovery	\N	Operations & Support	An organization has implemented a disaster recovery plan with automated failover to a secondary region. During a recent DR drill, failover took 45 minutes instead of the 15-minute RTO target, and 30 minutes of data was lost exceeding the 5-minute RPO. What is the MOST likely root cause and solution?	[{"text": "Network bandwidth insufficient; increase inter-region bandwidth", "isCorrect": false}, {"text": "Replication configuration not properly set; implement synchronous replication with automated failover scripts", "isCorrect": true}, {"text": "Secondary region resources not pre-provisioned; change to active-active configuration", "isCorrect": false}, {"text": "Backup frequency too low; increase backup frequency to every minute", "isCorrect": false}]	Replication configuration not properly set; implement synchronous replication with automated failover scripts	The 30-minute data loss indicates asynchronous replication with long lag. The 45-minute recovery time suggests manual failover steps. Synchronous replication ensures RPO near zero, and automated failover scripts achieve RTO targets.	{"summary": "DR Configuration Issues:", "breakdown": ["Data loss problem: Asynchronous replication with lag", "Recovery time problem: Manual or incomplete automation", "Synchronous replication: Ensures minimal/zero data loss", "Automated failover: Scripts/tools execute failover quickly", "Regular testing: DR drills identify configuration gaps"], "otherOptions": "Bandwidth affects replication speed but 30min loss suggests infrequent replication\\nActive-active costlier and complex than fixing replication\\nMinute-level backups don't provide continuous replication needed"}	0	\N	6716	Analysis	Advanced
333	3404	Serverless Computing	\N	Cloud Architecture	A company wants to build a document processing system that converts uploaded PDFs to text. Processing takes 2-5 minutes per document and occurs irregularly (10-100 documents per day). Which architecture provides the MOST cost-effective solution?	[{"text": "Always-on VM instances running document processing software", "isCorrect": false}, {"text": "Containerized application with auto-scaling based on queue depth", "isCorrect": false}, {"text": "Serverless functions triggered by object storage events with message queue for processing", "isCorrect": true}, {"text": "Reserved instances running batch processing jobs hourly", "isCorrect": false}]	Serverless functions triggered by object storage events with message queue for processing	Serverless architecture is ideal for irregular, event-driven workloads. Functions execute only when documents are uploaded, processing takes minutes (within function limits), and you pay only for actual processing time.	{"summary": "Serverless Architecture Benefits:", "breakdown": ["Event-driven: Automatic trigger on document upload", "Pay-per-execution: No charges when idle between documents", "Automatic scaling: Handles 10 or 100 documents without configuration", "No infrastructure management: No servers to maintain", "Cost for this pattern: ~99% savings vs always-on"], "otherOptions": "Always-on VMs waste resources during idle periods\\nContainers still have baseline costs between processing\\nReserved instances charged hourly regardless of usage"}	0	\N	6717	Synthesis	Intermediate
334	3405	Network Security	\N	Security	A company deploys a three-tier web application in the cloud: web servers, application servers, and database servers. Security requirements mandate that database servers should only accept connections from application servers, and application servers should only accept connections from web servers. Which implementation BEST satisfies these requirements?	[{"text": "Place all servers in the same subnet and use host-based firewalls", "isCorrect": false}, {"text": "Deploy each tier in separate subnets with network ACLs and security groups restricting traffic flow", "isCorrect": true}, {"text": "Use a web application firewall to inspect all traffic between tiers", "isCorrect": false}, {"text": "Implement VPN connections between each tier", "isCorrect": false}]	Deploy each tier in separate subnets with network ACLs and security groups restricting traffic flow	Network segmentation with subnet isolation combined with security groups provides defense-in-depth. Security groups enforce instance-level rules while network ACLs provide subnet-level control, creating multiple security layers.	{"summary": "Network Segmentation Strategy:", "breakdown": ["Subnet isolation: Each tier in separate subnet", "Security groups: Stateful rules allowing only required traffic", "Network ACLs: Stateless subnet-level traffic filtering", "Least privilege: Each tier accessible only by tier above", "Defense-in-depth: Multiple layers of security controls"], "otherOptions": "Same subnet increases lateral movement risk\\nWAF inspects application layer, not network connectivity\\nVPNs unnecessary and complex for internal traffic"}	0	\N	6718	Application	Intermediate
335	3406	Capacity Planning	\N	Cloud Architecture	An e-commerce platform experiences predictable traffic patterns: baseline 1,000 req/sec, 5x surge during flash sales (announced 1 week in advance), and 10x surge during holiday season (November-December). Current infrastructure runs at 70% capacity at baseline. Which capacity planning strategy is MOST appropriate?	[{"text": "Provision for 10x capacity year-round to handle peak loads", "isCorrect": false}, {"text": "Baseline capacity for 1,500 req/sec with auto-scaling up to 10x, use reserved instances for baseline and on-demand for bursts", "isCorrect": true}, {"text": "Use only spot instances to minimize costs", "isCorrect": false}, {"text": "Provision for 5x capacity and queue excess requests during peak", "isCorrect": false}]	Baseline capacity for 1,500 req/sec with auto-scaling up to 10x, use reserved instances for baseline and on-demand for bursts	This strategy provides headroom at baseline (30%), cost savings through reserved instances for predictable load, auto-scaling for surges, and uses on-demand pricing only for actual burst periods.	{"summary": "Capacity Planning Best Practices:", "breakdown": ["30% headroom: Buffer for baseline growth and unexpected spikes", "Reserved instances: 40-60% cost savings for baseline capacity", "Auto-scaling: Automatically handles predictable and unpredictable surges", "On-demand for bursts: Pay only for actual surge usage", "Predictable patterns: Can plan and test scaling policies"], "otherOptions": "10x year-round wastes 90% of resources most of the time\\nSpot instances risk interruption during critical sales periods\\nQueueing during peak degrades user experience and loses sales"}	0	\N	6719	Evaluation	Advanced
336	3407	Monitoring & Observability	\N	Operations & Support	A microservices application experiences intermittent latency spikes. CloudWatch metrics show normal CPU and memory usage. Application logs show no errors. Users report slow page loads randomly throughout the day. What monitoring enhancement would MOST effectively identify the root cause?	[{"text": "Increase log verbosity to capture more application details", "isCorrect": false}, {"text": "Implement distributed tracing to track requests across microservices", "isCorrect": true}, {"text": "Add more CloudWatch custom metrics for each service", "isCorrect": false}, {"text": "Enable AWS X-Ray snapshots for error debugging", "isCorrect": false}]	Implement distributed tracing to track requests across microservices	Distributed tracing tracks requests across multiple microservices, revealing latency in service-to-service communication. This is essential for diagnosing intermittent issues in distributed systems where metrics and logs alone are insufficient.	{"summary": "Observability for Microservices:", "breakdown": ["Distributed tracing: Visualizes request flow across services", "Service dependencies: Identifies which service causes delays", "Latency breakdown: Shows time spent in each service", "Intermittent issues: Traces capture timing of specific slow requests", "Tools: AWS X-Ray, Jaeger, Zipkin provide tracing"], "otherOptions": "Verbose logs add noise without showing cross-service timing\\nCustom metrics show aggregates, not individual request paths\\nX-Ray snapshots for errors don't help with performance issues"}	0	\N	6720	Analysis	Advanced
337	3408	Compliance	\N	Security	A global SaaS company must comply with GDPR requiring EU customer data to remain in EU regions. They currently use a single US-based region. The application uses a relational database and object storage. Which architecture modification BEST satisfies compliance while minimizing complexity?	[{"text": "Migrate entire application to EU region", "isCorrect": false}, {"text": "Implement data classification and store EU customer data in EU region with cross-region replication for backup", "isCorrect": true}, {"text": "Encrypt all EU customer data before storing in US region", "isCorrect": false}, {"text": "Use VPN to route all EU traffic through EU-based proxy servers", "isCorrect": false}]	Implement data classification and store EU customer data in EU region with cross-region replication for backup	Data residency requirements mandate EU data storage in EU. Data classification identifies EU customer data, regional storage ensures compliance, and cross-region replication to another EU region maintains availability without violating residency requirements.	{"summary": "Data Residency Compliance:", "breakdown": ["Data classification: Tag/identify EU customer data", "Regional storage: Store EU data in EU region physically", "GDPR compliance: EU data never leaves EU geography", "Backup strategy: Replicate within EU to another EU region", "Global application: Can serve multiple regions with regional data stores"], "otherOptions": "Full migration abandons US customers and existing infrastructure\\nEncryption doesn't address data location requirements\\nVPN routing doesn't change data storage location"}	0	\N	6721	Synthesis	Advanced
338	3409	Container Orchestration	\N	Cloud Architecture	A development team deploys a Kubernetes cluster with 3 master nodes and 10 worker nodes. They deploy a stateful application requiring persistent storage and notice pods are being rescheduled to different nodes, causing data loss. What configuration is MOST appropriate for this stateful application?	[{"text": "Use Deployments with local host path volumes", "isCorrect": false}, {"text": "Implement StatefulSets with PersistentVolumeClaims and stable network identities", "isCorrect": true}, {"text": "Use DaemonSets to ensure pods run on specific nodes", "isCorrect": false}, {"text": "Configure pod anti-affinity rules to prevent rescheduling", "isCorrect": false}]	Implement StatefulSets with PersistentVolumeClaims and stable network identities	StatefulSets are designed for stateful applications, providing stable network identities, ordered deployment/scaling, and stable persistent storage. PVCs ensure storage persists across pod rescheduling.	{"summary": "Kubernetes Stateful Applications:", "breakdown": ["StatefulSet: Kubernetes controller for stateful workloads", "Stable identity: Pods get predictable names and DNS", "Persistent storage: PVCs bind to specific pods across restarts", "Ordered operations: Predictable deployment and scaling", "vs Deployment: Deployments are for stateless applications"], "otherOptions": "Deployments with host paths aren't portable and risky\\nDaemonSets run one pod per node, not for typical stateful apps\\nAnti-affinity prevents rescheduling but doesn't solve storage"}	0	\N	6722	Application	Intermediate
339	3410	API Management	\N	Cloud Architecture	A public API serves 1,000 customers with varying usage patterns. Some customers consume 100x more resources than others, impacting performance for all users. The business wants to ensure fair usage while maintaining customer satisfaction. What is the MOST effective approach?	[{"text": "Block high-usage customers to protect other users", "isCorrect": false}, {"text": "Implement rate limiting with tiered plans and burst allowances at the API gateway", "isCorrect": true}, {"text": "Increase infrastructure capacity to handle all requests", "isCorrect": false}, {"text": "Queue all requests and process them in order received", "isCorrect": false}]	Implement rate limiting with tiered plans and burst allowances at the API gateway	API gateway rate limiting protects backend services while tiered plans accommodate different customer needs. Burst allowances handle temporary spikes while maintaining overall limits, balancing fairness with customer satisfaction.	{"summary": "API Rate Limiting Strategy:", "breakdown": ["Rate limiting: Prevent resource exhaustion by limiting requests", "Tiered plans: Different limits for different customer tiers", "Burst allowances: Handle temporary traffic spikes gracefully", "API gateway: Enforces limits before reaching backend", "Fair usage: Prevents one customer from impacting others"], "otherOptions": "Blocking customers loses revenue and satisfaction\\nUnlimited capacity increases costs without solving fairness\\nQueueing adds latency and doesn't prevent resource exhaustion"}	0	\N	6723	Synthesis	Advanced
340	3411	Backup & Recovery	\N	Operations & Support	A company stores critical business data in cloud object storage. Compliance requires 7-year retention with protection against accidental deletion or malicious modification. They need to optimize storage costs while meeting compliance. Which strategy BEST meets these requirements?	[{"text": "Enable versioning and lifecycle policies to move old versions to glacier storage with MFA delete", "isCorrect": true}, {"text": "Take daily snapshots and store in separate account", "isCorrect": false}, {"text": "Replicate to multiple regions with object lock", "isCorrect": false}, {"text": "Export data monthly to tape backup", "isCorrect": false}]	Enable versioning and lifecycle policies to move old versions to glacier storage with MFA delete	Versioning protects against deletion/modification, lifecycle policies automatically move old versions to glacier for cost savings, and MFA delete prevents unauthorized deletion. This combination meets compliance and cost requirements.	{"summary": "Data Protection and Retention:", "breakdown": ["Versioning: Keeps all versions of objects, protects against deletion", "Lifecycle policies: Automatically transition old data to cheaper storage", "Glacier storage: 80-90% cheaper than standard storage for archives", "MFA delete: Requires two-factor authentication to delete", "Compliance: Meets 7-year retention and protection requirements"], "otherOptions": "Snapshots don't provide versioning for object storage\\nMulti-region replication adds cost without lifecycle optimization\\nTape backup is operationally complex and slower to restore"}	0	\N	6724	Evaluation	Advanced
341	3412	Content Delivery	\N	Cloud Architecture	A global web application serves static content (images, videos, CSS, JS) alongside dynamic content. Users in Asia report slow page loads while North American users have good performance. The application origin is in US-East. What optimization provides the MOST improvement for Asian users?	[{"text": "Deploy additional application servers in Asia region", "isCorrect": false}, {"text": "Implement a CDN with edge locations in Asia for static content", "isCorrect": true}, {"text": "Increase bandwidth between US and Asia", "isCorrect": false}, {"text": "Compress all static assets before transmission", "isCorrect": false}]	Implement a CDN with edge locations in Asia for static content	CDN caches static content at edge locations near users, dramatically reducing latency by eliminating round-trips to origin for cached content. Since most web content is static, this provides the biggest impact for the effort.	{"summary": "Global Performance Optimization:", "breakdown": ["CDN benefits: Cache static content near users globally", "Latency reduction: 200-500ms saved by avoiding origin round-trips", "Static vs dynamic: CDN handles ~80% of typical web content", "Cost effective: Pay for cache hits, cheaper than infrastructure", "Easy implementation: No application changes required"], "otherOptions": "Asia servers help dynamic content but static content is majority\\nBandwidth helps but can't overcome speed of light latency\\nCompression reduces size but doesn't solve latency distance"}	0	\N	6725	Analysis	Intermediate
342	3413	Migration Strategies	\N	Cloud Deployment	A database server that has been running for 4 years will reach end-of-life in 6 months. The vendor will stop providing security updates, but the application is critical to business operations. What is the MOST comprehensive migration strategy?	[{"text": "Continue using the database with third-party support", "isCorrect": false}, {"text": "Plan migration to supported database version with compatibility testing and rollback procedures", "isCorrect": true}, {"text": "Implement additional security controls around the existing database", "isCorrect": false}, {"text": "Purchase extended support from the vendor", "isCorrect": false}]	Plan migration to supported database version with compatibility testing and rollback procedures	A comprehensive migration strategy addresses security, compatibility, and business continuity. Planning migration to a supported version with proper testing and rollback procedures ensures minimal business disruption while maintaining security.	{"summary": "Migration Strategy Best Practices:", "breakdown": ["Thorough compatibility testing prevents production issues", "Rollback procedures provide safety net for critical systems", "Supported versions receive ongoing security updates", "Planned migration allows for proper resource allocation"], "otherOptions": "Third-party support may be expensive and limited. Additional security controls don't address fundamental EOL risks. Extended vendor support is typically very costly and temporary."}	0	\N	6726	Evaluation	Intermediate
343	3414	Change Management	\N	Operations and Support	During a failed application update, you need to rollback to the previous version. However, the database schema was modified during the update. What should have been implemented to prevent this issue?	[{"text": "Database transaction logging", "isCorrect": false}, {"text": "Forward and backward compatible schema changes with versioned rollback procedures", "isCorrect": true}, {"text": "More frequent database backups", "isCorrect": false}, {"text": "Read-only database replicas", "isCorrect": false}]	Forward and backward compatible schema changes with versioned rollback procedures	Database schema changes should be designed for backward compatibility, allowing old application code to work with new schema. Versioned rollback procedures ensure safe reversion to previous states.	{"summary": "Schema Change Best Practices:", "breakdown": ["Backward compatibility prevents application breakage during rollback", "Forward compatibility allows gradual application updates", "Versioned procedures document exact rollback steps", "Blue-green deployments can leverage compatible schemas"], "otherOptions": "Transaction logs help with data recovery but not schema rollback. Frequent backups don't solve compatibility issues. Read replicas don't address schema versioning."}	0	\N	6727	Analysis	Advanced
344	3415	Infrastructure as Code	\N	DevOps Fundamentals	Your organization needs to implement a change management process for cloud resources. Multiple teams deploy infrastructure changes weekly, sometimes causing conflicts. What is the BEST approach to prevent deployment conflicts?	[{"text": "Implement Infrastructure as Code with pull request workflows and environment isolation", "isCorrect": true}, {"text": "Create a shared calendar for deployment scheduling", "isCorrect": false}, {"text": "Require all changes to be made during maintenance windows", "isCorrect": false}, {"text": "Designate one team to handle all deployments", "isCorrect": false}]	Implement Infrastructure as Code with pull request workflows and environment isolation	IaC with PR workflows provides automated conflict detection, peer review, and version control. Environment isolation prevents conflicts between teams working on different components.	{"summary": "IaC Change Management Benefits:", "breakdown": ["Pull requests enable peer review and conflict detection", "Environment isolation prevents team interference", "Version control tracks all infrastructure changes", "Automated testing validates changes before deployment"], "otherOptions": "Shared calendars don't prevent technical conflicts. Maintenance windows limit agility. Single team creates bottlenecks and reduces organizational efficiency."}	0	\N	6728	Application	Intermediate
345	3416	Backup and Recovery	\N	Operations and Support	A financial services company requires a backup strategy for customer transaction data. They cannot afford to lose more than 5 minutes of data and must restore services within 1 hour after a disaster. Which backup approach meets these requirements?	[{"text": "Daily full backups with weekly differential backups", "isCorrect": false}, {"text": "Continuous data replication with automated failover capabilities", "isCorrect": true}, {"text": "Hourly incremental backups with manual restoration procedures", "isCorrect": false}, {"text": "Real-time mirroring to a single backup location", "isCorrect": false}]	Continuous data replication with automated failover capabilities	RPO of 5 minutes and RTO of 1 hour require continuous replication and automated failover. This ensures minimal data loss and rapid recovery without manual intervention delays.	{"summary": "High-Availability Requirements:", "breakdown": ["5-minute RPO requires near real-time data protection", "1-hour RTO demands automated recovery processes", "Continuous replication minimizes data loss", "Automated failover eliminates manual recovery delays"], "otherOptions": "Daily/weekly backups exceed 5-minute RPO. Hourly backups with manual processes exceed 1-hour RTO. Single location creates single point of failure."}	0	\N	6729	Application	Advanced
346	3417	Compliance and Governance	\N	Security	A healthcare organization processes patient records that must be retained for 7 years due to regulatory requirements. They need to balance compliance, cost, and recovery time. What is the OPTIMAL backup strategy?	[{"text": "Keep all backups in hot storage for fastest access", "isCorrect": false}, {"text": "Implement tiered storage with recent backups in hot storage and older backups in archive storage", "isCorrect": true}, {"text": "Store all backups in archive storage to minimize costs", "isCorrect": false}, {"text": "Use only incremental backups to reduce storage requirements", "isCorrect": false}]	Implement tiered storage with recent backups in hot storage and older backups in archive storage	Tiered storage balances all three requirements: recent data in hot storage for fast access, older data in archive storage for cost efficiency, and all data retained for compliance.	{"summary": "Tiered Storage Strategy:", "breakdown": ["Hot storage provides fast access to recent critical data", "Archive storage reduces long-term retention costs", "Automated lifecycle policies manage transitions", "Meets 7-year retention requirements cost-effectively"], "otherOptions": "All hot storage is unnecessarily expensive for old data. All archive storage slows recent data access. Incremental-only backups create complex restoration chains."}	0	\N	6730	Synthesis	Advanced
347	3418	Disaster Recovery	\N	Operations and Support	During a disaster recovery test, your team discovers that the RTO of 4 hours cannot be met due to slow data restoration from backup storage. Current RPO requirements are 1 hour. What should you implement to meet the RTO requirement?	[{"text": "Implement faster storage tiers and parallel restoration processes", "isCorrect": true}, {"text": "Increase backup frequency to reduce RPO", "isCorrect": false}, {"text": "Reduce the amount of data being backed up", "isCorrect": false}, {"text": "Extend the RTO requirement to match current capabilities", "isCorrect": false}]	Implement faster storage tiers and parallel restoration processes	The issue is restoration speed, not data loss. Faster storage and parallel restoration directly address the RTO bottleneck without compromising RPO requirements.	{"summary": "RTO Optimization Strategies:", "breakdown": ["Faster storage reduces data transfer time", "Parallel restoration utilizes multiple resources simultaneously", "Hot standby systems eliminate restoration time", "Network bandwidth optimization accelerates transfers"], "otherOptions": "Increasing backup frequency affects RPO, not RTO. Reducing backup data may violate business requirements. Extending RTO doesn't solve the underlying performance issue."}	0	\N	6731	Analysis	Intermediate
348	3419	High Availability Design	\N	Cloud Architecture	A company operates in multiple geographic regions and needs to ensure data availability during regional disasters. Their application requires 99.99% availability. Which geographic distribution strategy is MOST appropriate?	[{"text": "Single data center with local backups", "isCorrect": false}, {"text": "Primary data center with backup data center in the same region", "isCorrect": false}, {"text": "Multi-region deployment with synchronous replication between regions", "isCorrect": true}, {"text": "Cloud-based backup with manual failover procedures", "isCorrect": false}]	Multi-region deployment with synchronous replication between regions	99.99% availability requires protection against regional disasters. Multi-region deployment with synchronous replication ensures immediate failover capability and data consistency.	{"summary": "High Availability Design:", "breakdown": ["Multi-region protects against regional disasters", "Synchronous replication ensures data consistency", "Automated failover meets 99.99% uptime requirements", "Geographic diversity minimizes correlated failures"], "otherOptions": "Single data center has regional disaster risk. Same-region backup doesn't protect against regional disasters. Manual failover is too slow for 99.99% availability."}	0	\N	6732	Evaluation	Advanced
349	3420	Backup Validation	\N	Operations and Support	After implementing a new backup solution, you need to verify that backups can be successfully restored and that the recovered data maintains integrity. What is the MOST comprehensive testing approach?	[{"text": "Monthly restore tests of random backup files", "isCorrect": false}, {"text": "Annual disaster recovery exercises", "isCorrect": false}, {"text": "Automated backup verification with periodic full disaster recovery testing in isolated environments", "isCorrect": true}, {"text": "Automated backup completion notifications", "isCorrect": false}]	Automated backup verification with periodic full disaster recovery testing in isolated environments	Comprehensive testing requires both automated verification for frequent validation and periodic full DR tests in isolated environments to validate complete recovery procedures without production risk.	{"summary": "Comprehensive Backup Testing:", "breakdown": ["Automated verification provides continuous validation", "Isolated environments prevent production impact", "Full DR tests validate complete procedures", "Periodic testing catches configuration drift"], "otherOptions": "Monthly tests may miss critical issues. Annual testing is too infrequent. Completion notifications don't verify data integrity or restore capability."}	0	\N	6733	Synthesis	Advanced
350	3421	Distributed Systems Monitoring	\N	Operations and Support	A microservices application is experiencing intermittent response time issues. Users report slow performance, but individual service metrics appear normal. What observability approach will BEST help identify the root cause?	[{"text": "Increase logging verbosity for all services", "isCorrect": false}, {"text": "Implement distributed tracing to track requests across service boundaries", "isCorrect": true}, {"text": "Monitor only CPU and memory utilization for each service", "isCorrect": false}, {"text": "Add more alerting rules for individual services", "isCorrect": false}]	Implement distributed tracing to track requests across service boundaries	Distributed tracing tracks individual requests as they flow through multiple services, revealing latency accumulation and bottlenecks that individual service metrics cannot identify.	{"summary": "Distributed Tracing Benefits:", "breakdown": ["Correlates requests across multiple service boundaries", "Identifies cumulative latency from service chains", "Reveals inter-service communication bottlenecks", "Provides end-to-end request visibility"], "otherOptions": "Verbose logging creates data overload without correlation. CPU/memory monitoring misses network and inter-service issues. More alerts don't provide root cause analysis."}	0	\N	6734	Application	Advanced
351	3422	Security - Vulnerability Management	\N	Security	Your security team discovers a critical CVE with a CVSS score of 9.8 affecting your web application servers. The vulnerability has active exploits in the wild, but the patch requires 2 hours of downtime per server. What should be your IMMEDIATE response?	[{"text": "Wait for the next maintenance window to apply patches", "isCorrect": false}, {"text": "Apply patches immediately without testing", "isCorrect": false}, {"text": "Implement temporary compensating controls such as WAF rules while scheduling emergency patching", "isCorrect": true}, {"text": "Monitor for attacks and apply patches only if exploitation is detected", "isCorrect": false}]	Implement temporary compensating controls such as WAF rules while scheduling emergency patching	Critical vulnerabilities (CVSS 9.0+) with active exploits require immediate action, but rushing patches without testing can cause service disruption. The best approach is to implement temporary mitigations (WAF rules, network segmentation, enhanced monitoring) to reduce risk while planning coordinated emergency patching. This balances security urgency with operational stability.	{"summary": "Critical CVE Response Strategy:", "breakdown": ["CVSS 9.8 indicates critical severity requiring urgent action", "Active exploits mean attackers are already targeting this vulnerability", "Compensating controls (WAF rules, IPS signatures, network ACLs) provide immediate risk reduction", "Emergency change management allows expedited but controlled patching", "Rolling updates minimize service impact during patching", "Post-patch validation ensures systems remain secure and functional"], "otherOptions": "Waiting for maintenance windows is too risky with active exploits. Patching without testing risks breaking production systems. Reactive monitoring only works if you catch attacks before damage occurs - proactive defense is essential for critical CVEs."}	0	\N	6735	Evaluation	Advanced
352	3423	Security - Penetration Testing	\N	Security	A financial services company must perform penetration testing on their cloud environment. The cloud provider requires advance notification and has specific restrictions on testing activities. Which approach ensures compliance while providing comprehensive testing?	[{"text": "Perform testing without notification to simulate real attacks", "isCorrect": false}, {"text": "Submit formal testing requests with detailed scope, obtain written approval, and follow provider guidelines", "isCorrect": true}, {"text": "Use automated vulnerability scanners instead of penetration testing", "isCorrect": false}, {"text": "Hire external testers to bypass provider restrictions", "isCorrect": false}]	Submit formal testing requests with detailed scope, obtain written approval, and follow provider guidelines	Cloud providers require formal authorization for penetration testing to prevent confusion with actual attacks and to avoid disrupting other tenants. The proper process includes submitting detailed test plans, defining scope (IP ranges, test types, timeframes), obtaining written approval, and adhering to acceptable use policies. This ensures legal compliance and maintains good provider relationships while enabling thorough security assessment.	{"summary": "Cloud Penetration Testing Requirements:", "breakdown": ["Most cloud providers require 7-14 days advance notice", "Test scope must specify: target systems, IP ranges, test types, duration", "Written authorization protects against legal issues and account suspension", "Provider restrictions typically prohibit: DoS testing, social engineering against provider staff, testing other customers", "Coordinated testing ensures provider security teams can distinguish authorized tests from real attacks", "Documentation proves due diligence for compliance audits (PCI DSS, SOC 2)"], "otherOptions": "Testing without notification violates cloud provider terms of service and may result in account suspension or legal action. Vulnerability scanning alone misses exploitation paths that penetration testing reveals. External testers must also follow provider requirements - hiring third parties does not bypass cloud provider policies."}	0	\N	6736	Application	Intermediate
353	3424	Security - Risk Management	\N	Security	During a vulnerability assessment, you identify that 409 of your cloud instances are running outdated operating systems with known vulnerabilities. Budget constraints prevent immediate replacement. What risk mitigation strategy should you prioritize?	[{"text": "Accept the risk and document it in the risk register", "isCorrect": false}, {"text": "Purchase cyber insurance to transfer the risk", "isCorrect": false}, {"text": "Disable all vulnerable systems until patches are available", "isCorrect": false}, {"text": "Implement network segmentation, WAF protection, and enhanced monitoring for affected systems", "isCorrect": true}]	Implement network segmentation, WAF protection, and enhanced monitoring for affected systems	When remediation is not immediately possible, implementing compensating controls is the most effective risk mitigation strategy. Network segmentation limits lateral movement if systems are compromised. Web Application Firewalls (WAF) block common exploits. Enhanced monitoring with SIEM correlation provides early detection of exploitation attempts. This defense-in-depth approach reduces risk while systems await patching or replacement.	{"summary": "Compensating Controls Strategy:", "breakdown": ["Network segmentation isolates vulnerable systems from critical assets", "Micro-segmentation with security groups limits attack surface", "WAF and IPS rules block known exploit signatures", "Enhanced logging and SIEM correlation detect anomalous behavior", "Privileged access restrictions minimize impact of potential compromises", "Regular security assessments verify control effectiveness", "This buys time for planned remediation without accepting unmitigated risk"], "otherOptions": "Simply accepting risk without mitigation leaves the organization vulnerable to exploitation. Cyber insurance transfers financial impact but does not prevent attacks or data breaches. Disabling 409 systems would cause massive business disruption - compensating controls allow continued operation with reduced risk."}	0	\N	6737	Analysis	Advanced
354	3425	Security - Vulnerability Management	\N	Security	Your organization needs to establish a vulnerability management program for a hybrid cloud environment. The program must handle different scanning requirements for on-premises and cloud resources. What framework should you implement?	[{"text": "Separate vulnerability management systems for each environment", "isCorrect": false}, {"text": "Unified vulnerability management platform with environment-specific scanning policies and prioritization based on asset criticality", "isCorrect": true}, {"text": "Cloud-only vulnerability management ignoring on-premises systems", "isCorrect": false}, {"text": "Manual vulnerability tracking using spreadsheets", "isCorrect": false}]	Unified vulnerability management platform with environment-specific scanning policies and prioritization based on asset criticality	A unified vulnerability management platform provides centralized visibility across hybrid environments while accommodating different scanning requirements. Cloud-based solutions can scan both on-premises (via deployed agents or scanners) and cloud resources (via API integration). Environment-specific policies handle unique requirements: agent-based scanning for on-prem, API-based for cloud IaaS, container scanning for Kubernetes. Asset criticality scoring ensures high-value systems receive priority remediation regardless of location.	{"summary": "Hybrid Cloud Vulnerability Management:", "breakdown": ["Unified platform provides single pane of glass visibility", "Agent-based scanning for traditional on-premises infrastructure", "API integration for cloud provider resources (AWS, Azure, GCP)", "Container and Kubernetes-specific scanning capabilities", "Risk-based prioritization using CVSS scores and asset criticality", "Integration with CMDB/asset inventory for complete context", "Automated workflow for remediation tracking and verification", "Compliance reporting across all environments (PCI, HIPAA, NIST)"], "otherOptions": "Separate systems create visibility gaps and duplicate effort. Ignoring on-premises leaves significant blind spots in security posture. Manual tracking with spreadsheets cannot scale to enterprise environments with thousands of assets and continuous vulnerability discovery."}	0	\N	6738	Synthesis	Expert
355	3426	Security - Identity and Access Management	\N	Security	A multinational corporation needs to implement single sign-on (SSO) for employees accessing cloud applications from multiple geographic locations. The solution must support both web applications and API access. Which implementation provides the BEST security and user experience?	[{"text": "Separate authentication systems for each geographic region", "isCorrect": false}, {"text": "Shared service accounts for regional access", "isCorrect": false}, {"text": "SAML-based SSO with multi-factor authentication and conditional access policies based on location and risk", "isCorrect": true}, {"text": "Simple password-based authentication with VPN requirements", "isCorrect": false}]	SAML-based SSO with multi-factor authentication and conditional access policies based on location and risk	SAML 2.0 provides enterprise-grade SSO supporting web applications and API access through federated identity. Combined with MFA and conditional access policies, this creates a zero-trust security model where access decisions consider user identity, device health, location, and behavior patterns. Conditional access can enforce stricter controls for high-risk scenarios (unusual locations, sensitive data access) while maintaining seamless experience for routine access. This balances security with usability across global operations.	{"summary": "Enterprise SSO Implementation:", "breakdown": ["SAML 2.0 enables federation between identity provider (IdP) and service providers", "Supports both browser-based and API authentication flows", "MFA adds strong authentication beyond passwords (TOTP, push notifications, biometrics)", "Conditional access policies enforce context-aware security: known devices, trusted locations, compliant endpoints", "Risk-based authentication adjusts requirements based on calculated risk score", "Centralized user management simplifies provisioning and deprovisioning", "Session management with timeout and re-authentication for sensitive operations", "Compatible with major cloud platforms (Microsoft 365, Google Workspace, AWS, Salesforce)"], "otherOptions": "Separate regional systems create management overhead and poor user experience when employees travel or access resources across regions. Shared service accounts eliminate accountability and violate least privilege principles. Password-only authentication with VPN is vulnerable to credential theft and provides poor mobile experience."}	0	\N	6739	Application	Advanced
356	3427	Security - Incident Response	\N	Security	An employee reports that their multi-factor authentication token was compromised during a phishing attack. The attacker may have gained access to corporate cloud resources. What should be your IMMEDIATE response priority?	[{"text": "Immediately disable the compromised account, initiate incident response, audit account activity, and implement additional security monitoring", "isCorrect": true}, {"text": "Reset the employee password and issue a new MFA token", "isCorrect": false}, {"text": "Monitor the account for suspicious activity", "isCorrect": false}, {"text": "Send security awareness training to the employee", "isCorrect": false}]	Immediately disable the compromised account, initiate incident response, audit account activity, and implement additional security monitoring	When MFA compromise is reported, immediate account lockout prevents further unauthorized access. This must be followed by comprehensive incident response: reviewing audit logs for unauthorized activities, checking for persistence mechanisms (backdoor accounts, API keys), identifying accessed data, and monitoring for lateral movement. Time is critical - every minute the account remains active increases potential damage. After containment, forensic investigation determines full scope of compromise.	{"summary": "MFA Compromise Response Procedures:", "breakdown": ["Immediately disable account to halt active intrusion", "Initiate formal incident response (declare incident, assemble team, establish war room)", "Audit cloud access logs: login times, source IPs, API calls, resource access", "Check for attacker persistence: new users created, API keys generated, backdoor accounts, privilege escalations", "Review accessed resources: VMs launched, data downloaded, configurations modified", "Implement enhanced monitoring for related accounts and resources", "Revoke all tokens and sessions, not just MFA device", "After containment: reset credentials, issue new MFA device, forensic analysis", "Post-incident: review detection gaps, update playbooks, employee training"], "otherOptions": "Simply resetting password and MFA without account lockout allows attacker continued access during the reset process. Passive monitoring is too slow - active intrusions require immediate containment. Training is important long-term but does not address the active security incident requiring immediate response."}	0	\N	6740	Evaluation	Advanced
357	3428	Security - Access Control	\N	Security	A development team requires access to production cloud resources for troubleshooting. Current security policies prohibit developers from having production access. How should you balance security requirements with operational needs?	[{"text": "Grant developers permanent production access with monitoring", "isCorrect": false}, {"text": "Implement break-glass access procedures with temporary privilege escalation, comprehensive logging, and automated approval workflows", "isCorrect": true}, {"text": "Deny all developer access to production environments", "isCorrect": false}, {"text": "Allow developers to use shared administrative accounts", "isCorrect": false}]	Implement break-glass access procedures with temporary privilege escalation, comprehensive logging, and automated approval workflows	Break-glass access provides emergency access mechanisms while maintaining security controls. This approach grants time-limited elevated privileges only when justified (incident response, critical troubleshooting), requires approval (automated or manual based on severity), logs all activities comprehensively, and automatically revokes access after the defined period. This balances operational needs with security principles of least privilege and accountability.	{"summary": "Break-Glass Access Implementation:", "breakdown": ["Just-in-time (JIT) access grants temporary permissions for specific time windows", "Multi-factor authentication required for break-glass access requests", "Approval workflows: automated for low-risk, manager approval for high-risk", "Comprehensive audit logging captures all actions during elevated access", "Automatic privilege revocation after time expiration (1-4 hours typical)", "Real-time alerting to security team when break-glass is activated", "Session recording for compliance and forensic analysis", "Regular reviews of break-glass usage patterns identify process improvements", "Integration with ITSM tools for incident correlation"], "otherOptions": "Permanent production access violates least privilege and increases attack surface. Complete denial creates operational bottlenecks during critical incidents. Shared accounts eliminate accountability and enable attackers to hide malicious activity."}	0	\N	6741	Synthesis	Expert
358	3429	Security - Identity and Access Management	\N	Security	Your organization is implementing role-based access control (RBAC) for a cloud environment supporting 500 employees across multiple departments. Users need access to different applications based on their job functions. What RBAC strategy provides the BEST balance of security and manageability?	[{"text": "Create individual roles for each user", "isCorrect": false}, {"text": "Use a single role for all users with application-level restrictions", "isCorrect": false}, {"text": "Design role hierarchy based on job functions with principle of least privilege, regular access reviews, and automated user provisioning", "isCorrect": true}, {"text": "Grant administrative access to department heads for user management", "isCorrect": false}]	Design role hierarchy based on job functions with principle of least privilege, regular access reviews, and automated user provisioning	Effective RBAC design groups users by job function rather than individual needs, creating manageable role sets that scale across the organization. Role hierarchy allows inheritance of permissions (e.g., Senior Developer inherits Developer permissions plus additional access). Least privilege ensures roles contain only necessary permissions. Regular access reviews (quarterly or semi-annual) catch privilege creep. Automated provisioning through HR system integration ensures consistent application and reduces manual errors.	{"summary": "Enterprise RBAC Design Principles:", "breakdown": ["Job function-based roles: Developer, QA Engineer, DevOps, Product Manager, etc.", "Role hierarchy enables permission inheritance reducing duplication", "Least privilege: roles contain minimum permissions for job function", "Separation of duties prevents single user from completing high-risk transactions alone", "Automated provisioning via HR integration (new hire  auto role assignment)", "De-provisioning automation when employees leave or change roles", "Quarterly access reviews: managers certify team members still need assigned roles", "Audit trails track permission changes and role assignments", "Integration with SSO for centralized user management"], "otherOptions": "Individual roles per user creates unmanageable complexity at scale (500 roles for 500 users). Single role for all users violates least privilege and makes access control meaningless. Delegating user management to non-security personnel without oversight creates security gaps and inconsistent policy enforcement."}	0	\N	6742	Synthesis	Advanced
359	3430	Security - Container Security	\N	Security	A software development company is implementing a container security strategy for their Kubernetes-based application platform. They need to ensure container images are secure from development through production deployment. What comprehensive approach should they implement?	[{"text": "Basic antivirus scanning on container hosts only", "isCorrect": false}, {"text": "Integrated security pipeline with image vulnerability scanning, admission controllers, runtime protection, and continuous monitoring", "isCorrect": true}, {"text": "Manual security reviews of container configurations", "isCorrect": false}, {"text": "Network-level security controls only", "isCorrect": false}]	Integrated security pipeline with image vulnerability scanning, admission controllers, runtime protection, and continuous monitoring	Comprehensive container security requires defense-in-depth across the entire lifecycle. Image scanning during build catches known vulnerabilities before deployment. Admission controllers enforce security policies at deployment time (no privileged containers, required resource limits). Runtime protection detects anomalous behavior (unexpected process execution, network connections). Continuous monitoring provides visibility into running containers. This layered approach addresses security at every stage rather than relying on a single control point.	{"summary": "Container Security Lifecycle:", "breakdown": ["BUILD PHASE: Scan base images and dependencies for CVEs using Trivy, Snyk, or Aqua", "BUILD PHASE: Static analysis of Dockerfiles for misconfigurations and secrets", "BUILD PHASE: Image signing ensures provenance and prevents tampering", "DEPLOY PHASE: Admission controllers (OPA, Kyverno) enforce security policies", "DEPLOY PHASE: Policies prevent: privileged containers, hostPath mounts, excessive capabilities", "RUNTIME PHASE: Runtime protection monitors container behavior for threats", "RUNTIME PHASE: Behavioral analysis detects anomalies (crypto mining, reverse shells)", "MONITORING: Continuous vulnerability assessment of running containers", "MONITORING: Security event correlation and alerting via SIEM integration"], "otherOptions": "Host-level antivirus misses container-specific threats and vulnerabilities in application dependencies. Manual reviews cannot scale to environments deploying hundreds of containers daily. Network controls alone miss application-layer vulnerabilities and misconfigurations within containers."}	0	\N	6743	Synthesis	Expert
360	3431	Security - Network Security	\N	Security	A financial services company needs to implement network policies for their Kubernetes environment to ensure proper micro-segmentation. The environment hosts multiple applications with different security requirements. Which network policy approach provides the BEST security?	[{"text": "Allow all traffic between pods for operational simplicity", "isCorrect": false}, {"text": "Implement default deny policies with specific allow rules for required communications, label-based segmentation, and regular policy audits", "isCorrect": true}, {"text": "Use only host-based firewalls without Kubernetes network policies", "isCorrect": false}, {"text": "Implement policies only for internet-facing services", "isCorrect": false}]	Implement default deny policies with specific allow rules for required communications, label-based segmentation, and regular policy audits	Zero-trust network architecture starts with default deny policies that block all traffic, then explicitly allows only required communications. Label-based segmentation uses Kubernetes labels to define security zones (frontend, backend, database) and enforce boundaries between them. This prevents lateral movement during breaches. Regular policy audits ensure rules remain aligned with application requirements and identify unused permissions. This approach implements least-privilege networking at the pod level.	{"summary": "Kubernetes Network Policy Best Practices:", "breakdown": ["Default deny ingress and egress policies for all namespaces", "Explicit allow rules define permitted pod-to-pod communications", "Label-based selectors enable flexible, maintainable policies", "Namespace-level isolation prevents cross-application traffic", "Egress policies restrict outbound connections (prevent data exfiltration)", "Integration with service mesh (Istio, Linkerd) adds mTLS and advanced routing", "Policy-as-code in Git enables version control and change management", "Automated testing validates policies before production deployment", "Regular audits identify overly permissive rules and unused policies"], "otherOptions": "Allowing all pod traffic eliminates network segmentation benefits and enables unrestricted lateral movement during attacks. Host-based firewalls operate at wrong level and cannot enforce pod-to-pod policies within same node. Protecting only internet-facing services ignores internal threats and lateral movement scenarios."}	0	\N	6744	Application	Advanced
361	3432	Security - Container Security	\N	Security	Your organization needs to secure a container registry that stores proprietary application images. The registry must support automated builds and integrate with your CI/CD pipeline while maintaining security. What security measures should you prioritize?	[{"text": "Basic password authentication for registry access", "isCorrect": false}, {"text": "Network-level access restrictions without authentication", "isCorrect": false}, {"text": "Physical security of registry servers only", "isCorrect": false}, {"text": "Image signing with cryptographic verification, vulnerability scanning integration, access controls with RBAC, and audit logging", "isCorrect": true}]	Image signing with cryptographic verification, vulnerability scanning integration, access controls with RBAC, and audit logging	Comprehensive registry security requires multiple controls. Image signing (Docker Content Trust, Notary, Cosign) ensures image integrity and authenticity. Integrated vulnerability scanning provides continuous assessment of stored images. RBAC controls who can push/pull specific images. Audit logging tracks all registry operations for compliance and incident investigation. These controls protect against image tampering, unauthorized access, and vulnerable image deployment.	{"summary": "Container Registry Security Controls:", "breakdown": ["Image signing with cryptographic keys prevents tampering in transit or storage", "Signature verification during pull ensures only trusted images deploy", "Automated vulnerability scanning on every image push", "Continuous re-scanning of stored images as new CVEs are published", "RBAC policies: developers push to dev repos, only CI/CD pushes to production", "Service accounts with minimal permissions for automated operations", "Immutable tags prevent overwriting production images with malicious versions", "Audit logs capture: image push/pull, deletion, access attempts", "Integration with secrets management for registry credentials"], "otherOptions": "Password authentication alone is vulnerable to credential theft and lacks granular controls. Network restrictions provide defense-in-depth but insufficient as primary controlinsider threats bypass this. Physical security is important but irrelevant for cloud-hosted registries and insufficient without logical controls."}	0	\N	6745	Application	Advanced
362	3433	Security - Performance Optimization	\N	Security	A development team reports that their Kubernetes pods are experiencing performance issues. Investigation reveals that security scanning is consuming excessive CPU resources during runtime. How should you optimize security scanning without compromising protection?	[{"text": "Implement layered scanning approach with lightweight runtime protection and comprehensive scanning during build/deployment phases", "isCorrect": true}, {"text": "Disable all runtime security scanning to improve performance", "isCorrect": false}, {"text": "Reduce scanning frequency to monthly intervals", "isCorrect": false}, {"text": "Move all security scanning to external systems", "isCorrect": false}]	Implement layered scanning approach with lightweight runtime protection and comprehensive scanning during build/deployment phases	Shift-left security moves expensive scanning operations earlier in the pipeline where performance impact is acceptable. Comprehensive vulnerability scanning during image build catches issues before deployment. Admission-time policy enforcement adds minimal overhead. Lightweight runtime protection focuses on behavioral detection (unexpected processes, network activity) rather than deep scanning. This provides strong security with minimal production performance impact by doing intensive work during development rather than runtime.	{"summary": "Security Scanning Optimization Strategy:", "breakdown": ["BUILD TIME: Comprehensive vulnerability scanning of images and dependencies", "BUILD TIME: Static analysis of application code and configurations", "BUILD TIME: Secret scanning to prevent credential leaks", "DEPLOYMENT TIME: Policy enforcement via admission controllers (fast validation)", "DEPLOYMENT TIME: Final image signature verification", "RUNTIME: Lightweight behavioral monitoring (process, network, file system)", "RUNTIME: Anomaly detection using established baselines", "RUNTIME: Focus on indicators of compromise, not comprehensive scanning", "Periodic re-scanning of deployed workloads during low-utilization periods"], "otherOptions": "Disabling runtime security eliminates visibility into active threats and zero-day exploits. Monthly scanning misses critical vulnerabilities with active exploits requiring immediate response. External scanning creates latency and may miss container-specific context needed for accurate threat detection."}	0	\N	6746	Analysis	Advanced
363	3434	Security - Compliance	\N	Security	A healthcare organization must achieve HIPAA compliance for their cloud-hosted electronic health record system. The compliance program must address both technical and administrative requirements. What comprehensive approach should they implement?	[{"text": "Focus only on encryption of patient data", "isCorrect": false}, {"text": "Apply basic security controls without formal compliance program", "isCorrect": false}, {"text": "Rely on cloud provider certifications without additional controls", "isCorrect": false}, {"text": "Implement comprehensive compliance framework including risk assessments, access controls, audit trails, employee training, and regular compliance monitoring", "isCorrect": true}]	Implement comprehensive compliance framework including risk assessments, access controls, audit trails, employee training, and regular compliance monitoring	HIPAA compliance requires holistic approach addressing administrative, physical, and technical safeguards. Risk assessments identify threats to ePHI and guide control implementation. Access controls enforce minimum necessary standard. Audit trails provide accountability. Employee training ensures workforce understands obligations. Business associate agreements (BAAs) with cloud providers establish shared responsibility. Regular monitoring through audits and risk assessments maintains compliance over time. This addresses all HIPAA Security Rule requirements.	{"summary": "HIPAA Compliance Framework Components:", "breakdown": ["ADMINISTRATIVE: Risk analysis identifying threats to ePHI confidentiality, integrity, availability", "ADMINISTRATIVE: Security policies covering access control, incident response, disaster recovery", "ADMINISTRATIVE: Workforce training on HIPAA requirements and security awareness", "ADMINISTRATIVE: Business associate agreements (BAA) with cloud provider and third parties", "TECHNICAL: Access controls implementing minimum necessary principle", "TECHNICAL: Audit trails logging all ePHI access and modifications", "TECHNICAL: Encryption at rest (AES-256) and in transit (TLS 1.2+)", "TECHNICAL: Authentication mechanisms including MFA for remote access", "PHYSICAL: Controls over facility access where ePHI is accessed", "MONITORING: Regular audits, penetration testing, vulnerability assessments", "MONITORING: Incident response procedures specific to ePHI breaches"], "otherOptions": "Encryption alone addresses only one technical safeguard, missing administrative and physical requirements. Basic controls without formal program lack documentation and accountability HIPAA requires. Cloud provider BAA is necessary but insufficientshared responsibility requires customer to implement additional controls for their applications and data."}	0	\N	6747	Synthesis	Expert
364	3435	Security - Zero Trust	\N	Security	Your organization is transitioning from perimeter-based security to a Zero Trust architecture for cloud resources. The environment includes legacy applications, modern microservices, and remote workforce accessing cloud applications. What implementation strategy provides the BEST security posture?	[{"text": "Replace all legacy applications before implementing Zero Trust", "isCorrect": false}, {"text": "Implement Zero Trust only for new cloud-native applications", "isCorrect": false}, {"text": "Deploy unified identity-centric controls with continuous verification, micro-segmentation, least privilege access, and adaptive authentication across all resources", "isCorrect": true}, {"text": "Focus Zero Trust controls on network perimeter only", "isCorrect": false}]	Deploy unified identity-centric controls with continuous verification, micro-segmentation, least privilege access, and adaptive authentication across all resources	Zero Trust architecture assumes no implicit trust based on network location. Identity becomes the primary security perimeter with continuous verification of users, devices, and applications. Micro-segmentation limits lateral movement. Least privilege ensures users and workloads have only necessary access. Adaptive authentication adjusts requirements based on risk signals. This approach works across hybrid environments including legacy systems by wrapping them with modern identity controls and segmentation.	{"summary": "Zero Trust Implementation Strategy:", "breakdown": ["Identity-centric approach: strong authentication (MFA, passwordless) for all users", "Continuous verification: repeatedly validate user, device, and workload trustworthiness", "Micro-segmentation: enforce least-privilege network access between workloads", "Adaptive authentication: risk-based policies adjust requirements (device posture, location, behavior)", "Device trust: verify endpoint health and compliance before granting access", "Application-aware policies: different access rules per application sensitivity", "Legacy integration: identity-aware proxies provide Zero Trust for legacy apps", "Encryption everywhere: TLS for all communications, not just external traffic", "Comprehensive logging: security analytics identify anomalies and policy violations", "Gradual rollout: phase implementation starting with highest-risk resources"], "otherOptions": "Waiting to replace legacy apps delays security improvements and may be economically infeasible. Zero Trust only for new apps leaves significant attack surface in legacy environment. Perimeter-focused approach contradicts Zero Trust principlesthreats come from inside and outside networks."}	0	\N	6748	Synthesis	Expert
365	3436	Security - Secrets Management	\N	Security	Your development team is storing database credentials, API keys, and encryption keys in application configuration files and environment variables. Security audit identifies this as high-risk practice. What secrets management approach should you implement?	[{"text": "Encrypt configuration files and commit to version control", "isCorrect": false}, {"text": "Use centralized secrets management service with dynamic secrets, encryption at rest, audit logging, and fine-grained access controls", "isCorrect": true}, {"text": "Store secrets in environment variables only", "isCorrect": false}, {"text": "Share secrets via encrypted email between team members", "isCorrect": false}]	Use centralized secrets management service with dynamic secrets, encryption at rest, audit logging, and fine-grained access controls	Centralized secrets management solutions (HashiCorp Vault, AWS Secrets Manager, Azure Key Vault) provide secure storage with encryption at rest and in transit. Dynamic secrets are generated on-demand and automatically rotate, limiting exposure window. Fine-grained access controls ensure only authorized applications and users can retrieve specific secrets. Comprehensive audit logging tracks all secret access for compliance and incident investigation. This eliminates hardcoded credentials and reduces secret sprawl.	{"summary": "Enterprise Secrets Management:", "breakdown": ["Centralized storage: single source of truth for all secrets", "Encryption at rest: secrets encrypted with HSM-backed keys", "Dynamic secrets: database credentials generated on-demand with short TTLs", "Automatic rotation: periodic rotation of long-lived secrets (API keys, certificates)", "Fine-grained access control: applications get only secrets they need", "Audit logging: complete trail of secret access, rotation, and modifications", "Secret versioning: maintain previous versions for rollback scenarios", "Integration with CI/CD: secrets injected at deployment time, not stored in artifacts", "Break-glass procedures: emergency access with enhanced logging and approval", "Secret sprawl reduction: discover and migrate hardcoded secrets"], "otherOptions": "Encrypted configuration files in version control still expose secrets to anyone with repository access and complicate rotation. Environment variables alone lack encryption at rest, access controls, rotation, and audit capabilities. Email distribution eliminates centralized management, prevents rotation, and creates uncontrolled secret copies."}	0	\N	6749	Application	Advanced
366	3437	Security - Data Protection	\N	Security	A financial services company processes sensitive customer financial data in their cloud environment. They need to prevent unauthorized data exfiltration while allowing legitimate business operations. What DLP strategy should they implement?	[{"text": "Block all outbound data transfers from cloud environment", "isCorrect": false}, {"text": "Rely on employee training alone without technical controls", "isCorrect": false}, {"text": "Deploy cloud-native DLP with content inspection, contextual analysis, automated policy enforcement, and user education", "isCorrect": true}, {"text": "Monitor data transfers manually through periodic reviews", "isCorrect": false}]	Deploy cloud-native DLP with content inspection, contextual analysis, automated policy enforcement, and user education	Effective DLP combines multiple controls: content inspection identifies sensitive data (PII, financial data, intellectual property) using pattern matching and classification. Contextual analysis considers who, what, where, when, and how data is being accessed or transferred. Automated enforcement blocks or quarantines policy violations in real-time. User education reduces accidental data loss. Cloud-native DLP integrates with cloud storage, email, and applications for comprehensive coverage.	{"summary": "Cloud DLP Implementation:", "breakdown": ["Content inspection: identify sensitive data using regex patterns, fingerprinting, machine learning", "Data classification: categorize data by sensitivity (public, internal, confidential, restricted)", "Contextual analysis: evaluate risk based on user role, destination, time, device posture", "Policy enforcement: block, quarantine, encrypt, or notify based on policy rules", "Multi-channel protection: cloud storage, email, web uploads, API calls", "Integration with CASB: extend DLP to SaaS applications (Microsoft 365, Salesforce)", "Incident response: automated workflows for policy violations", "User notifications: educate users when violations occur", "Comprehensive reporting: track data movement patterns and policy effectiveness", "Regular policy tuning: adjust rules based on false positives and business needs"], "otherOptions": "Blocking all transfers prevents legitimate business operations and is operationally infeasible. Training alone lacks technical enforcement to catch accidental or malicious data exfiltration. Manual monitoring cannot scale to real-time protection across cloud environments with thousands of daily transactions."}	0	\N	6750	Analysis	Advanced
367	3438	Security - Data Governance	\N	Security	During a compliance audit, auditors request evidence of data classification and protection controls for your cloud environment. Your organization processes multiple data types with different sensitivity levels. What evidence should you provide to demonstrate adequate data governance?	[{"text": "Comprehensive data classification scheme with protection matrix, automated classification tools, access controls by classification level, and audit reports", "isCorrect": true}, {"text": "Basic documentation of data types only", "isCorrect": false}, {"text": "Screenshots of security tool configurations", "isCorrect": false}, {"text": "Verbal explanation of data handling procedures", "isCorrect": false}]	Comprehensive data classification scheme with protection matrix, automated classification tools, access controls by classification level, and audit reports	Auditors require documented evidence demonstrating systematic data governance. A comprehensive classification scheme defines data categories (Public, Internal, Confidential, Restricted) with clear criteria. The protection matrix maps classifications to required controls (encryption, access restrictions, retention). Automated classification tools show consistent application. Access control reports prove enforcement. Audit logs demonstrate monitoring. This evidence package proves data governance is not ad-hoc but systematically implemented and verifiable.	{"summary": "Data Governance Evidence Requirements:", "breakdown": ["Classification scheme: documented taxonomy with definitions and examples for each level", "Protection matrix: mapping of classification levels to required security controls", "Automated classification: tools that tag data based on content analysis and metadata", "Policy documentation: formal policies governing data handling by classification", "Access controls: role-based restrictions aligned with classification levels", "Encryption evidence: at-rest and in-transit protection for sensitive classifications", "Audit reports: logs showing data access, classification changes, and policy violations", "Training records: evidence workforce understands classification requirements", "Regular reviews: documented periodic assessment of classification accuracy", "Incident response: procedures specific to breaches at each classification level"], "otherOptions": "Basic documentation lacks the detail and verifiability auditors require for compliance validation. Screenshots show point-in-time configurations but not systematic processes or effectiveness. Verbal explanations provide no auditable evidence and cannot be verified independently after the audit."}	0	\N	6751	Evaluation	Advanced
368	3439	Security - Legal Compliance	\N	Security	A legal hold notice requires your organization to preserve all data related to a specific project for litigation purposes. The data is stored across multiple cloud services with automated retention policies. What should be your IMMEDIATE response?	[{"text": "Continue with normal retention policies to avoid suspicion", "isCorrect": false}, {"text": "Wait for additional legal guidance before taking action", "isCorrect": false}, {"text": "Delete only obviously irrelevant data", "isCorrect": false}, {"text": "Immediately suspend automated deletions for relevant data, implement legal hold procedures, document preserved data, and notify legal team", "isCorrect": true}]	Immediately suspend automated deletions for relevant data, implement legal hold procedures, document preserved data, and notify legal team	Legal holds require immediate preservation to prevent spoliation of evidence, which can result in severe legal penalties. The first action is suspending all automated deletion and retention policies for in-scope data. Implement legal hold flags in systems to prevent destruction. Document what data exists and where it resides. Create forensically sound copies if needed. Notify legal counsel immediately to confirm scope and receive guidance. Delays or continued deletions after receiving notice constitute failure to preserve evidence.	{"summary": "Legal Hold Implementation Process:", "breakdown": ["IMMEDIATE: Suspend automated retention policies and deletion jobs for in-scope data", "IMMEDIATE: Notify all relevant personnel (IT, legal, records management) about hold", "Identify data scope: determine all systems, backups, and archives containing relevant data", "Implement hold flags: mark data in systems to prevent deletion or modification", "Create forensic copies: preserve data in tamper-evident format with chain of custody", "Document preservation: inventory all preserved data with locations and dates", "Communicate with custodians: instruct employees not to delete relevant communications", "Extend to backups: ensure backup retention policies preserve in-scope backups", "Monitor compliance: ongoing verification that hold is maintained", "Release procedures: formal process for releasing data only after legal authorization"], "otherOptions": "Continuing normal retention risks destroying evidence and severe legal sanctions including adverse inference rulings. Waiting for guidance allows automated systems to continue deleting dataholds require immediate action. Deleting any data after receiving hold notice, even if seemingly irrelevant, constitutes spoliationscope determination is legal counsel responsibility."}	0	\N	6752	Application	Advanced
369	3440	Security - Data Sovereignty	\N	Security	Your organization operates in multiple countries with different data sovereignty requirements. Cloud data must remain within specific geographic boundaries for regulatory compliance. How should you implement data sovereignty controls?	[{"text": "Store all data in the least restrictive jurisdiction", "isCorrect": false}, {"text": "Use encryption to satisfy all data sovereignty requirements", "isCorrect": false}, {"text": "Implement geographic data controls with region-specific storage policies, data residency verification, and cross-border transfer restrictions", "isCorrect": true}, {"text": "Ignore data sovereignty requirements for cloud services", "isCorrect": false}]	Implement geographic data controls with region-specific storage policies, data residency verification, and cross-border transfer restrictions	Data sovereignty compliance requires technical controls ensuring data remains within approved geographic boundaries. Use cloud regions matching regulatory requirements (EU data in EU regions for GDPR). Implement storage policies that automatically route data to correct regions based on customer location or data type. Deploy verification mechanisms confirming data location. Restrict cross-border transfers through network controls and data classification. Document data flows for regulatory audits. This ensures compliance with regulations like GDPR, China Cybersecurity Law, and Russia Data Localization Law.	{"summary": "Data Sovereignty Implementation:", "breakdown": ["Geographic storage policies: automatically store data in region matching regulatory requirements", "Cloud region selection: choose provider regions with appropriate jurisdictions and certifications", "Data classification: tag data with residency requirements based on type and subject", "Access controls: restrict cross-border data access to authorized personnel only", "Network restrictions: prevent data replication or backup to unapproved regions", "Encryption in transit: protect data during authorized cross-border transfers", "Residency verification: regular audits confirming data remains in approved locations", "Vendor contracts: ensure cloud provider SLAs guarantee data residency commitments", "Transfer mechanisms: implement Standard Contractual Clauses (SCC) for lawful transfers", "Documentation: maintain data flow maps and processing records for regulators"], "otherOptions": "Least restrictive jurisdiction violates regulations in stricter jurisdictions and creates legal risk. Encryption alone does not satisfy sovereignty requirementsregulations mandate physical data location regardless of encryption. Ignoring requirements for cloud services is not an exemptionorganizations remain liable for compliance regardless of infrastructure model."}	0	\N	6753	Application	Advanced
370	3441	Security - Incident Response	\N	Security	Your organization cloud environment is experiencing a distributed denial-of-service (DDoS) attack targeting your e-commerce website. The attack is causing service degradation and potential revenue loss. What should be your IMMEDIATE response strategy?	[{"text": "Contact cloud provider support and wait for assistance", "isCorrect": false}, {"text": "Activate DDoS protection services, implement traffic filtering rules, scale infrastructure resources, and communicate with stakeholders", "isCorrect": true}, {"text": "Shut down the website until the attack stops", "isCorrect": false}, {"text": "Ignore the attack and hope it subsides naturally", "isCorrect": false}]	Activate DDoS protection services, implement traffic filtering rules, scale infrastructure resources, and communicate with stakeholders	Effective DDoS response requires immediate multi-layered action. Activate DDoS protection services (AWS Shield Advanced, Cloudflare, Akamai) to absorb and filter attack traffic. Implement rate limiting and geo-blocking to reduce attack surface. Scale infrastructure to handle increased load where cost-effective. Communicate with internal stakeholders about incident status and customer-facing teams about potential impacts. This maintains service availability while mitigating attack effects. Cloud providers offer DDoS protection but require activation and configuration.	{"summary": "DDoS Response Strategy:", "breakdown": ["IMMEDIATE: Activate cloud DDoS protection services (AWS Shield, Azure DDoS Protection)", "IMMEDIATE: Contact cloud provider support for additional assistance and insights", "Traffic filtering: implement rate limiting on API endpoints and web servers", "Geo-blocking: restrict traffic from regions not relevant to business operations", "WAF rules: block known attack signatures and suspicious patterns", "Content delivery: leverage CDN to absorb traffic and cache static content", "Auto-scaling: increase capacity to handle legitimate traffic during attack", "Traffic analysis: identify attack vectors, source IPs, and targeted resources", "Stakeholder communication: notify executives, customer support, and PR teams", "Post-incident: analyze attack patterns and improve preventive controls"], "otherOptions": "Simply contacting support without taking immediate action allows continued service degradation during response time. Shutting down website eliminates revenue and gives attackers their desired outcomeavailability is the goal. Ignoring attacks hoping they stop risks extended outage and does not prepare for repeat attacks."}	0	\N	6754	Application	Intermediate
371	3442	Security - Security Monitoring	\N	Security	Your SIEM system is generating 10,000 security alerts daily, overwhelming your security team and causing alert fatigue. Many alerts appear to be false positives. How should you optimize your security monitoring strategy?	[{"text": "Ignore all low-priority alerts to focus on critical issues", "isCorrect": false}, {"text": "Implement alert prioritization using risk scoring, tune detection rules to reduce false positives, and establish tiered response procedures", "isCorrect": true}, {"text": "Disable alerting for all but the most critical security events", "isCorrect": false}, {"text": "Hire additional security analysts to handle all alerts", "isCorrect": false}]	Implement alert prioritization using risk scoring, tune detection rules to reduce false positives, and establish tiered response procedures	Alert fatigue degrades security effectiveness when analysts miss critical alerts among noise. Systematic optimization starts with risk-based prioritization scoring alerts by impact, asset criticality, and threat confidence. Tune detection rules by analyzing false positives and adjusting thresholds, correlation logic, and whitelists. Establish tiered response procedures: critical alerts require immediate investigation, medium-tier alerts queue for daily review, low-tier alerts aggregate for trend analysis. This focuses analyst attention on highest-risk events while maintaining visibility.	{"summary": "SIEM Alert Optimization Process:", "breakdown": ["Risk scoring: calculate alert priority based on asset value, threat severity, user criticality", "False positive analysis: identify recurring false positives through alert classification", "Rule tuning: adjust detection thresholds, timing windows, and correlation logic", "Contextual enrichment: add asset context, user behavior baselines, threat intelligence", "Alert suppression: whitelist known-good activities causing false positives", "Tiered response: Critical (immediate), High (2-hour SLA), Medium (daily review), Low (weekly trends)", "Automated response: implement SOAR playbooks for common alert types", "Continuous improvement: regular review of alert quality metrics and response effectiveness", "Baseline establishment: create behavioral baselines to reduce anomaly false positives", "Documentation: maintain runbooks for common alert types to improve response consistency"], "otherOptions": "Ignoring low-priority alerts creates blind spotssophisticated attacks often start with low-severity indicators. Disabling most alerting eliminates early warning capabilities and violates defense-in-depth principles. Hiring more analysts treats symptoms not causeswithout optimization, alert volume will continue overwhelming expanded teams."}	0	\N	6755	Analysis	Advanced
372	3443	Security - Incident Response	\N	Security	A security incident response team discovers evidence of advanced persistent threat (APT) activity in your cloud environment. The threat actor appears to have maintained access for several weeks. What should be your comprehensive incident response approach?	[{"text": "Simply change all passwords and hope the threat is eliminated", "isCorrect": false}, {"text": "Execute formal incident response plan including threat containment, forensic analysis, system hardening, and stakeholder communication", "isCorrect": true}, {"text": "Ignore the activity if no data appears to be stolen", "isCorrect": false}, {"text": "Monitor the threat actor to understand their methods", "isCorrect": false}]	Execute formal incident response plan including threat containment, forensic analysis, system hardening, and stakeholder communication	APT incidents require comprehensive response following established incident response frameworks (NIST, SANS). Immediately contain the threat by isolating compromised systems while preserving evidence. Conduct thorough forensic analysis to determine scope, persistence mechanisms, and data accessed. Eradicate all threat actor presence including backdoors, web shells, and compromised credentials. Harden systems to prevent re-entry. Maintain detailed timeline and evidence chain for potential legal action. Communicate appropriately with executives, legal, affected parties, and regulators. APTs require methodical responsehasty actions may alert attackers or destroy evidence.	{"summary": "APT Incident Response Phases:", "breakdown": ["PREPARATION: Activate incident response team, establish war room, preserve evidence", "IDENTIFICATION: Determine attack timeline, entry vectors, lateral movement, data accessed", "CONTAINMENT: Isolate compromised systems without alerting attacker if possible", "CONTAINMENT: Disable compromised accounts, revoke credentials, block C2 communications", "ERADICATION: Remove all attacker presencebackdoors, web shells, malware, persistence mechanisms", "ERADICATION: Reimaging compromised systems may be necessary for APT", "RECOVERY: Restore systems from clean backups, implement additional monitoring", "HARDENING: Apply security improvements to prevent similar attacks", "LESSONS LEARNED: Conduct post-incident review to improve defenses and procedures", "COMMUNICATION: Executive briefings, legal notifications, regulatory reporting as required", "DOCUMENTATION: Maintain detailed timeline and evidence for potential legal proceedings"], "otherOptions": "Password changes alone miss persistence mechanisms (backdoors, web shells, authorized API keys) APTs establish. Ignoring activity because theft is not detected is naiveAPTs establish presence before exfiltration and current visibility may be incomplete. Continued monitoring without containment allows ongoing damage and data theft while attackers may detect investigation and accelerate exfiltration."}	0	\N	6756	Evaluation	Expert
373	3444	Security - Endpoint Security	\N	Security	Your organization needs to implement endpoint protection for cloud-based virtual machines running various operating systems. The solution must provide real-time protection without impacting system performance. What endpoint protection strategy should you implement?	[{"text": "Install different security products for each operating system", "isCorrect": false}, {"text": "Deploy unified endpoint protection platform with centralized management, behavioral analysis, and cloud-native integration", "isCorrect": true}, {"text": "Rely on operating system built-in security features only", "isCorrect": false}, {"text": "Use network-based security controls instead of endpoint protection", "isCorrect": false}]	Deploy unified endpoint protection platform with centralized management, behavioral analysis, and cloud-native integration	Modern endpoint protection requires unified platforms supporting multiple OS types (Windows, Linux, macOS) through single management console. Cloud-native EPP integrates with cloud infrastructure APIs for automated deployment and monitoring. Behavioral analysis detects threats through machine learning rather than signature-only approaches, catching zero-day exploits. Centralized management provides visibility across all endpoints and enables consistent policy enforcement. Performance optimization through cloud-based threat intelligence lookups reduces endpoint resource consumption.	{"summary": "Cloud Endpoint Protection Architecture:", "breakdown": ["Unified platform: single agent supporting Windows, Linux, macOS endpoints", "Centralized management: cloud-based console providing visibility across all endpoints", "Behavioral detection: machine learning identifies malicious behavior without signatures", "Next-gen antivirus: combines signatures with behavioral analysis and threat intelligence", "Endpoint detection and response (EDR): provides forensic capabilities and threat hunting", "Cloud integration: APIs for automated agent deployment on new instances", "Performance optimization: cloud-based lookups reduce local scanning overhead", "Automated remediation: policies for automatic threat quarantine and system isolation", "Vulnerability scanning: identifies missing patches and misconfigurations", "Compliance reporting: demonstrates endpoint security posture for audits"], "otherOptions": "Multiple products create management complexity, inconsistent protection levels, and higher costs. OS built-in features provide basic protection but lack centralized management, advanced detection, and enterprise-grade reporting required for production environments. Network controls complement but cannot replace endpoint protectionthey miss on-endpoint attacks, malware execution, and privilege escalation."}	0	\N	6757	Application	Advanced
374	3445	Security - Insider Threats	\N	Security	Security monitoring reveals unusual data access patterns suggesting potential insider threat activity. An employee is accessing large amounts of sensitive data outside their normal job responsibilities. What should be your investigation and response approach?	[{"text": "Conduct discrete investigation following established procedures, preserve evidence, restrict access if necessary, and involve HR and legal teams", "isCorrect": true}, {"text": "Immediately terminate the employee without investigation", "isCorrect": false}, {"text": "Ignore the activity if no data has left the organization", "isCorrect": false}, {"text": "Confront the employee directly about the suspicious activity", "isCorrect": false}]	Conduct discrete investigation following established procedures, preserve evidence, restrict access if necessary, and involve HR and legal teams	Insider threat investigations require careful balance between security response and legal/HR considerations. Conduct discrete investigation to gather evidence without alerting suspect. Preserve audit logs and data access records with proper chain of custody. Determine if activity is malicious or legitimate business need misunderstood by monitoring. Involve HR and legal early to ensure proper procedures and employee rights. If threat is confirmed, restrict access to prevent further damage. Premature confrontation may alert insider to destroy evidence or accelerate malicious activity.	{"summary": "Insider Threat Investigation Protocol:", "breakdown": ["INITIAL ASSESSMENT: Review access logs, determine scope of unusual activity, identify accessed data", "EVIDENCE PRESERVATION: Create forensic copies of logs with chain of custody documentation", "DISCRETE MONITORING: Increase monitoring without alerting employee to investigation", "CROSS-REFERENCE: Check for external communications, file downloads, USB usage", "BEHAVIORAL ANALYSIS: Review recent HR issues, performance problems, life changes", "TEAM COORDINATION: Brief HR, legal counsel, and security leadership", "LEGITIMATE USE CHECK: Verify with manager if access could be business-justified", "ACCESS RESTRICTION: If threat confirmed, disable accounts and revoke credentials", "DOCUMENTATION: Maintain detailed timeline for potential legal proceedings", "POST-INCIDENT: Review controls that failed to prevent unauthorized access"], "otherOptions": "Immediate termination without investigation risks wrongful termination lawsuits and may destroy evidence needed for criminal prosecution if theft occurred. Ignoring activity because no exfiltration detected is prematuresophisticated insiders stage data before exfiltration and current detection may be incomplete. Direct confrontation alerts insider to investigation, provides opportunity to destroy evidence, and may accelerate malicious activity."}	0	\N	6758	Analysis	Advanced
\.


--
-- Data for Name: users; Type: TABLE DATA; Schema: prepper; Owner: postgres
--

COPY prepper.users (id, username, email, password_hash, role, created_at, last_login, is_active) FROM stdin;
14	erbbo	erbbolovesless@gmail.com	$2b$10$s1yOs34JLMt2GPYq7DQh4OE4vKshFthav.sFoghGVYeo3ZuSd14wW	user	2025-12-02 13:08:36.883095	\N	t
15	ebc	ericryanbowser@gmail.com	$2b$10$fMKfjQRUCxOa5ThPM/HMSe3eJzjrS./KJ10yH/Kfxx8j13RAa43ty	user	2025-12-02 13:17:33.183439	\N	t
13	ericbo	laser@new-collar.space	$2b$10$NIu/KXRxOtiyLF70d3HZBOpA/KlYxT6aYSHTb4cjTIxQ1hidbFW0O	admin	2025-12-01 23:10:38.910773	2025-12-02 14:19:41.81116	t
\.


--
-- Name: env_id_seq; Type: SEQUENCE SET; Schema: config; Owner: postgres
--

SELECT pg_catalog.setval('config.env_id_seq', 4, true);


--
-- Name: contact_contactid_seq; Type: SEQUENCE SET; Schema: lasertg; Owner: postgres
--

SELECT pg_catalog.setval('lasertg.contact_contactid_seq', 112, true);


--
-- Name: orders_orderid_seq; Type: SEQUENCE SET; Schema: lasertg; Owner: postgres
--

SELECT pg_catalog.setval('lasertg.orders_orderid_seq', 49, true);


--
-- Name: admin_adminkey_seq; Type: SEQUENCE SET; Schema: prepper; Owner: postgres
--

SELECT pg_catalog.setval('prepper.admin_adminkey_seq', 1, false);


--
-- Name: aws_certified_architect_associate_questions_id_seq; Type: SEQUENCE SET; Schema: prepper; Owner: ericbo
--

SELECT pg_catalog.setval('prepper.aws_certified_architect_associate_questions_id_seq', 144, true);


--
-- Name: aws_question_id_seq; Type: SEQUENCE SET; Schema: prepper; Owner: ericbo
--

SELECT pg_catalog.setval('prepper.aws_question_id_seq', 248, true);


--
-- Name: aws_question_number_seq; Type: SEQUENCE SET; Schema: prepper; Owner: ericbo
--

SELECT pg_catalog.setval('prepper.aws_question_number_seq', 147, true);


--
-- Name: comptia_cloud_plus_questions_id_seq; Type: SEQUENCE SET; Schema: prepper; Owner: postgres
--

SELECT pg_catalog.setval('prepper.comptia_cloud_plus_questions_id_seq', 6758, true);


--
-- Name: question_id_seq; Type: SEQUENCE SET; Schema: prepper; Owner: postgres
--

SELECT pg_catalog.setval('prepper.question_id_seq', 374, true);


--
-- Name: question_number_seq; Type: SEQUENCE SET; Schema: prepper; Owner: postgres
--

SELECT pg_catalog.setval('prepper.question_number_seq', 3445, true);


--
-- Name: users_id_seq; Type: SEQUENCE SET; Schema: prepper; Owner: postgres
--

SELECT pg_catalog.setval('prepper.users_id_seq', 15, true);


--
-- Name: env env_pkey; Type: CONSTRAINT; Schema: config; Owner: postgres
--

ALTER TABLE ONLY config.env
    ADD CONSTRAINT env_pkey PRIMARY KEY (id);


--
-- Name: contact contactid; Type: CONSTRAINT; Schema: lasertg; Owner: postgres
--

ALTER TABLE ONLY lasertg.contact
    ADD CONSTRAINT contactid PRIMARY KEY (id);


--
-- Name: qrcode id; Type: CONSTRAINT; Schema: lasertg; Owner: postgres
--

ALTER TABLE ONLY lasertg.qrcode
    ADD CONSTRAINT id PRIMARY KEY (id);


--
-- Name: inventory inventory_pkey; Type: CONSTRAINT; Schema: lasertg; Owner: postgres
--

ALTER TABLE ONLY lasertg.inventory
    ADD CONSTRAINT inventory_pkey PRIMARY KEY (id);


--
-- Name: material material_pkey; Type: CONSTRAINT; Schema: lasertg; Owner: postgres
--

ALTER TABLE ONLY lasertg.material
    ADD CONSTRAINT material_pkey PRIMARY KEY (id);


--
-- Name: orders orders_pkey; Type: CONSTRAINT; Schema: lasertg; Owner: postgres
--

ALTER TABLE ONLY lasertg.orders
    ADD CONSTRAINT orders_pkey PRIMARY KEY (id);


--
-- Name: tag tag_pkey; Type: CONSTRAINT; Schema: lasertg; Owner: postgres
--

ALTER TABLE ONLY lasertg.tag
    ADD CONSTRAINT tag_pkey PRIMARY KEY (id);


--
-- Name: admin admin_pkey; Type: CONSTRAINT; Schema: prepper; Owner: postgres
--

ALTER TABLE ONLY prepper.admin
    ADD CONSTRAINT admin_pkey PRIMARY KEY (id);


--
-- Name: comptia_cloud_plus_questions id; Type: CONSTRAINT; Schema: prepper; Owner: postgres
--

ALTER TABLE ONLY prepper.comptia_cloud_plus_questions
    ADD CONSTRAINT id PRIMARY KEY (id) INCLUDE (id);


--
-- Name: users users_email_key; Type: CONSTRAINT; Schema: prepper; Owner: postgres
--

ALTER TABLE ONLY prepper.users
    ADD CONSTRAINT users_email_key UNIQUE (email);


--
-- Name: users users_pkey; Type: CONSTRAINT; Schema: prepper; Owner: postgres
--

ALTER TABLE ONLY prepper.users
    ADD CONSTRAINT users_pkey PRIMARY KEY (id);


--
-- Name: users users_username_key; Type: CONSTRAINT; Schema: prepper; Owner: postgres
--

ALTER TABLE ONLY prepper.users
    ADD CONSTRAINT users_username_key UNIQUE (username);


--
-- Name: fki_contactid; Type: INDEX; Schema: lasertg; Owner: postgres
--

CREATE INDEX fki_contactid ON lasertg.orders USING btree (contactid) WITH (fillfactor='100', deduplicate_items='true');


--
-- Name: fki_contactid_fk_contact; Type: INDEX; Schema: lasertg; Owner: postgres
--

CREATE INDEX fki_contactid_fk_contact ON lasertg.qrcode USING btree (contactid);


--
-- Name: fki_materialid; Type: INDEX; Schema: lasertg; Owner: postgres
--

CREATE INDEX fki_materialid ON lasertg.inventory USING btree (materialid);


--
-- Name: fki_orderid; Type: INDEX; Schema: lasertg; Owner: postgres
--

CREATE INDEX fki_orderid ON lasertg.qrcode USING btree (orderid);


--
-- Name: id_index; Type: INDEX; Schema: lasertg; Owner: postgres
--

CREATE INDEX id_index ON lasertg.contact USING btree (id) WITH (deduplicate_items='true');


--
-- Name: idx_orders_contactid; Type: INDEX; Schema: lasertg; Owner: postgres
--

CREATE INDEX idx_orders_contactid ON lasertg.orders USING btree (contactid) WITH (fillfactor='100', deduplicate_items='true');


--
-- Name: idx_category; Type: INDEX; Schema: prepper; Owner: postgres
--

CREATE INDEX idx_category ON prepper.comptia_cloud_plus_questions USING btree (category);


--
-- Name: idx_cognitive_level; Type: INDEX; Schema: prepper; Owner: postgres
--

CREATE INDEX idx_cognitive_level ON prepper.comptia_cloud_plus_questions USING btree (cognitive_level);


--
-- Name: idx_cognitive_skill; Type: INDEX; Schema: prepper; Owner: postgres
--

CREATE INDEX idx_cognitive_skill ON prepper.comptia_cloud_plus_questions USING btree (cognitive_level, skill_level);


--
-- Name: idx_domain; Type: INDEX; Schema: prepper; Owner: postgres
--

CREATE INDEX idx_domain ON prepper.comptia_cloud_plus_questions USING btree (domain);


--
-- Name: idx_skill_level; Type: INDEX; Schema: prepper; Owner: postgres
--

CREATE INDEX idx_skill_level ON prepper.comptia_cloud_plus_questions USING btree (skill_level);


--
-- Name: idx_users_email; Type: INDEX; Schema: prepper; Owner: postgres
--

CREATE INDEX idx_users_email ON prepper.users USING btree (email);


--
-- Name: idx_users_is_active; Type: INDEX; Schema: prepper; Owner: postgres
--

CREATE INDEX idx_users_is_active ON prepper.users USING btree (is_active);


--
-- Name: idx_users_role; Type: INDEX; Schema: prepper; Owner: postgres
--

CREATE INDEX idx_users_role ON prepper.users USING btree (role);


--
-- Name: idx_users_username; Type: INDEX; Schema: prepper; Owner: postgres
--

CREATE INDEX idx_users_username ON prepper.users USING btree (username);


--
-- Name: question_id; Type: INDEX; Schema: prepper; Owner: postgres
--

CREATE INDEX question_id ON prepper.comptia_cloud_plus_questions USING btree (question_id) WITH (deduplicate_items='true');


--
-- Name: question_number; Type: INDEX; Schema: prepper; Owner: postgres
--

CREATE INDEX question_number ON prepper.comptia_cloud_plus_questions USING btree (question_number) WITH (deduplicate_items='true');


--
-- Name: orders contactid; Type: FK CONSTRAINT; Schema: lasertg; Owner: postgres
--

ALTER TABLE ONLY lasertg.orders
    ADD CONSTRAINT contactid FOREIGN KEY (contactid) REFERENCES lasertg.contact(id);


--
-- Name: qrcode contactid; Type: FK CONSTRAINT; Schema: lasertg; Owner: postgres
--

ALTER TABLE ONLY lasertg.qrcode
    ADD CONSTRAINT contactid FOREIGN KEY (contactid) REFERENCES lasertg.contact(id);


--
-- Name: inventory materialid; Type: FK CONSTRAINT; Schema: lasertg; Owner: postgres
--

ALTER TABLE ONLY lasertg.inventory
    ADD CONSTRAINT materialid FOREIGN KEY (materialid) REFERENCES lasertg.material(id);


--
-- Name: qrcode orderid; Type: FK CONSTRAINT; Schema: lasertg; Owner: postgres
--

ALTER TABLE ONLY lasertg.qrcode
    ADD CONSTRAINT orderid FOREIGN KEY (orderid) REFERENCES lasertg.orders(id);


--
-- Name: TABLE users; Type: ACL; Schema: prepper; Owner: postgres
--

GRANT ALL ON TABLE prepper.users TO ericbo;


--
-- Name: SEQUENCE users_id_seq; Type: ACL; Schema: prepper; Owner: postgres
--

GRANT SELECT,USAGE ON SEQUENCE prepper.users_id_seq TO ericbo;


--
-- PostgreSQL database dump complete
--

\unrestrict c6HX8DCQDod7ra3PzEQJpb9WtGdbSm3scLrzqrbpQmAWesYAdsMJ1XIMoEb0NLz

